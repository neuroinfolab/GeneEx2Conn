{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98363b8f-1d61-429d-865a-3d8213cf3406",
   "metadata": {},
   "source": [
    "## Sim Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83562164-b395-4948-85bd-0cc2a4d2c0e9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25060b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29dd74e-acf1-4701-bb72-ce3701d3d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566563ca-070f-4671-a98a-338fea600261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sim.sim_run' from '/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim_run.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import data\n",
    "\n",
    "import models\n",
    "import sim.sim\n",
    "import sim.sim_utils\n",
    "from sim.sim_utils import bytes2human, print_system_usage\n",
    "from sim.sim import Simulation\n",
    "from sim.sim_run import single_sim_run\n",
    "\n",
    "importlib.reload(sim.sim)\n",
    "importlib.reload(sim.sim_run) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa8bad-5d12-46eb-a39f-18057f3923d7",
   "metadata": {},
   "source": [
    "#### Check job specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7c8d67-a18c-4208-b20f-8ba13fd58e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 52.2%\n",
      "RAM Usage: 35.3%\n",
      "Available RAM: 244.1G\n",
      "Total RAM: 377.1G\n",
      "52.4G\n"
     ]
    }
   ],
   "source": [
    "print_system_usage()\n",
    "\n",
    "total = psutil.disk_usage('/').total\n",
    "print(bytes2human(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6495ec",
   "metadata": {},
   "source": [
    "## Wandb API Query <a id=\"sims\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7393acfa-6242-4be8-a51d-adf08ec2fa1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4038994576.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    relevant info about wandb\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "relevant info about wandb\n",
    "\n",
    "https://wandb.ai/alexander-ratzan-new-york-university/gx2conn?nw=nwuserasratzan\n",
    "https://wandb.ai/asratzan\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"alexander-ratzan-new-york-university/gx2conn/<run_id>\")\n",
    "run.config[\"key\"] = updated_value\n",
    "run.update()\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# run is specified by <entity>/<project>/<run_id>\n",
    "run = api.run(\"alexander-ratzan-new-york-university/gx2conn/<run_id>\")\n",
    "\n",
    "# save the metrics for the run to a csv file\n",
    "metrics_dataframe = run.history()\n",
    "metrics_dataframe.to_csv(\"metrics.csv\")\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"alexander-ratzan-new-york-university/gx2conn/<run_id>\")\n",
    "if run.state == \"finished\":\n",
    "    for i, row in run.history().iterrows():\n",
    "      print(row[\"_timestamp\"], row[\"accuracy\"])\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"alexander-ratzan-new-york-university/gx2conn/<run_id>\")\n",
    "history = run.scan_history()\n",
    "losses = [row[\"loss\"] for row in history]\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "sweep = api.sweep(\"alexander-ratzan-new-york-university/gx2conn/<sweep_id>\")\n",
    "runs = sorted(sweep.runs,\n",
    "  key=lambda run: run.summary.get(\"val_acc\", 0), reverse=True)\n",
    "val_acc = runs[0].summary.get(\"val_acc\", 0)\n",
    "print(f\"Best run {runs[0].name} with {val_acc}% validation accuracy\")\n",
    "\n",
    "runs[0].file(\"model.h5\").download(replace=True)\n",
    "print(\"Best model saved to model-best.h5\")\n",
    "\n",
    "Project visibility\n",
    "Team\n",
    "Last active\n",
    "7/11/2025, 5:46:21 PM\n",
    "Contributors\n",
    "1 user\n",
    "Total runs\n",
    "20270\n",
    "Total compute\n",
    "111 days\n",
    "\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'cge': CGEModel,\n",
    "    'gaussian_kernel': GaussianKernelModel,\n",
    "    'exponential_decay': ExponentialDecayModel,\n",
    "    'bilinear_lowrank': BilinearLowRank,\n",
    "    'bilinear_CM': BilinearCM,\n",
    "    'pls_twostep': PLSTwoStepModel,\n",
    "    'pls_mlpdecoder': PLS_MLPDecoderModel,\n",
    "    'pls_bilineardecoder': PLS_BilinearDecoderModel,\n",
    "    'dynamic_mlp': DynamicMLP,\n",
    "    'shared_mlp_encoder': SharedMLPEncoderModel,\n",
    "    'shared_linear_encoder': SharedLinearEncoderModel,\n",
    "    'shared_transformer': SharedSelfAttentionModel,\n",
    "    'shared_transformer_cls': SharedSelfAttentionCLSModel,\n",
    "    'cross_attention': CrossAttentionModel\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_sweep_torch(config, model_type, train_indices, feature_type, connectome_target, dataset, cv_type, cv_obj, outer_fold_idx, device, sweep_id, model_classes, parcellation, hemisphere, omit_subcortical, gene_list, seed, binarize, impute_strategy, sort_genes, null_model):\n",
    "    \"\"\"\n",
    "    Training function for W&B sweeps for deep learning models.\n",
    "    \n",
    "    Args:\n",
    "        config: W&B sweep configuration\n",
    "        model_type: Type of deep learning model\n",
    "        feature_type: List of feature dictionaries\n",
    "        connectome_target: Target connectome type\n",
    "        cv_type: Type of cross-validation\n",
    "        outer_fold_idx: Current outer fold index\n",
    "        inner_fold_splits: List of inner fold data splits\n",
    "        device: torch device (cuda/cpu)\n",
    "        sweep_id: Current W&B sweep ID\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean validation loss across inner folds\n",
    "    \"\"\"\n",
    "    feature_str = \"+\".join(str(k) if v is None else f\"{k}_{v}\"\n",
    "                         for feat in feature_type \n",
    "                         for k,v in feat.items())\n",
    "    run_name = f\"{model_type}_{feature_str}_{connectome_target}_{cv_type}{seed}_fold{outer_fold_idx}_innerCV\" \n",
    "    run = wandb.init(\n",
    "        project=\"gx2conn\",\n",
    "        name=run_name,\n",
    "        group=f\"sweep_{sweep_id}\",\n",
    "        tags=[\n",
    "            \"inner cross validation\",\n",
    "            f'cv_type_{cv_type}',\n",
    "            f\"fold{outer_fold_idx}\",\n",
    "            f\"model_{model_type}\",\n",
    "            f\"split_{cv_type}{seed}\",\n",
    "            f'feature_type_{feature_str}',\n",
    "            f'target_{connectome_target}',\n",
    "            f\"parcellation_{parcellation}\",\n",
    "            f\"hemisphere_{hemisphere}\",\n",
    "            f\"omit_subcortical_{omit_subcortical}\",\n",
    "            f\"gene_list_{gene_list}\",\n",
    "            f\"binarize_{binarize}\",\n",
    "            f\"impute_strategy_{impute_strategy}\",\n",
    "            f\"sort_genes_{sort_genes}\",\n",
    "            f\"null_model_{null_model}\"\n",
    "        ],\n",
    "        reinit=True\n",
    "    )\n",
    "    sweep_config = wandb.config\n",
    "    \n",
    "    inner_fold_metrics = {'final_train_loss': [], 'final_val_loss': [], \n",
    "                          'final_train_pearson': [], 'final_val_pearson': []}\n",
    "\n",
    "    # Get the appropriate model class\n",
    "    if model_type not in model_classes:\n",
    "        raise ValueError(f\"Model type {model_type} not supported for W&B sweeps\")\n",
    "    \n",
    "    ModelClass = model_classes[model_type]\n",
    "\n",
    "    # Process each inner fold\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(cv_obj.split()):\n",
    "        print(f'Processing inner fold {fold_idx}')\n",
    "        \n",
    "        torch._dynamo.reset()  # Clear compiled graph cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if fold_idx == 0:  # Only run CV on the first inner fold to test more parameters\n",
    "            train_region_pairs = expand_X_symmetric(np.array(train_indices).reshape(-1, 1)).astype(int)\n",
    "            test_region_pairs = expand_X_symmetric(np.array(test_indices).reshape(-1, 1)).astype(int)\n",
    "            train_indices_expanded = np.array([dataset.valid_pair_to_expanded_idx[tuple(pair)] for pair in train_region_pairs])\n",
    "            test_indices_expanded = np.array([dataset.valid_pair_to_expanded_idx[tuple(pair)] for pair in test_region_pairs])    \n",
    "    \n",
    "            # Initialize model dynamically based on sweep config and fit\n",
    "            if 'pls' in model_type:\n",
    "                model = ModelClass(**sweep_config, train_indices=train_indices, test_indices=test_indices, region_pair_dataset=dataset).to(device)\n",
    "            else:\n",
    "                model = ModelClass(**sweep_config).to(device)\n",
    "            \n",
    "            if model_type == 'pls_twostep':\n",
    "                history = model.fit(dataset, train_indices, test_indices)\n",
    "                # Store final metrics\n",
    "                inner_fold_metrics['final_train_loss'].append(history['train_loss'])\n",
    "                inner_fold_metrics['final_val_loss'].append(history['val_loss'])\n",
    "                inner_fold_metrics['final_train_pearson'].append(history['train_pearson'])\n",
    "                inner_fold_metrics['final_val_pearson'].append(history['val_pearson'])\n",
    "            else: \n",
    "                history = model.fit(dataset, train_indices_expanded, test_indices_expanded)\n",
    "                \n",
    "                # Log epoch-wise metrics\n",
    "                for epoch, metrics in enumerate(zip(history['train_loss'], history['val_loss'])):\n",
    "                    wandb.log({\n",
    "                        'inner_fold': fold_idx,\n",
    "                        f'fold{fold_idx}_epoch': epoch,\n",
    "                        f'fold{fold_idx}_train_loss': metrics[0],\n",
    "                        f'fold{fold_idx}_val_loss': metrics[1]\n",
    "                    })\n",
    "                \n",
    "                train_dataset = Subset(dataset, train_indices_expanded)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False, pin_memory=True)\n",
    "                test_dataset = Subset(dataset, test_indices_expanded)\n",
    "                val_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, pin_memory=True)\n",
    "            \n",
    "                predictions, targets = model.predict(train_loader)\n",
    "                train_pearson = pearsonr(predictions, targets)[0]\n",
    "                predictions, targets = model.predict(val_loader)\n",
    "                val_pearson = pearsonr(predictions, targets)[0]\n",
    "\n",
    "                # Store final metrics\n",
    "                inner_fold_metrics['final_train_loss'].append(history['train_loss'][-1])\n",
    "                inner_fold_metrics['final_val_loss'].append(history['val_loss'][-1])\n",
    "                inner_fold_metrics['final_train_pearson'].append(train_pearson)\n",
    "                inner_fold_metrics['final_val_pearson'].append(val_pearson)\n",
    "\n",
    "            # del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Log mean metrics across folds\n",
    "    mean_metrics = {\n",
    "        'mean_train_loss': np.mean(inner_fold_metrics['final_train_loss']),\n",
    "        'mean_val_loss': np.mean(inner_fold_metrics['final_val_loss']),\n",
    "        'mean_train_pearson': np.mean(inner_fold_metrics['final_train_pearson']),\n",
    "        'mean_val_pearson': np.mean(inner_fold_metrics['final_val_pearson'])\n",
    "    }\n",
    "    wandb.log(mean_metrics)\n",
    "    run.finish()\n",
    "    \n",
    "    return mean_metrics['mean_val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_metrics_output(metrics, mode):\n",
    "    \"\"\"Format metrics into organized categories for clean printing\"\"\"\n",
    "    # Define metric categories\n",
    "    global_metrics = ['mse', 'mae', 'r2', 'pearson_r', 'spearman_r', 'geodesic_distance', 'accuracy', 'precision', 'recall', 'f1', 'auc_roc']\n",
    "    distance_metrics = ['short_r', 'mid_r', 'long_r'] \n",
    "    hemispheric_metrics = ['left_hemi_r', 'right_hemi_r', 'inter_hemi_r']\n",
    "    strength_metrics = ['strong_neg_r', 'weak_r', 'strong_pos_r']\n",
    "    graph_theory_metrics = ['geodesic_distance']\n",
    "    network_names = ['Cont', 'Default', 'SalVentAttn', 'Limbic', 'DorsAttn', 'SomMot', 'Vis', 'Subcortical', 'Cerebellum']\n",
    "    \n",
    "    print(f\"{mode.upper()} METRICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Global Metrics\n",
    "    global_found = {k: v for k, v in metrics.items() if k in global_metrics and not np.isnan(v)}\n",
    "    if global_found:\n",
    "        print(\"GLOBAL:\", end=\" \")\n",
    "        metrics_str = []\n",
    "        for k, v in global_found.items():\n",
    "            if k in ['mse', 'mae']:\n",
    "                metrics_str.append(f\"{k}={v:.6f}\")\n",
    "            else:\n",
    "                metrics_str.append(f\"{k}={v:.4f}\")\n",
    "        print(\", \".join(metrics_str))\n",
    "\n",
    "    # Distance, Hemispheric and Strength metrics\n",
    "    distance_found = {k: v for k, v in metrics.items() if k in distance_metrics and not np.isnan(v)}\n",
    "    if distance_found:\n",
    "        print(\"DISTANCE-BASED:\", end=\" \")\n",
    "        print(\", \".join([f\"{k.replace('_r','')}={v:.4f}\" for k,v in distance_found.items()]))\n",
    "    \n",
    "    hemispheric_found = {k: v for k, v in metrics.items() if k in hemispheric_metrics and not np.isnan(v)}\n",
    "    if hemispheric_found:\n",
    "        print(\"HEMISPHERIC:\", end=\" \")\n",
    "        labels = {\"left_hemi\": \"left\", \"right_hemi\": \"right\", \"inter_hemi\": \"inter\"}\n",
    "        print(\", \".join([f\"{labels[k.replace('_r','')]}={v:.4f}\" for k,v in hemispheric_found.items()]))\n",
    "    \n",
    "    strength_found = {k: v for k, v in metrics.items() if k in strength_metrics and not np.isnan(v)}\n",
    "    if strength_found:\n",
    "        print(\"CONNECTION STRENGTH:\", end=\" \")\n",
    "        labels = {\"strong_neg\": \"neg\", \"weak\": \"weak\", \"strong_pos\": \"pos\"}\n",
    "        print(\", \".join([f\"{labels[k.replace('_r','')]}={v:.4f}\" for k,v in strength_found.items()]))\n",
    "\n",
    "    # Network metrics\n",
    "    intra_metrics = {k: v for k, v in metrics.items() if k.startswith('intra_network_') and not np.isnan(v)}\n",
    "    inter_metrics = {k: v for k, v in metrics.items() if k.startswith('inter_network_') and not np.isnan(v)}\n",
    "    \n",
    "    if intra_metrics or inter_metrics:\n",
    "        print(\"NETWORK CORRELATIONS:\")\n",
    "        print(\"  NETWORK      INTRA      INTER\")\n",
    "        print(\"  \" + \"-\" * 30)\n",
    "        for net in network_names:\n",
    "            intra_val = metrics.get(f'intra_network_{net}_r')\n",
    "            inter_val = metrics.get(f'inter_network_{net}_r')\n",
    "            if not (np.isnan(intra_val) if intra_val is not None else True) or \\\n",
    "               not (np.isnan(inter_val) if inter_val is not None else True):\n",
    "                intra_str = f\"{intra_val:.4f}\" if intra_val is not None and not np.isnan(intra_val) else \"N/A\"\n",
    "                inter_str = f\"{inter_val:.4f}\" if inter_val is not None and not np.isnan(inter_val) else \"N/A\"\n",
    "                print(f\"  {net:<10}  {intra_str:>8}  {inter_str:>8}\")\n",
    "\n",
    "    print(f\"{'='*50}\" + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "def compute_distance_metrics(y_true, y_pred, distances):\n",
    "    \"\"\"Compute distance-based correlation metrics\"\"\"\n",
    "    # Handle both tensor and numpy distances\n",
    "    if torch.is_tensor(distances):\n",
    "        distances = distances.cpu().numpy()\n",
    "    else:\n",
    "        distances = getattr(distances, 'get', lambda: distances)()\n",
    "\n",
    "    # Distance range thresholds \n",
    "    dist_33, dist_67 = 175/3, 175*2/3\n",
    "    \n",
    "    # Create masks\n",
    "    short_mask = distances <= dist_33\n",
    "    mid_mask = (distances > dist_33) & (distances <= dist_67)\n",
    "    long_mask = distances > dist_67\n",
    "\n",
    "    # Calculate correlations\n",
    "    metrics = {}\n",
    "    \n",
    "    # Convert masks to numpy if needed\n",
    "    if torch.is_tensor(short_mask):\n",
    "        short_mask = short_mask.cpu().numpy()\n",
    "        mid_mask = mid_mask.cpu().numpy() \n",
    "        long_mask = long_mask.cpu().numpy()\n",
    "\n",
    "    # Use numpy sum to count mask elements\n",
    "    if np.sum(short_mask) > 100:\n",
    "        metrics['short_r'] = pearsonr(y_true[short_mask], y_pred[short_mask])[0]\n",
    "    else:\n",
    "        metrics['short_r'] = np.nan\n",
    "        \n",
    "    if np.sum(mid_mask) > 100:\n",
    "        metrics['mid_r'] = pearsonr(y_true[mid_mask], y_pred[mid_mask])[0]\n",
    "    else:\n",
    "        metrics['mid_r'] = np.nan\n",
    "        \n",
    "    if np.sum(long_mask) > 100:\n",
    "        metrics['long_r'] = pearsonr(y_true[long_mask], y_pred[long_mask])[0]\n",
    "    else:\n",
    "        metrics['long_r'] = np.nan\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def compute_strength_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute correlation metrics based on connection strength ranges.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth values\n",
    "        y_pred: Predicted values\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing correlations for strong negative (<-0.3), \n",
    "        weak (-0.3 to 0.3), and strong positive (>0.3) connections\n",
    "    \"\"\"\n",
    "    # Create masks for different strength ranges\n",
    "    strong_neg_mask = y_true < -0.3\n",
    "    weak_mask = (y_true >= -0.3) & (y_true <= 0.3)\n",
    "    strong_pos_mask = y_true > 0.3\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Calculate correlations for each range if enough samples exist\n",
    "    if np.sum(strong_neg_mask) > 100:\n",
    "        metrics['strong_neg_r'] = pearsonr(y_true[strong_neg_mask], y_pred[strong_neg_mask])[0]\n",
    "    else:\n",
    "        metrics['strong_neg_r'] = np.nan\n",
    "        \n",
    "    if np.sum(weak_mask) > 100:\n",
    "        metrics['weak_r'] = pearsonr(y_true[weak_mask], y_pred[weak_mask])[0]\n",
    "    else:\n",
    "        metrics['weak_r'] = np.nan\n",
    "        \n",
    "    if np.sum(strong_pos_mask) > 100:\n",
    "        metrics['strong_pos_r'] = pearsonr(y_true[strong_pos_mask], y_pred[strong_pos_mask])[0]\n",
    "    else:\n",
    "        metrics['strong_pos_r'] = np.nan\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def compute_hemispheric_metrics(y_true, y_pred, indices, coords):\n",
    "    \"\"\"Compute hemisphere-based correlation metrics\"\"\"\n",
    "    left_true, left_pred = [], []\n",
    "    right_true, right_pred = [], []\n",
    "    inter_true, inter_pred = [], []\n",
    "    \n",
    "    for idx, (i, j) in enumerate(combinations(indices, 2)):\n",
    "        # Get x coordinates for both regions\n",
    "        x_i = coords[i][0]\n",
    "        x_j = coords[j][0]\n",
    "        \n",
    "        # Determine hemisphere based on x coordinates\n",
    "        if x_i < 0 and x_j < 0:  # Left-left\n",
    "            left_true.append(y_true[2*idx])\n",
    "            left_pred.append(y_pred[2*idx])\n",
    "        elif x_i > 0 and x_j > 0:  # Right-right\n",
    "            right_true.append(y_true[2*idx])\n",
    "            right_pred.append(y_pred[2*idx])\n",
    "        else:  # Inter-hemispheric\n",
    "            inter_true.append(y_true[2*idx])\n",
    "            inter_pred.append(y_pred[2*idx])\n",
    "\n",
    "    metrics = {}\n",
    "    if len(left_true) > 1:\n",
    "        metrics['left_hemi_r'] = pearsonr(left_true, left_pred)[0]\n",
    "    if len(right_true) > 1:\n",
    "        metrics['right_hemi_r'] = pearsonr(right_true, right_pred)[0]\n",
    "    if len(inter_true) > 1:\n",
    "        metrics['inter_hemi_r'] = pearsonr(inter_true, inter_pred)[0]\n",
    "        \n",
    "    return metrics\n",
    "    \n",
    "def compute_subnetwork_metrics(y_true, y_pred, indices, network_labels, shared_indices=None):\n",
    "    \"\"\"\n",
    "    Compute network-based correlation metrics for both intra and inter-network connections.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True connectivity values\n",
    "        y_pred: Predicted connectivity values \n",
    "        indices: List of region indices\n",
    "        network_labels: Network label for each region\n",
    "        shared_indices: Optional list of shared region indices to include connections with\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with correlations for:\n",
    "        - Intra-network connections within each network\n",
    "        - Inter-network connections between networks\n",
    "    \"\"\"\n",
    "    networks = ['Cont', 'Default', 'SalVentAttn', 'Limbic', \n",
    "               'DorsAttn', 'SomMot', 'Vis', 'Subcortical', 'Cerebellum']\n",
    "    \n",
    "    # Initialize data structures for intra and inter-network connections\n",
    "    intra_network_data = {net: {'true': [], 'pred': []} for net in networks}\n",
    "    inter_network_data = {net: {'true': [], 'pred': []} for net in networks}\n",
    "\n",
    "    # Helper function to process a pair of regions and their values\n",
    "    def process_region_pair(net_i, net_j, true_val, pred_val):\n",
    "        # Skip if either network label is not in our list\n",
    "        if net_i not in networks or net_j not in networks:\n",
    "            return\n",
    "            \n",
    "        # Add to appropriate network data\n",
    "        if net_i == net_j: # Intra-network connections\n",
    "            intra_network_data[net_i]['true'].append(true_val)\n",
    "            intra_network_data[net_i]['pred'].append(pred_val)\n",
    "        else: # Inter-network connections\n",
    "            for net in [net_i, net_j]:\n",
    "                inter_network_data[net]['true'].append(true_val)\n",
    "                inter_network_data[net]['pred'].append(pred_val)\n",
    "\n",
    "    # Process connections between indices\n",
    "    n_pairs = len(list(combinations(indices, 2)))\n",
    "    for idx, (i, j) in enumerate(combinations(indices, 2)):\n",
    "        net_i, net_j = network_labels[i], network_labels[j]\n",
    "        process_region_pair(net_i, net_j, y_true[idx], y_pred[idx])\n",
    "\n",
    "    # Process connections with shared indices if provided\n",
    "    if shared_indices is not None:\n",
    "        offset = n_pairs\n",
    "        for i_idx, i in enumerate(indices):\n",
    "            for j_idx, j in enumerate(shared_indices):\n",
    "                idx = offset + i_idx * len(shared_indices) + j_idx\n",
    "                if idx < len(y_true):  # Ensure index is within bounds\n",
    "                    net_i, net_j = network_labels[i], network_labels[j]\n",
    "                    process_region_pair(net_i, net_j, y_true[idx], y_pred[idx])\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Calculate intra-network correlations\n",
    "    for network in networks:\n",
    "        if len(intra_network_data[network]['true']) > 1:\n",
    "            metrics[f'intra_network_{network}_r'] = pearsonr(\n",
    "                intra_network_data[network]['true'],\n",
    "                intra_network_data[network]['pred'])[0]\n",
    "        else:\n",
    "            metrics[f'intra_network_{network}_r'] = np.nan\n",
    "    \n",
    "    # Calculate inter-network correlations  \n",
    "    for network in networks:\n",
    "        if len(inter_network_data[network]['true']) > 1:\n",
    "            metrics[f'inter_network_{network}_r'] = pearsonr(\n",
    "                inter_network_data[network]['true'],\n",
    "                inter_network_data[network]['pred'])[0]\n",
    "        else:\n",
    "            metrics[f'inter_network_{network}_r'] = np.nan\n",
    "            \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892f901e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Filter for pls_bilineardecoder runs\u001b[39;00m\n\u001b[1;32m     15\u001b[0m pls_bilinear_runs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m runs:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Check if this run is for pls_bilineardecoder model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpls_bilineardecoder\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tag \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m run\u001b[38;5;241m.\u001b[39mtags) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpls_bilineardecoder\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m run\u001b[38;5;241m.\u001b[39mname:\n\u001b[1;32m     19\u001b[0m         pls_bilinear_runs\u001b[38;5;241m.\u001b[39mappend(run)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/apis/paginator.py:75\u001b[0m, in \u001b[0;36mPaginator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_page():\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/apis/paginator.py:62\u001b[0m, in \u001b[0;36mPaginator._load_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_variables()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQUERY, variable_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_objects())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/apis/public/runs.py:155\u001b[0m, in \u001b[0;36mRuns.convert_objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m     sweep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sweeps[run\u001b[38;5;241m.\u001b[39msweep_name]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     sweep \u001b[38;5;241m=\u001b[39m public\u001b[38;5;241m.\u001b[39mSweep\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient,\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject,\n\u001b[1;32m    159\u001b[0m         run\u001b[38;5;241m.\u001b[39msweep_name,\n\u001b[1;32m    160\u001b[0m         withRuns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sweeps[run\u001b[38;5;241m.\u001b[39msweep_name] \u001b[38;5;241m=\u001b[39m sweep\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sweep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/apis/public/sweeps.py:197\u001b[0m, in \u001b[0;36mSweep.get\u001b[0;34m(cls, client, entity, project, sid, order, query, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mexecute(query, variable_values\u001b[38;5;241m=\u001b[39mvariables)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Don't handle exception, rely on legacy query\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# TODO(gst): Implement updated introspection workaround\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mLEGACY_QUERY\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/sdk/lib/retry.py:212\u001b[0m, in \u001b[0;36mretriable.<locals>.decorator.<locals>.wrapped_fn\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrier(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/sdk/lib/retry.py:131\u001b[0m, in \u001b[0;36mRetry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;66;03m# Only print resolved attempts once every minute\u001b[39;00m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m now \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_print \u001b[38;5;241m>\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(\n\u001b[1;32m    134\u001b[0m             minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    135\u001b[0m         ):\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/apis/public/api.py:71\u001b[0m, in \u001b[0;36mRetryingClient.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@retry\u001b[39m\u001b[38;5;241m.\u001b[39mretriable(\n\u001b[1;32m     65\u001b[0m     retry_timedelta\u001b[38;5;241m=\u001b[39mRETRY_TIMEDELTA,\n\u001b[1;32m     66\u001b[0m     check_retry_fn\u001b[38;5;241m=\u001b[39mutil\u001b[38;5;241m.\u001b[39mno_retry_auth,\n\u001b[1;32m     67\u001b[0m     retryable_exceptions\u001b[38;5;241m=\u001b[39m(RetryError, requests\u001b[38;5;241m.\u001b[39mRequestException),\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: D102  # User not encouraged to use this class directly\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mReadTimeout:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py:52\u001b[0m, in \u001b[0;36mClient.execute\u001b[0;34m(self, document, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(document)\n\u001b[0;32m---> 52\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_result(document, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merrors:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(result\u001b[38;5;241m.\u001b[39merrors[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py:60\u001b[0m, in \u001b[0;36mClient._get_result\u001b[0;34m(self, document, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, document, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport\u001b[38;5;241m.\u001b[39mexecute(document, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     62\u001b[0m     last_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     retries_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/sdk/lib/gql_request.py:58\u001b[0m, in \u001b[0;36mGraphQLSession.execute\u001b[0;34m(self, document, variable_values, timeout)\u001b[0m\n\u001b[1;32m     51\u001b[0m data_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_json \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m post_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcookies\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_timeout,\n\u001b[1;32m     56\u001b[0m     data_key: payload,\n\u001b[1;32m     57\u001b[0m }\n\u001b[0;32m---> 58\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpost_args)\n\u001b[1;32m     59\u001b[0m request\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     61\u001b[0m result \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:812\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clean_exit:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# We hit some kind of exception, handled or otherwise. We need\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;66;03m# to throw the connection away unless explicitly told not to.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;66;03m# Close the connection, set the variable to None, and make sure\u001b[39;00m\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;66;03m# we put the None back in the pool to avoid leaking it.\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m         conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;129;01mand\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    813\u001b[0m         release_this_conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m release_this_conn:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Put the connection back to be reused. If the connection is\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# expired then it will be None, which will get replaced with a\u001b[39;00m\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;66;03m# fresh connection during _get_conn.\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/http/client.py:964\u001b[0m, in \u001b[0;36mHTTPConnection.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sock:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 964\u001b[0m         sock\u001b[38;5;241m.\u001b[39mclose()   \u001b[38;5;66;03m# close it manually... there may be other refs\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__response\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/socket.py:503\u001b[0m, in \u001b[0;36msocket.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_refs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_real_close()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/ssl.py:1370\u001b[0m, in \u001b[0;36mSSLSocket._real_close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_real_close()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Access your project\n",
    "project = \"alexander-ratzan-new-york-university/gx2conn\"\n",
    "\n",
    "# Get all runs from the project\n",
    "runs = api.runs(project)\n",
    "\n",
    "# Filter for pls_bilineardecoder runs\n",
    "pls_bilinear_runs = []\n",
    "for run in runs:\n",
    "    # Check if this run is for pls_bilineardecoder model\n",
    "    if any('pls_bilineardecoder' in tag for tag in run.tags) or 'pls_bilineardecoder' in run.name:\n",
    "        pls_bilinear_runs.append(run)\n",
    "\n",
    "print(f\"Found {len(pls_bilinear_runs)} pls_bilineardecoder runs\")\n",
    "\n",
    "# # Create a list to store run information\n",
    "# run_data = []\n",
    "\n",
    "# for run in pls_bilinear_runs:\n",
    "#     # Get run summary metrics\n",
    "#     summary = run.summary\n",
    "    \n",
    "#     # Extract Pearson correlation metrics (try different possible metric names)\n",
    "#     pearson_metrics = {}\n",
    "#     possible_pearson_keys = [\n",
    "#         'mean_val_pearson', 'val_pearson', 'pearson_r', \n",
    "#         'mean_train_pearson', 'train_pearson', 'final_val_pearson'\n",
    "#     ]\n",
    "    \n",
    "#     for key in possible_pearson_keys:\n",
    "#         if key in summary:\n",
    "#             pearson_metrics[key] = summary[key]\n",
    "    \n",
    "#     # Use the best available pearson metric (prioritize validation metrics)\n",
    "#     best_pearson = None\n",
    "#     metric_name = None\n",
    "    \n",
    "#     # Priority order: validation > mean > train > final\n",
    "#     priority_order = ['mean_val_pearson', 'val_pearson', 'pearson_r', 'final_val_pearson', 'mean_train_pearson', 'train_pearson']\n",
    "    \n",
    "#     for metric in priority_order:\n",
    "#         if metric in pearson_metrics and not np.isnan(pearson_metrics[metric]):\n",
    "#             best_pearson = pearson_metrics[metric]\n",
    "#             metric_name = metric\n",
    "#             break\n",
    "    \n",
    "#     if best_pearson is not None:\n",
    "#         run_info = {\n",
    "#             'run_id': run.id,\n",
    "#             'run_name': run.name,\n",
    "#             'best_pearson': best_pearson,\n",
    "#             'metric_used': metric_name,\n",
    "#             'state': run.state,\n",
    "#             'created_at': run.created_at,\n",
    "#             'config': dict(run.config),\n",
    "#             'all_pearson_metrics': pearson_metrics,\n",
    "#             'tags': run.tags\n",
    "#         }\n",
    "#         run_data.append(run_info)\n",
    "\n",
    "# # Sort by best pearson correlation (descending)\n",
    "# run_data.sort(key=lambda x: x['best_pearson'], reverse=True)\n",
    "\n",
    "# print(f\"\\nTop 10 pls_bilineardecoder models by Pearson correlation:\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# for i, run_info in enumerate(run_data[:10]):\n",
    "#     print(f\"\\nRank {i+1}:\")\n",
    "#     print(f\"  Run ID: {run_info['run_id']}\")\n",
    "#     print(f\"  Run Name: {run_info['run_name']}\")\n",
    "#     print(f\"  Best Pearson: {run_info['best_pearson']:.4f} ({run_info['metric_used']})\")\n",
    "#     print(f\"  State: {run_info['state']}\")\n",
    "#     print(f\"  Created: {run_info['created_at']}\")\n",
    "    \n",
    "#     # Show key config parameters\n",
    "#     config = run_info['config']\n",
    "#     print(f\"  Config highlights:\")\n",
    "#     for key in ['learning_rate', 'batch_size', 'n_components', 'hidden_dim', 'dropout']:\n",
    "#         if key in config:\n",
    "#             print(f\"    {key}: {config[key]}\")\n",
    "    \n",
    "#     # Show all pearson metrics for this run\n",
    "#     print(f\"  All Pearson metrics: {run_info['all_pearson_metrics']}\")\n",
    "\n",
    "# # Get the best run for detailed analysis\n",
    "# if run_data:\n",
    "#     best_run_info = run_data[0]\n",
    "#     print(f\"\\n\" + \"=\" * 80)\n",
    "#     print(f\"BEST MODEL DETAILS:\")\n",
    "#     print(f\"=\" * 80)\n",
    "    \n",
    "#     # Access the actual run object\n",
    "#     best_run = api.run(f\"{project}/{best_run_info['run_id']}\")\n",
    "    \n",
    "#     print(f\"Run ID: {best_run_info['run_id']}\")\n",
    "#     print(f\"Run Name: {best_run_info['run_name']}\")\n",
    "#     print(f\"Best Pearson: {best_run_info['best_pearson']:.4f}\")\n",
    "#     print(f\"State: {best_run_info['state']}\")\n",
    "    \n",
    "#     print(f\"\\nFull Configuration:\")\n",
    "#     for key, value in best_run_info['config'].items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "    \n",
    "#     print(f\"\\nAll Summary Metrics:\")\n",
    "#     for key, value in best_run.summary.items():\n",
    "#         if not key.startswith('_'):  # Skip internal wandb metrics\n",
    "#             print(f\"  {key}: {value}\")\n",
    "    \n",
    "#     # Download model artifacts if available\n",
    "#     print(f\"\\nAvailable Files:\")\n",
    "#     for file in best_run.files():\n",
    "#         print(f\"  - {file.name}\")\n",
    "    \n",
    "#     # Option to download the best model\n",
    "#     print(f\"\\nTo download the best model, run:\")\n",
    "#     print(f\"best_run = api.run('{project}/{best_run_info['run_id']}')\")\n",
    "#     print(f\"# Download specific files:\")\n",
    "#     print(f\"# best_run.file('model.pth').download()\")\n",
    "# else:\n",
    "#     print(\"No pls_bilineardecoder runs found with Pearson correlation metrics.\")\n",
    "\n",
    "# # Create a DataFrame for easier analysis\n",
    "# if run_data:\n",
    "#     df_data = []\n",
    "#     for run_info in run_data:\n",
    "#         row = {\n",
    "#             'run_id': run_info['run_id'],\n",
    "#             'run_name': run_info['run_name'],\n",
    "#             'best_pearson': run_info['best_pearson'],\n",
    "#             'metric_used': run_info['metric_used'],\n",
    "#             'state': run_info['state'],\n",
    "#             'created_at': run_info['created_at']\n",
    "#         }\n",
    "#         # Add config parameters as separate columns\n",
    "#         for key, value in run_info['config'].items():\n",
    "#             row[f'config_{key}'] = value\n",
    "        \n",
    "#         df_data.append(row)\n",
    "    \n",
    "#     df = pd.DataFrame(df_data)\n",
    "#     print(f\"\\nDataFrame created with {len(df)} runs\")\n",
    "#     print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "#     # Save to CSV for further analysis\n",
    "#     df.to_csv('pls_bilineardecoder_results.csv', index=False)\n",
    "#     print(f\"Results saved to 'pls_bilineardecoder_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec148a0-998f-4cfb-b29d-61e37b6cb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "project_path = \"alexander-ratzan-new-york-university/gx2conn\"\n",
    "\n",
    "# Get timestamp for 7 days ago (you can change this to 30, etc.)\n",
    "seven_days_ago = datetime.now() - timedelta(days=7)\n",
    "\n",
    "# W&B API requires ISO format string\n",
    "timestamp_filter = {\"$gte\": seven_days_ago.isoformat()}\n",
    "\n",
    "# Build the filter\n",
    "filters = {\n",
    "    \"config.model_type\": \"pls_bilineardecoder\",\n",
    "    \"created_at\": timestamp_filter,\n",
    "    \"state\": \"finished\"  # Optional\n",
    "}\n",
    "\n",
    "# Fetch the filtered runs (can limit for efficiency)\n",
    "runs = api.runs(project_path, filters=filters, order=\"-created_at\", per_page=10)\n",
    "\n",
    "# Print result\n",
    "for i, run in enumerate(runs):\n",
    "    print(f\"{i + 1}. Run ID: {run.id} | Created: {run.created_at} | Name: {run.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bf0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
