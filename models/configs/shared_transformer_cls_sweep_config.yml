name: shared_transformer_cls
method: random
metric:
  name: mean_val_loss
  goal: minimize

parameters: 
  # input_dim:
  #   value: !!int 0
  # token_encoder_dim:
  #   values: [10, 20, 30] # 60
  # d_model:
  #   values: [64, 128]
  # encoder_output_dim:
  #   values: [10]
  # use_alibi:
  #   values: [True, False]
  # use_attention_pooling:
  #   values: [False]
  # nhead:
  #   values: [2, 4]
  # num_layers:
  #   values: [2, 4]
  # deep_hidden_dims:
  #   values:
  #    - [256, 128]
  #    - [512, 256, 128]
  # cls_init:
  #   values: ['spatial_learned']
  # transformer_dropout:
  #   values: [0.1, 0.2]
  # dropout_rate:
  #   values: [0.1, 0.2]
  # learning_rate:
  #   values: [0.00009, 0.0001] # this has an interplay with token_encoder_dim 
  # weight_decay:
  #   values: [0.0001, .001]
  # batch_size:
  #   values: [1024]
  # aug_prob:
  #   values: [0, 0.45]
  # epochs:
  #   values: [70, 90, 110]
  # num_workers:
  #   values: [2]
  # prefetch_factor:
  #   values: [4]
  input_dim:
     value: !!int 0
  token_encoder_dim:
    value: 20
  d_model:
    value: 128
  encoder_output_dim:
    value: 10
  use_alibi:
    value: True
  use_attention_pooling:
    value: True
  nhead:
    value: 4
  num_layers:
    value: 4
  deep_hidden_dims:
    value: [512, 256, 128]
  transformer_dropout:
    value: 0.2
  dropout_rate:
    value: 0.2
  learning_rate:
    values: [0.01, .001, .0001, .00001, 0.000001]
  weight_decay:
    value: 0.00001
  batch_size:
    value: 1024
  aug_prob:
    value: 0.3
  epochs:
    value: 100
  num_workers:
    value: 2
  prefetch_factor:
    value: 4
    
best_parameters:
  input_dim:
    value: !!int 0
  token_encoder_dim:
    values: [20]
  d_model:
    values: [128]
  encoder_output_dim:
    values: [10]
  use_alibi:
    values: [True]
  use_attention_pooling:
    values: [True]
  nhead:
    values: [4]
  num_layers:
    values: [4]
  deep_hidden_dims:
    values:
      - [512, 256, 128]
  cls_init:
    values: ['spatial_learned']
  transformer_dropout:
    values: [0.2]
  dropout_rate:
    values: [0.2]
  learning_rate:
    values: [0.00009]
  weight_decay:
    values: [0.0001]
  batch_size:
    values: [1024]
  aug_prob:
    values: [0.3]
  epochs:
    values: [100]
  num_workers:
    values: [2]
  prefetch_factor:
    values: [4]