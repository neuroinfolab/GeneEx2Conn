name: cross_attention_gene2vec
method: random
metric:
  name: mean_val_loss
  goal: minimize

parameters:
  input_dim:
    value: !!int 0
  expression_bins:       # Number of expression bins per gene
    values: [5]
  d_model:              # Transformer embedding dimension
    values: [16, 32, 64, 128]
  nhead:                # Number of attention heads
    values: [2, 4]
  num_layers:           # Number of cross-attention layers
    values: [2, 4]
  deep_hidden_dims:     # MLP head dimensions
    values:
      - [512, 256, 128]
  genevec_type:
    values: ['gene2vec', 'coexpression', 'one_hot']
  cls_init:             # CLS token initialization
    values: ['random']
  pooling_mode:         # Output pooling method
    values: ['mean', 'attention', 'linear']
  use_alibi:            # Use ALiBi positional biases
    values: [True]
  transformer_dropout:  # Dropout in attention blocks
    values: [0.1, 0.3]
  dropout_rate:        # Dropout in MLP head
    values: [0.1, 0.3]
  learning_rate:
    values: [0.0001, .00008, 0.00005]
  weight_decay:
    values: [0.00001, 0.0001, 0.001]
  batch_size:
    values: [512]
  epochs:
    values: [100, 150, 200]
  aug_prob:            # Data augmentation probability
    values: [0.0, 0.1, 0.3]
  aug_style:
    values: ['curriculum_swap_linear_decay']
  lambda_sym:
    values: [.0001, .00001, 0]
  num_workers:
    values: [2]
  prefetch_factor:
    values: [4]
  cosine_lr:           # Use cosine learning rate schedule
    values: [True, False]

best_parameters:
  input_dim:
    value: !!int 0
  expression_bins:
    values: [5]
  genevec_type:
    values: ['gene2vec'] # 'gene2vec', 'coexpression', 'one_hot'
  d_model:
    values: [32]
  nhead:
    values: [2]
  num_layers:
    values: [2]
  deep_hidden_dims:
    values:
      - [512, 256, 128]
  cls_init:
    values: ['random']
  pooling_mode:
    values: ['attention'] # stick to mean for now - same performance and fewest parameters
  use_alibi:
    values: [True]
  transformer_dropout:
    values: [0.2]
  dropout_rate:
    values: [0.2]
  learning_rate:
    values: [0.00008]
  weight_decay:
    values: [0.0001]
  batch_size:
    values: [512]
  epochs:
    values: [200]
  aug_prob:
    values: [0.1]
  aug_style:
    values: ['curriculum_swap_linear_decay']
  lambda_sym:
    values: [0.0001]
  num_workers:
    values: [2]
  prefetch_factor:
    values: [4]
  cosine_lr:
    values: [False]