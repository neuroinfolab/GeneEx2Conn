name: shared_transformer
method: random
metric:
  name: mean_val_loss
  goal: minimize

parameters: 
  input_dim:
    value: !!int 0
  token_encoder_dim:
    values: [20, 60]
  d_model:
    values: [64, 128]
  encoder_output_dim:
    values: [5, 10]
  use_alibi:
    values: [True]
  nhead:
    values: [2, 4]
  num_layers:
    values: [2, 4]
  deep_hidden_dims:
    values:
    - [256, 128]
    - [512, 256, 128]
  transformer_dropout:
    values: [0.1, 0.2]
  dropout_rate:
    values: [0.1, 0.2, 0.3]
  learning_rate:
    values: [0.00009, 0.0001]
  weight_decay:
    values: [0.0001, 0.001]
  batch_size:
    values: [512, 1024]
  aug_prob:
    values: [0, 0.1, 0.3]
  aug_style:
    values: ['linear_decay', 'curriculum_swap_constant', 'curriculum_swap_linear_decay']
  epochs:
    values: [90, 110, 130]
  num_workers:
    values: [2]
  prefetch_factor:
    values: [4]

best_parameters:
  input_dim:
    value: !!int 0
  token_encoder_dim:
    values: [20]
  d_model:
    values: [128]
  encoder_output_dim:
    values: [10]
  use_alibi:
    values: [True]
  nhead:
    values: [4]
  num_layers:
    values: [4]
  deep_hidden_dims:
    values:
      - [512, 256, 128]
  transformer_dropout:
    values: [0.2]
  dropout_rate:
    values: [0.3]
  learning_rate:
    values: [0.0001] # usually 0.0001 for 20, .00009 for 60/180
  weight_decay:
    values: [0.0001]
  batch_size:
    values: [512] # usually 1024 but 512 for swap 
  aug_prob:
    values: [0.3]
  aug_style:
    values: ['curriculum_swap_linear_decay']
  epochs:
    values: [110]
  num_workers:
    values: [2]
  prefetch_factor:
    values: [4]