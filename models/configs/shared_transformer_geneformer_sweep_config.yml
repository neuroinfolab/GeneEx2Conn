name: shared_transformer_geneformer
method: random
metric:
  name: mean_val_loss
  goal: minimize

parameters: 
  input_dim:
    value: !!int 0
  token_encoder_dim:
    values: [1]  # Fixed at 1 for scalar Geneformer embeddings
  d_model:
    values: [64, 128, 256]
  use_alibi:
    values: [False]
  nhead:
    values: [2, 4]
  num_layers:
    values: [2, 4]
  deep_hidden_dims:
    values:
    - [512, 256, 128]
  transformer_dropout:
    values: [0.1, 0.2]
  dropout_rate:
    values: [0.1, 0.2]
  learning_rate:
    values: [0.0003, 0.0005, 0.001, .00009, .00001]
  weight_decay:
    values: [0, 0.00001, 0.0001]
  batch_size:
    values: [1024]
  aug_prob:
    values: [0] #, 0.3, 0.45]
  epochs:
    values: [110, 130]
  num_workers:
    value: 2
  prefetch_factor:
    value: 4
  use_attention_pooling:
    values: [True, False]  # Default False, but test both
  use_mlp_downsampler:
    values: [True, False]  # Default False (linear), but test MLP too

best_parameters:
  input_dim:
    value: !!int 0
  token_encoder_dim:
    values: [1]
  d_model:
    values: [128]
  use_alibi:
    values: [False] 
  nhead:
    values: [4]
  num_layers:
    values: [4]
  deep_hidden_dims:
    values:
      - [512, 256, 128]
  transformer_dropout:
    values: [0.1]
  dropout_rate:
    values: [0.1]
  learning_rate:
    values: [0.0005]
  weight_decay:
    values: [0.00001]
  batch_size:
    values: [1024]
  aug_prob:
    values: [0.0]
  epochs:
    values: [200]
  num_workers:
    values: [2]
  prefetch_factor:
    values: [4]
  use_attention_pooling:
    values: [False]  # Default setting
  use_mlp_downsampler:
    values: [True]  # Default linear downsampler
