name: shared_mlp_encoder
method: random
metric:
  name: mean_val_loss
  goal: minimize

parameters: 
  input_dim:
    value: !!int 0
  encoder_hidden_dim:
    distribution: int_uniform
    min: 128
    max: 512
  encoder_output_dim:
    distribution: int_uniform
    min: 8
    max: 32
  deep_hidden_dims:
    distribution: categorical
    values:
      - [32]
  use_bilinear:
    distribution: categorical
    values: [True]
  dropout_rate:
    distribution: categorical
    values: [0.0, 0.1, 0.2, 0.3]
  learning_rate:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.01
  weight_decay:
    distribution: categorical
    values: [0.0, 0.0001, 0.001, 0.01]
  lambda_reg:
    distribution: categorical
    values: [0.0, 0.0001, 0.001, 0.01]
  batch_size:
    distribution: int_uniform
    min: 64
    max: 256
  epochs:
    distribution: int_uniform
    min: 100
    max: 200

# SMALL GRID 
input_dim:
  value: !!int 0
encoder_hidden_dim:
  values: [128, 256]
encoder_output_dim:
  values: [4, 8, 16, 32]
deep_hidden_dims:
  values:
    - [0]
use_bilinear:
  values: [True]
dropout_rate:
  values: [0.0, 0.1]
learning_rate:
  values: [0.0001, 0.0005]
weight_decay:
  values: [0.0, 0.0001]
lambda_reg:
  values: [0.0, 0.0001]
batch_size:
  values: [128]
epochs:
  values: [150]

# BEST
best_parameters: # best params - 01/28
  input_dim:
      value: !!int 0
  encoder_hidden_dim:
    values: [256]
  encoder_output_dim:
    values: [16]
  deep_hidden_dims:
    values: 
    - [32]
  use_bilinear:
    values: [True]
  dropout_rate:
    values: [0.0]
  learning_rate:
    values: [0.0001]
  weight_decay:
    values: [0.000] 
  lambda_reg:
    values: [0.000] # keep this as an option for bayesian
  batch_size:
    values: [128]
  epochs:
    values: [150]