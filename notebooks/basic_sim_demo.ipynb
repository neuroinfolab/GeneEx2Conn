{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98363b8f-1d61-429d-865a-3d8213cf3406",
   "metadata": {},
   "source": [
    "## Sim Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83562164-b395-4948-85bd-0cc2a4d2c0e9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25060b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b29dd74e-acf1-4701-bb72-ce3701d3d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "566563ca-070f-4671-a98a-338fea600261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall imports\n",
    "import importlib\n",
    "import data\n",
    "\n",
    "import sim.sim \n",
    "import sim.sim_utils\n",
    "from sim.sim_utils import bytes2human, print_system_usage\n",
    "from sim.sim import Simulation\n",
    "from sim.sim_run import single_sim_run, open_pickled_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa8bad-5d12-46eb-a39f-18057f3923d7",
   "metadata": {},
   "source": [
    "#### Check job specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b7c8d67-a18c-4208-b20f-8ba13fd58e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 44.4%\n",
      "RAM Usage: 7.1%\n",
      "Available RAM: 935.1G\n",
      "Total RAM: 1007.0G\n",
      "52.4G\n"
     ]
    }
   ],
   "source": [
    "print_system_usage()\n",
    "\n",
    "total = psutil.disk_usage('/').total\n",
    "print(bytes2human(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c698e8f8-7439-42bb-9ada-353b9b9648f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 2.0.3\n",
      "cupy version: 13.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost version:\", xgboost.__version__)\n",
    "print(\"cupy version:\", cp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b5cf80d-8efb-464a-9f46-69f4ecae5ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPUtil.getAvailable()\n",
    "# if a number is seen a GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6a6a997-879a-4f47-97ec-03355acc2b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found 0\n"
     ]
    }
   ],
   "source": [
    "GPUtil.getGPUs()\n",
    "\n",
    "DEVICE_ID_LIST = GPUtil.getFirstAvailable()\n",
    "DEVICE_ID = DEVICE_ID_LIST[0] # grab first element from list\n",
    "if DEVICE_ID != None: \n",
    "    print('GPU found', DEVICE_ID)\n",
    "    use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24bc64b4-6659-494f-9962-5915259a50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 20% |\n"
     ]
    }
   ],
   "source": [
    "GPUtil.showUtilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9658fa0e-ab6d-415e-8445-47b1ddfa0d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff851b71-71c9-4435-9276-e3736858e2d9",
   "metadata": {},
   "source": [
    "#### Simulation tests <a id=\"sims\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2d464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='dynamic_nn',\n",
    "              feature_type=[{'transcriptome': 'PCA'}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('grid', 'mse'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfdac05-099a-4eb9-9101-af7344c6d0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name euclidean\n",
      "processing_type None\n",
      "X shape (114, 3)\n",
      "\n",
      " Test fold num: 1\n",
      "(12070, 6) (12070,) (812, 6) (812,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "2\n",
      "3\n",
      "4\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.027 total time=   0.0s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.02781618267591412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masratzan\u001b[0m (\u001b[33malexander-ratzan-new-york-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_094546-7q6mhbni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/7q6mhbni' target=\"_blank\">ridge_euclidean_predFC_random_fold0_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/7q6mhbni' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/7q6mhbni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>-0.02782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ridge_euclidean_predFC_random_fold0_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/7q6mhbni' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/7q6mhbni</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_094546-7q6mhbni/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "\n",
      "Train Metrics: {'mse': 0.028845057258233976, 'mae': 0.13149863132294037, 'r2': 0.13711199810523578, 'pearson_corr': 0.3702863742123049}\n",
      "Test Metrics: {'mse': 0.024882765307924614, 'mae': 0.12295614316783007, 'r2': 0.1630024064256036, 'pearson_corr': 0.42883380456139925, 'connectome_corr': 0.2823952614184139, 'connectome_r2': -4.282912784740204, 'geodesic_distance': 6.561954847541638}\n",
      "BEST VAL SCORE -0.02781618267591412\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 31.9%\n",
      "RAM Usage: 5.3%\n",
      "Available RAM: 953.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "\n",
      " Test fold num: 2\n",
      "(12070, 6) (12070,) (812, 6) (812,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "3\n",
      "4\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.027 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.026 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.027 total time=   0.0s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 0, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.026945869012446503\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_094550-odx7fwxh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/odx7fwxh' target=\"_blank\">ridge_euclidean_predFC_random_fold1_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/odx7fwxh' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/odx7fwxh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>-0.02695</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ridge_euclidean_predFC_random_fold1_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/odx7fwxh' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/odx7fwxh</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_094550-odx7fwxh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "\n",
      "Train Metrics: {'mse': 0.028657961127511768, 'mae': 0.131073186442198, 'r2': 0.12882930321167874, 'pearson_corr': 0.3589279916803351}\n",
      "Test Metrics: {'mse': 0.02754606515989599, 'mae': 0.12665806772650975, 'r2': 0.2726069368042884, 'pearson_corr': 0.5293168126205281, 'connectome_corr': 0.361093469044526, 'connectome_r2': -14.798010809249478, 'geodesic_distance': 6.868695882944916}\n",
      "BEST VAL SCORE -0.026945869012446503\n",
      "BEST MODEL PARAMS {'alpha': 0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 31.9%\n",
      "RAM Usage: 5.3%\n",
      "Available RAM: 953.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "\n",
      " Test fold num: 3\n",
      "(12126, 6) (12126,) (756, 6) (756,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "2\n",
      "4\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   0.0s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.027168831982130794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_094553-ha5ooez9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/ha5ooez9' target=\"_blank\">ridge_euclidean_predFC_random_fold2_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/ha5ooez9' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/ha5ooez9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>-0.02717</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ridge_euclidean_predFC_random_fold2_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/ha5ooez9' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/ha5ooez9</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_094553-ha5ooez9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "\n",
      "Train Metrics: {'mse': 0.028683523809289957, 'mae': 0.13111115307900528, 'r2': 0.13913146401454346, 'pearson_corr': 0.37300330410324356}\n",
      "Test Metrics: {'mse': 0.027077622998804823, 'mae': 0.12142571510515247, 'r2': 0.13593317511949277, 'pearson_corr': 0.39326999869588375, 'connectome_corr': 0.2099200137040555, 'connectome_r2': -0.2921565293186986, 'geodesic_distance': 6.5480889316219235}\n",
      "BEST VAL SCORE -0.027168831982130794\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 31.9%\n",
      "RAM Usage: 5.3%\n",
      "Available RAM: 953.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "\n",
      " Test fold num: 4\n",
      "(12126, 6) (12126,) (756, 6) (756,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "2\n",
      "3\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.029 total time=   0.0s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.025 total time=   0.0s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   0.0s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.029 total time=   0.0s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.027513927619089013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_094556-jkknnafh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/jkknnafh' target=\"_blank\">ridge_euclidean_predFC_random_fold3_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/jkknnafh' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/jkknnafh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>-0.02751</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ridge_euclidean_predFC_random_fold3_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/jkknnafh' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/jkknnafh</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_094556-jkknnafh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "\n",
      "Train Metrics: {'mse': 0.028742649467198428, 'mae': 0.1312452749281661, 'r2': 0.13804170515059944, 'pearson_corr': 0.3715396425479373}\n",
      "Test Metrics: {'mse': 0.026180488551267433, 'mae': 0.12353691054843409, 'r2': 0.14913020873577676, 'pearson_corr': 0.3871272882847764, 'connectome_corr': 0.16044218073695365, 'connectome_r2': -3.767436413630827, 'geodesic_distance': 5.767893088793565}\n",
      "BEST VAL SCORE -0.027513927619089013\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 31.9%\n",
      "RAM Usage: 5.3%\n",
      "Available RAM: 953.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.028845057258233976,\n",
       "    'mae': 0.13149863132294037,\n",
       "    'r2': 0.13711199810523578,\n",
       "    'pearson_corr': 0.3702863742123049},\n",
       "   'best_val_score': -0.02781618267591412,\n",
       "   'test_metrics': {'mse': 0.024882765307924614,\n",
       "    'mae': 0.12295614316783007,\n",
       "    'r2': 0.1630024064256036,\n",
       "    'pearson_corr': 0.42883380456139925,\n",
       "    'connectome_corr': 0.2823952614184139,\n",
       "    'connectome_r2': -4.282912784740204,\n",
       "    'geodesic_distance': 6.561954847541638},\n",
       "   'y_true': array([ 0.25718  ,  0.25718  ,  0.21614  ,  0.21614  ,  0.17879  ,\n",
       "           0.17879  ,  0.20003  ,  0.20003  ,  0.18572  ,  0.18572  ,\n",
       "           0.25026  ,  0.25026  ,  0.11669  ,  0.11669  ,  0.24455  ,\n",
       "           0.24455  ,  0.11424  ,  0.11424  ,  0.29925  ,  0.29925  ,\n",
       "           0.18083  ,  0.18083  ,  0.18606  ,  0.18606  ,  0.32224  ,\n",
       "           0.32224  ,  0.18932  ,  0.18932  ,  0.40234  ,  0.40234  ,\n",
       "           0.18223  ,  0.18223  ,  0.26495  ,  0.26495  ,  0.23553  ,\n",
       "           0.23553  ,  0.21608  ,  0.21608  ,  0.059681 ,  0.059681 ,\n",
       "           0.21966  ,  0.21966  ,  0.14268  ,  0.14268  ,  0.22835  ,\n",
       "           0.22835  ,  0.42801  ,  0.42801  ,  0.21521  ,  0.21521  ,\n",
       "           0.057658 ,  0.057658 ,  0.024612 ,  0.024612 ,  0.063759 ,\n",
       "           0.063759 ,  0.37308  ,  0.37308  ,  0.34132  ,  0.34132  ,\n",
       "           0.32791  ,  0.32791  ,  0.27542  ,  0.27542  ,  0.36759  ,\n",
       "           0.36759  ,  0.32945  ,  0.32945  ,  0.19856  ,  0.19856  ,\n",
       "           0.24324  ,  0.24324  ,  0.28955  ,  0.28955  ,  0.30675  ,\n",
       "           0.30675  ,  0.23096  ,  0.23096  ,  0.26071  ,  0.26071  ,\n",
       "           0.73698  ,  0.73698  ,  0.4264   ,  0.4264   ,  0.27143  ,\n",
       "           0.27143  ,  0.45758  ,  0.45758  ,  0.36727  ,  0.36727  ,\n",
       "           0.35085  ,  0.35085  ,  0.25663  ,  0.25663  ,  0.27071  ,\n",
       "           0.27071  ,  0.33455  ,  0.33455  ,  0.20266  ,  0.20266  ,\n",
       "           0.31722  ,  0.31722  ,  0.14571  ,  0.14571  ,  0.027941 ,\n",
       "           0.027941 ,  0.060493 ,  0.060493 ,  0.15441  ,  0.15441  ,\n",
       "           0.6665   ,  0.6665   ,  0.57117  ,  0.57117  ,  0.38161  ,\n",
       "           0.38161  ,  0.37302  ,  0.37302  ,  0.28516  ,  0.28516  ,\n",
       "           0.21945  ,  0.21945  ,  0.17296  ,  0.17296  ,  0.20472  ,\n",
       "           0.20472  ,  0.21497  ,  0.21497  ,  0.19057  ,  0.19057  ,\n",
       "           0.18716  ,  0.18716  ,  0.31581  ,  0.31581  ,  0.36647  ,\n",
       "           0.36647  ,  0.39506  ,  0.39506  ,  0.30871  ,  0.30871  ,\n",
       "           0.41039  ,  0.41039  ,  0.3606   ,  0.3606   ,  0.1418   ,\n",
       "           0.1418   ,  0.13914  ,  0.13914  ,  0.22276  ,  0.22276  ,\n",
       "           0.27225  ,  0.27225  ,  0.24408  ,  0.24408  ,  0.17506  ,\n",
       "           0.17506  ,  0.072852 ,  0.072852 ,  0.051264 ,  0.051264 ,\n",
       "           0.14285  ,  0.14285  ,  0.71373  ,  0.71373  ,  0.51352  ,\n",
       "           0.51352  ,  0.50611  ,  0.50611  ,  0.40024  ,  0.40024  ,\n",
       "           0.21241  ,  0.21241  ,  0.14927  ,  0.14927  ,  0.11245  ,\n",
       "           0.11245  ,  0.17597  ,  0.17597  ,  0.10735  ,  0.10735  ,\n",
       "           0.1207   ,  0.1207   ,  0.29097  ,  0.29097  ,  0.40723  ,\n",
       "           0.40723  ,  0.53582  ,  0.53582  ,  0.33953  ,  0.33953  ,\n",
       "           0.51564  ,  0.51564  ,  0.49405  ,  0.49405  ,  0.15535  ,\n",
       "           0.15535  ,  0.054372 ,  0.054372 ,  0.27296  ,  0.27296  ,\n",
       "           0.26516  ,  0.26516  ,  0.17728  ,  0.17728  ,  0.14978  ,\n",
       "           0.14978  ,  0.081124 ,  0.081124 ,  0.059407 ,  0.059407 ,\n",
       "           0.17772  ,  0.17772  ,  0.41042  ,  0.41042  ,  0.37986  ,\n",
       "           0.37986  ,  0.2326   ,  0.2326   ,  0.23302  ,  0.23302  ,\n",
       "           0.1023   ,  0.1023   ,  0.22117  ,  0.22117  ,  0.18856  ,\n",
       "           0.18856  ,  0.22485  ,  0.22485  ,  0.20126  ,  0.20126  ,\n",
       "           0.27932  ,  0.27932  ,  0.36743  ,  0.36743  ,  0.46038  ,\n",
       "           0.46038  ,  0.30677  ,  0.30677  ,  0.36399  ,  0.36399  ,\n",
       "           0.35942  ,  0.35942  ,  0.081346 ,  0.081346 ,  0.12832  ,\n",
       "           0.12832  ,  0.16343  ,  0.16343  ,  0.30423  ,  0.30423  ,\n",
       "           0.22562  ,  0.22562  ,  0.18438  ,  0.18438  ,  0.10238  ,\n",
       "           0.10238  ,  0.046463 ,  0.046463 ,  0.12617  ,  0.12617  ,\n",
       "           0.57758  ,  0.57758  ,  0.25516  ,  0.25516  ,  0.20396  ,\n",
       "           0.20396  ,  0.082235 ,  0.082235 ,  0.15964  ,  0.15964  ,\n",
       "           0.16099  ,  0.16099  ,  0.13542  ,  0.13542  ,  0.16695  ,\n",
       "           0.16695  ,  0.23593  ,  0.23593  ,  0.40179  ,  0.40179  ,\n",
       "           0.83487  ,  0.83487  ,  0.50918  ,  0.50918  ,  0.49254  ,\n",
       "           0.49254  ,  0.53114  ,  0.53114  ,  0.10013  ,  0.10013  ,\n",
       "           0.10591  ,  0.10591  ,  0.22573  ,  0.22573  ,  0.22525  ,\n",
       "           0.22525  ,  0.1804   ,  0.1804   ,  0.15122  ,  0.15122  ,\n",
       "           0.077285 ,  0.077285 ,  0.032009 ,  0.032009 ,  0.088393 ,\n",
       "           0.088393 ,  0.52482  ,  0.52482  ,  0.17916  ,  0.17916  ,\n",
       "           0.12961  ,  0.12961  ,  0.15284  ,  0.15284  ,  0.23742  ,\n",
       "           0.23742  ,  0.075633 ,  0.075633 ,  0.29751  ,  0.29751  ,\n",
       "           0.26412  ,  0.26412  ,  0.5749   ,  0.5749   ,  0.531    ,\n",
       "           0.531    ,  0.64141  ,  0.64141  ,  0.65289  ,  0.65289  ,\n",
       "           0.82631  ,  0.82631  ,  0.26751  ,  0.26751  ,  0.14327  ,\n",
       "           0.14327  ,  0.41185  ,  0.41185  ,  0.18529  ,  0.18529  ,\n",
       "           0.23687  ,  0.23687  ,  0.12537  ,  0.12537  ,  0.061412 ,\n",
       "           0.061412 ,  0.05069  ,  0.05069  ,  0.16083  ,  0.16083  ,\n",
       "           0.099214 ,  0.099214 ,  0.28009  ,  0.28009  ,  0.14229  ,\n",
       "           0.14229  ,  0.43305  ,  0.43305  ,  0.14596  ,  0.14596  ,\n",
       "           0.24134  ,  0.24134  ,  0.30743  ,  0.30743  ,  0.3538   ,\n",
       "           0.3538   ,  0.22765  ,  0.22765  ,  0.33891  ,  0.33891  ,\n",
       "           0.46497  ,  0.46497  ,  0.45604  ,  0.45604  ,  0.41889  ,\n",
       "           0.41889  ,  0.078087 ,  0.078087 ,  0.38251  ,  0.38251  ,\n",
       "           0.11138  ,  0.11138  ,  0.11192  ,  0.11192  ,  0.049193 ,\n",
       "           0.049193 ,  0.0029833,  0.0029833,  0.064237 ,  0.064237 ,\n",
       "           0.19875  ,  0.19875  ,  0.088349 ,  0.088349 ,  0.31613  ,\n",
       "           0.31613  ,  0.17825  ,  0.17825  ,  0.31984  ,  0.31984  ,\n",
       "           0.20939  ,  0.20939  ,  0.18911  ,  0.18911  ,  0.27698  ,\n",
       "           0.27698  ,  0.20825  ,  0.20825  ,  0.19395  ,  0.19395  ,\n",
       "           0.16338  ,  0.16338  ,  0.15413  ,  0.15413  ,  0.031288 ,\n",
       "           0.031288 ,  0.14008  ,  0.14008  ,  0.090333 ,  0.090333 ,\n",
       "           0.3232   ,  0.3232   ,  0.19489  ,  0.19489  ,  0.21312  ,\n",
       "           0.21312  ,  0.089073 ,  0.089073 ,  0.019102 ,  0.019102 ,\n",
       "           0.057632 ,  0.057632 ,  0.3293   ,  0.3293   ,  0.3667   ,\n",
       "           0.3667   ,  0.21927  ,  0.21927  ,  0.25507  ,  0.25507  ,\n",
       "           0.24062  ,  0.24062  ,  0.15685  ,  0.15685  ,  0.063974 ,\n",
       "           0.063974 ,  0.12795  ,  0.12795  ,  0.14111  ,  0.14111  ,\n",
       "           0.10713  ,  0.10713  ,  0.29691  ,  0.29691  ,  0.31763  ,\n",
       "           0.31763  ,  0.21632  ,  0.21632  ,  0.088533 ,  0.088533 ,\n",
       "           0.19251  ,  0.19251  ,  0.09435  ,  0.09435  , -0.0097822,\n",
       "          -0.0097822,  0.042749 ,  0.042749 ,  0.1148   ,  0.1148   ,\n",
       "           0.63295  ,  0.63295  ,  0.89729  ,  0.89729  ,  0.78426  ,\n",
       "           0.78426  ,  0.25748  ,  0.25748  ,  0.35261  ,  0.35261  ,\n",
       "           0.13637  ,  0.13637  ,  0.12896  ,  0.12896  ,  0.10413  ,\n",
       "           0.10413  ,  0.069807 ,  0.069807 ,  0.29382  ,  0.29382  ,\n",
       "           0.66414  ,  0.66414  ,  0.11436  ,  0.11436  ,  0.33009  ,\n",
       "           0.33009  ,  0.40708  ,  0.40708  ,  0.21692  ,  0.21692  ,\n",
       "           0.046517 ,  0.046517 ,  0.024411 ,  0.024411 ,  0.074275 ,\n",
       "           0.074275 ,  0.49719  ,  0.49719  ,  0.55071  ,  0.55071  ,\n",
       "           0.29181  ,  0.29181  ,  0.22733  ,  0.22733  ,  0.14105  ,\n",
       "           0.14105  ,  0.18604  ,  0.18604  ,  0.15466  ,  0.15466  ,\n",
       "           0.16411  ,  0.16411  ,  0.35593  ,  0.35593  ,  0.45398  ,\n",
       "           0.45398  ,  0.2662   ,  0.2662   ,  0.15843  ,  0.15843  ,\n",
       "           0.23967  ,  0.23967  ,  0.13648  ,  0.13648  ,  0.01001  ,\n",
       "           0.01001  ,  0.041901 ,  0.041901 ,  0.12886  ,  0.12886  ,\n",
       "           0.54538  ,  0.54538  ,  0.23193  ,  0.23193  ,  0.16253  ,\n",
       "           0.16253  ,  0.14732  ,  0.14732  ,  0.037314 ,  0.037314 ,\n",
       "           0.029983 ,  0.029983 ,  0.019508 ,  0.019508 ,  0.039895 ,\n",
       "           0.039895 ,  0.26181  ,  0.26181  , -0.036992 , -0.036992 ,\n",
       "           0.32256  ,  0.32256  ,  0.2233   ,  0.2233   ,  0.18918  ,\n",
       "           0.18918  ,  0.062411 ,  0.062411 ,  0.021565 ,  0.021565 ,\n",
       "           0.072701 ,  0.072701 ,  0.20146  ,  0.20146  ,  0.34138  ,\n",
       "           0.34138  ,  0.14711  ,  0.14711  ,  0.16088  ,  0.16088  ,\n",
       "           0.1457   ,  0.1457   ,  0.16897  ,  0.16897  ,  0.22849  ,\n",
       "           0.22849  ,  0.52415  ,  0.52415  ,  0.19247  ,  0.19247  ,\n",
       "           0.1996   ,  0.1996   ,  0.42029  ,  0.42029  ,  0.19304  ,\n",
       "           0.19304  ,  0.035691 ,  0.035691 ,  0.02462  ,  0.02462  ,\n",
       "           0.0827   ,  0.0827   ,  0.35579  ,  0.35579  ,  0.24985  ,\n",
       "           0.24985  ,  0.44664  ,  0.44664  ,  0.28575  ,  0.28575  ,\n",
       "           0.27349  ,  0.27349  ,  0.22864  ,  0.22864  ,  0.21489  ,\n",
       "           0.21489  ,  0.29219  ,  0.29219  ,  0.20292  ,  0.20292  ,\n",
       "           0.21713  ,  0.21713  ,  0.13414  ,  0.13414  ,  0.033316 ,\n",
       "           0.033316 ,  0.055339 ,  0.055339 ,  0.15933  ,  0.15933  ,\n",
       "           0.43353  ,  0.43353  ,  0.66881  ,  0.66881  ,  0.6345   ,\n",
       "           0.6345   ,  0.58515  ,  0.58515  ,  0.3033   ,  0.3033   ,\n",
       "           0.31446  ,  0.31446  ,  0.40582  ,  0.40582  ,  0.37597  ,\n",
       "           0.37597  ,  0.46799  ,  0.46799  ,  0.1874   ,  0.1874   ,\n",
       "           0.091397 ,  0.091397 ,  0.047718 ,  0.047718 ,  0.14286  ,\n",
       "           0.14286  ,  0.56453  ,  0.56453  ,  0.51335  ,  0.51335  ,\n",
       "           0.5968   ,  0.5968   ,  0.091152 ,  0.091152 ,  0.083658 ,\n",
       "           0.083658 ,  0.24034  ,  0.24034  ,  0.25253  ,  0.25253  ,\n",
       "           0.17331  ,  0.17331  ,  0.15214  ,  0.15214  ,  0.088057 ,\n",
       "           0.088057 ,  0.033493 ,  0.033493 ,  0.10485  ,  0.10485  ,\n",
       "           0.61631  ,  0.61631  ,  0.7151   ,  0.7151   ,  0.29513  ,\n",
       "           0.29513  ,  0.22922  ,  0.22922  ,  0.48951  ,  0.48951  ,\n",
       "           0.1781   ,  0.1781   ,  0.23601  ,  0.23601  ,  0.12201  ,\n",
       "           0.12201  ,  0.057065 ,  0.057065 ,  0.045402 ,  0.045402 ,\n",
       "           0.13086  ,  0.13086  ,  0.69467  ,  0.69467  ,  0.20101  ,\n",
       "           0.20101  ,  0.066433 ,  0.066433 ,  0.30269  ,  0.30269  ,\n",
       "           0.19766  ,  0.19766  ,  0.26341  ,  0.26341  ,  0.1146   ,\n",
       "           0.1146   ,  0.058149 ,  0.058149 ,  0.053463 ,  0.053463 ,\n",
       "           0.13575  ,  0.13575  ,  0.27974  ,  0.27974  ,  0.11393  ,\n",
       "           0.11393  ,  0.44104  ,  0.44104  ,  0.18226  ,  0.18226  ,\n",
       "           0.19928  ,  0.19928  ,  0.10922  ,  0.10922  ,  0.065321 ,\n",
       "           0.065321 ,  0.055716 ,  0.055716 ,  0.16755  ,  0.16755  ,\n",
       "           0.64809  ,  0.64809  ,  0.78581  ,  0.78581  ,  0.027869 ,\n",
       "           0.027869 ,  0.073261 ,  0.073261 ,  0.019392 ,  0.019392 ,\n",
       "          -0.028453 , -0.028453 ,  0.048853 ,  0.048853 ,  0.15552  ,\n",
       "           0.15552  ,  0.47912  ,  0.47912  ,  0.13938  ,  0.13938  ,\n",
       "           0.37744  ,  0.37744  ,  0.14814  ,  0.14814  ,  0.0052557,\n",
       "           0.0052557,  0.029382 ,  0.029382 ,  0.085198 ,  0.085198 ,\n",
       "           0.048122 ,  0.048122 ,  0.12057  ,  0.12057  ,  0.06033  ,\n",
       "           0.06033  ,  0.014264 ,  0.014264 ,  0.054312 ,  0.054312 ,\n",
       "           0.18358  ,  0.18358  ,  0.25977  ,  0.25977  ,  0.18118  ,\n",
       "           0.18118  ,  0.11717  ,  0.11717  ,  0.02445  ,  0.02445  ,\n",
       "           0.074665 ,  0.074665 ,  0.21137  ,  0.21137  ,  0.045916 ,\n",
       "           0.045916 ,  0.027396 ,  0.027396 ,  0.06124  ,  0.06124  ,\n",
       "           0.11414  ,  0.11414  ,  0.028379 ,  0.028379 ,  0.07981  ,\n",
       "           0.07981  ,  0.011463 ,  0.011463 ,  0.04058  ,  0.04058  ,\n",
       "           0.094907 ,  0.094907 ]),\n",
       "   'y_pred': array([0.26241324, 0.26241324, 0.2197797 , 0.2197797 , 0.20813517,\n",
       "          0.20813517, 0.22856851, 0.22856851, 0.26920674, 0.26920674,\n",
       "          0.25462878, 0.25462878, 0.17965395, 0.17965395, 0.14172408,\n",
       "          0.14172408, 0.24565545, 0.24565545, 0.27995313, 0.27995313,\n",
       "          0.12903159, 0.12903159, 0.19125649, 0.19125649, 0.22550419,\n",
       "          0.22550419, 0.26410407, 0.26410407, 0.26052242, 0.26052242,\n",
       "          0.25593104, 0.25593104, 0.30317055, 0.30317055, 0.3081022 ,\n",
       "          0.3081022 , 0.25652776, 0.25652776, 0.27142526, 0.27142526,\n",
       "          0.29508184, 0.29508184, 0.16967185, 0.16967185, 0.16093485,\n",
       "          0.16093485, 0.24989923, 0.24989923, 0.18634339, 0.18634339,\n",
       "          0.16615438, 0.16615438, 0.17189902, 0.17189902, 0.18461816,\n",
       "          0.18461816, 0.29025918, 0.29025918, 0.27861465, 0.27861465,\n",
       "          0.29904799, 0.29904799, 0.33968622, 0.33968622, 0.32510826,\n",
       "          0.32510826, 0.25013344, 0.25013344, 0.21220357, 0.21220357,\n",
       "          0.31613494, 0.31613494, 0.35043262, 0.35043262, 0.19951108,\n",
       "          0.19951108, 0.26173598, 0.26173598, 0.29598368, 0.29598368,\n",
       "          0.33458356, 0.33458356, 0.3310019 , 0.3310019 , 0.32641053,\n",
       "          0.32641053, 0.37365004, 0.37365004, 0.37858169, 0.37858169,\n",
       "          0.32700725, 0.32700725, 0.34190475, 0.34190475, 0.36556133,\n",
       "          0.36556133, 0.24015134, 0.24015134, 0.23141434, 0.23141434,\n",
       "          0.32037871, 0.32037871, 0.25682287, 0.25682287, 0.23663387,\n",
       "          0.23663387, 0.24237851, 0.24237851, 0.25509765, 0.25509765,\n",
       "          0.23598111, 0.23598111, 0.25641445, 0.25641445, 0.29705268,\n",
       "          0.29705268, 0.28247472, 0.28247472, 0.2074999 , 0.2074999 ,\n",
       "          0.16957002, 0.16957002, 0.2735014 , 0.2735014 , 0.30779908,\n",
       "          0.30779908, 0.15687754, 0.15687754, 0.21910243, 0.21910243,\n",
       "          0.25335014, 0.25335014, 0.29195002, 0.29195002, 0.28836836,\n",
       "          0.28836836, 0.28377698, 0.28377698, 0.33101649, 0.33101649,\n",
       "          0.33594814, 0.33594814, 0.28437371, 0.28437371, 0.2992712 ,\n",
       "          0.2992712 , 0.32292779, 0.32292779, 0.1975178 , 0.1975178 ,\n",
       "          0.1887808 , 0.1887808 , 0.27774517, 0.27774517, 0.21418933,\n",
       "          0.21418933, 0.19400033, 0.19400033, 0.19974496, 0.19974496,\n",
       "          0.21246411, 0.21246411, 0.24476992, 0.24476992, 0.28540815,\n",
       "          0.28540815, 0.27083019, 0.27083019, 0.19585537, 0.19585537,\n",
       "          0.15792549, 0.15792549, 0.26185687, 0.26185687, 0.29615455,\n",
       "          0.29615455, 0.14523301, 0.14523301, 0.2074579 , 0.2074579 ,\n",
       "          0.24170561, 0.24170561, 0.28030549, 0.28030549, 0.27672383,\n",
       "          0.27672383, 0.27213245, 0.27213245, 0.31937196, 0.31937196,\n",
       "          0.32430361, 0.32430361, 0.27272918, 0.27272918, 0.28762667,\n",
       "          0.28762667, 0.31128325, 0.31128325, 0.18587326, 0.18587326,\n",
       "          0.17713627, 0.17713627, 0.26610064, 0.26610064, 0.2025448 ,\n",
       "          0.2025448 , 0.18235579, 0.18235579, 0.18810043, 0.18810043,\n",
       "          0.20081957, 0.20081957, 0.30584149, 0.30584149, 0.29126353,\n",
       "          0.29126353, 0.21628871, 0.21628871, 0.17835883, 0.17835883,\n",
       "          0.28229021, 0.28229021, 0.31658789, 0.31658789, 0.16566635,\n",
       "          0.16566635, 0.22789125, 0.22789125, 0.26213895, 0.26213895,\n",
       "          0.30073883, 0.30073883, 0.29715717, 0.29715717, 0.29256579,\n",
       "          0.29256579, 0.33980531, 0.33980531, 0.34473695, 0.34473695,\n",
       "          0.29316252, 0.29316252, 0.30806001, 0.30806001, 0.3317166 ,\n",
       "          0.3317166 , 0.20630661, 0.20630661, 0.19756961, 0.19756961,\n",
       "          0.28653398, 0.28653398, 0.22297814, 0.22297814, 0.20278914,\n",
       "          0.20278914, 0.20853377, 0.20853377, 0.22125292, 0.22125292,\n",
       "          0.33190176, 0.33190176, 0.25692694, 0.25692694, 0.21899706,\n",
       "          0.21899706, 0.32292844, 0.32292844, 0.35722612, 0.35722612,\n",
       "          0.20630458, 0.20630458, 0.26852947, 0.26852947, 0.30277717,\n",
       "          0.30277717, 0.34137706, 0.34137706, 0.3377954 , 0.3377954 ,\n",
       "          0.33320402, 0.33320402, 0.38044353, 0.38044353, 0.38537518,\n",
       "          0.38537518, 0.33380075, 0.33380075, 0.34869824, 0.34869824,\n",
       "          0.37235482, 0.37235482, 0.24694483, 0.24694483, 0.23820784,\n",
       "          0.23820784, 0.32717221, 0.32717221, 0.26361637, 0.26361637,\n",
       "          0.24342736, 0.24342736, 0.249172  , 0.249172  , 0.26189114,\n",
       "          0.26189114, 0.24234898, 0.24234898, 0.2044191 , 0.2044191 ,\n",
       "          0.30835048, 0.30835048, 0.34264816, 0.34264816, 0.19172662,\n",
       "          0.19172662, 0.25395152, 0.25395152, 0.28819922, 0.28819922,\n",
       "          0.3267991 , 0.3267991 , 0.32321744, 0.32321744, 0.31862606,\n",
       "          0.31862606, 0.36586558, 0.36586558, 0.37079723, 0.37079723,\n",
       "          0.31922279, 0.31922279, 0.33412028, 0.33412028, 0.35777687,\n",
       "          0.35777687, 0.23236688, 0.23236688, 0.22362988, 0.22362988,\n",
       "          0.31259425, 0.31259425, 0.24903841, 0.24903841, 0.22884941,\n",
       "          0.22884941, 0.23459404, 0.23459404, 0.24731319, 0.24731319,\n",
       "          0.12944428, 0.12944428, 0.23337565, 0.23337565, 0.26767333,\n",
       "          0.26767333, 0.1167518 , 0.1167518 , 0.17897669, 0.17897669,\n",
       "          0.21322439, 0.21322439, 0.25182428, 0.25182428, 0.24824262,\n",
       "          0.24824262, 0.24365124, 0.24365124, 0.29089075, 0.29089075,\n",
       "          0.2958224 , 0.2958224 , 0.24424797, 0.24424797, 0.25914546,\n",
       "          0.25914546, 0.28280204, 0.28280204, 0.15739205, 0.15739205,\n",
       "          0.14865506, 0.14865506, 0.23761943, 0.23761943, 0.17406359,\n",
       "          0.17406359, 0.15387458, 0.15387458, 0.15961922, 0.15961922,\n",
       "          0.17233836, 0.17233836, 0.19544578, 0.19544578, 0.22974346,\n",
       "          0.22974346, 0.07882192, 0.07882192, 0.14104682, 0.14104682,\n",
       "          0.17529452, 0.17529452, 0.2138944 , 0.2138944 , 0.21031274,\n",
       "          0.21031274, 0.20572137, 0.20572137, 0.25296088, 0.25296088,\n",
       "          0.25789253, 0.25789253, 0.20631809, 0.20631809, 0.22121559,\n",
       "          0.22121559, 0.24487217, 0.24487217, 0.11946218, 0.11946218,\n",
       "          0.11072518, 0.11072518, 0.19968955, 0.19968955, 0.13613371,\n",
       "          0.13613371, 0.11594471, 0.11594471, 0.12168935, 0.12168935,\n",
       "          0.13440849, 0.13440849, 0.33367483, 0.33367483, 0.18275329,\n",
       "          0.18275329, 0.24497819, 0.24497819, 0.27922589, 0.27922589,\n",
       "          0.31782577, 0.31782577, 0.31424412, 0.31424412, 0.30965274,\n",
       "          0.30965274, 0.35689225, 0.35689225, 0.3618239 , 0.3618239 ,\n",
       "          0.31024946, 0.31024946, 0.32514696, 0.32514696, 0.34880354,\n",
       "          0.34880354, 0.22339355, 0.22339355, 0.21465655, 0.21465655,\n",
       "          0.30362093, 0.30362093, 0.24006509, 0.24006509, 0.21987608,\n",
       "          0.21987608, 0.22562072, 0.22562072, 0.23833986, 0.23833986,\n",
       "          0.21705097, 0.21705097, 0.27927587, 0.27927587, 0.31352357,\n",
       "          0.31352357, 0.35212345, 0.35212345, 0.3485418 , 0.3485418 ,\n",
       "          0.34395042, 0.34395042, 0.39118993, 0.39118993, 0.39612158,\n",
       "          0.39612158, 0.34454714, 0.34454714, 0.35944464, 0.35944464,\n",
       "          0.38310122, 0.38310122, 0.25769123, 0.25769123, 0.24895423,\n",
       "          0.24895423, 0.33791861, 0.33791861, 0.27436277, 0.27436277,\n",
       "          0.25417376, 0.25417376, 0.2599184 , 0.2599184 , 0.27263754,\n",
       "          0.27263754, 0.12835433, 0.12835433, 0.16260203, 0.16260203,\n",
       "          0.20120192, 0.20120192, 0.19762026, 0.19762026, 0.19302888,\n",
       "          0.19302888, 0.24026839, 0.24026839, 0.24520004, 0.24520004,\n",
       "          0.19362561, 0.19362561, 0.2085231 , 0.2085231 , 0.23217968,\n",
       "          0.23217968, 0.10676969, 0.10676969, 0.0980327 , 0.0980327 ,\n",
       "          0.18699707, 0.18699707, 0.12344123, 0.12344123, 0.10325222,\n",
       "          0.10325222, 0.10899686, 0.10899686, 0.121716  , 0.121716  ,\n",
       "          0.22482693, 0.22482693, 0.26342681, 0.26342681, 0.25984515,\n",
       "          0.25984515, 0.25525378, 0.25525378, 0.30249329, 0.30249329,\n",
       "          0.30742494, 0.30742494, 0.2558505 , 0.2558505 , 0.270748  ,\n",
       "          0.270748  , 0.29440458, 0.29440458, 0.16899459, 0.16899459,\n",
       "          0.16025759, 0.16025759, 0.24922196, 0.24922196, 0.18566612,\n",
       "          0.18566612, 0.16547712, 0.16547712, 0.17122176, 0.17122176,\n",
       "          0.1839409 , 0.1839409 , 0.29767451, 0.29767451, 0.29409286,\n",
       "          0.29409286, 0.28950148, 0.28950148, 0.33674099, 0.33674099,\n",
       "          0.34167264, 0.34167264, 0.2900982 , 0.2900982 , 0.3049957 ,\n",
       "          0.3049957 , 0.32865228, 0.32865228, 0.20324229, 0.20324229,\n",
       "          0.19450529, 0.19450529, 0.28346967, 0.28346967, 0.21991383,\n",
       "          0.21991383, 0.19972482, 0.19972482, 0.20546946, 0.20546946,\n",
       "          0.2181886 , 0.2181886 , 0.33269274, 0.33269274, 0.32810136,\n",
       "          0.32810136, 0.37534087, 0.37534087, 0.38027252, 0.38027252,\n",
       "          0.32869809, 0.32869809, 0.34359558, 0.34359558, 0.36725216,\n",
       "          0.36725216, 0.24184217, 0.24184217, 0.23310518, 0.23310518,\n",
       "          0.32206955, 0.32206955, 0.25851371, 0.25851371, 0.2383247 ,\n",
       "          0.2383247 , 0.24406934, 0.24406934, 0.25678848, 0.25678848,\n",
       "          0.3245197 , 0.3245197 , 0.37175921, 0.37175921, 0.37669086,\n",
       "          0.37669086, 0.32511643, 0.32511643, 0.34001392, 0.34001392,\n",
       "          0.3636705 , 0.3636705 , 0.23826051, 0.23826051, 0.22952352,\n",
       "          0.22952352, 0.31848789, 0.31848789, 0.25493205, 0.25493205,\n",
       "          0.23474304, 0.23474304, 0.24048768, 0.24048768, 0.25320682,\n",
       "          0.25320682, 0.36716784, 0.36716784, 0.37209949, 0.37209949,\n",
       "          0.32052505, 0.32052505, 0.33542255, 0.33542255, 0.35907913,\n",
       "          0.35907913, 0.23366914, 0.23366914, 0.22493214, 0.22493214,\n",
       "          0.31389651, 0.31389651, 0.25034067, 0.25034067, 0.23015167,\n",
       "          0.23015167, 0.23589631, 0.23589631, 0.24861545, 0.24861545,\n",
       "          0.419339  , 0.419339  , 0.36776456, 0.36776456, 0.38266206,\n",
       "          0.38266206, 0.40631864, 0.40631864, 0.28090865, 0.28090865,\n",
       "          0.27217165, 0.27217165, 0.36113602, 0.36113602, 0.29758018,\n",
       "          0.29758018, 0.27739118, 0.27739118, 0.28313582, 0.28313582,\n",
       "          0.29585496, 0.29585496, 0.37269621, 0.37269621, 0.38759371,\n",
       "          0.38759371, 0.41125029, 0.41125029, 0.2858403 , 0.2858403 ,\n",
       "          0.2771033 , 0.2771033 , 0.36606767, 0.36606767, 0.30251183,\n",
       "          0.30251183, 0.28232283, 0.28232283, 0.28806747, 0.28806747,\n",
       "          0.30078661, 0.30078661, 0.33601927, 0.33601927, 0.35967585,\n",
       "          0.35967585, 0.23426586, 0.23426586, 0.22552887, 0.22552887,\n",
       "          0.31449324, 0.31449324, 0.2509374 , 0.2509374 , 0.23074839,\n",
       "          0.23074839, 0.23649303, 0.23649303, 0.24921217, 0.24921217,\n",
       "          0.37457335, 0.37457335, 0.24916336, 0.24916336, 0.24042636,\n",
       "          0.24042636, 0.32939073, 0.32939073, 0.26583489, 0.26583489,\n",
       "          0.24564589, 0.24564589, 0.25139053, 0.25139053, 0.26410967,\n",
       "          0.26410967, 0.27281994, 0.27281994, 0.26408294, 0.26408294,\n",
       "          0.35304732, 0.35304732, 0.28949148, 0.28949148, 0.26930247,\n",
       "          0.26930247, 0.27504711, 0.27504711, 0.28776625, 0.28776625,\n",
       "          0.13867295, 0.13867295, 0.22763733, 0.22763733, 0.16408148,\n",
       "          0.16408148, 0.14389248, 0.14389248, 0.14963712, 0.14963712,\n",
       "          0.16235626, 0.16235626, 0.21890033, 0.21890033, 0.15534449,\n",
       "          0.15534449, 0.13515548, 0.13515548, 0.14090012, 0.14090012,\n",
       "          0.15361926, 0.15361926, 0.24430886, 0.24430886, 0.22411986,\n",
       "          0.22411986, 0.22986449, 0.22986449, 0.24258364, 0.24258364,\n",
       "          0.16056402, 0.16056402, 0.16630865, 0.16630865, 0.1790278 ,\n",
       "          0.1790278 , 0.14611965, 0.14611965, 0.15883879, 0.15883879,\n",
       "          0.16458343, 0.16458343]),\n",
       "   'feature_importances': array([ 4.14294063e-05, -8.83178591e-04,  1.16616297e-03,  4.14294063e-05,\n",
       "          -8.83178591e-04,  1.16616297e-03]),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'alpha': 0,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.028657961127511768,\n",
       "    'mae': 0.131073186442198,\n",
       "    'r2': 0.12882930321167874,\n",
       "    'pearson_corr': 0.3589279916803351},\n",
       "   'best_val_score': -0.026945869012446503,\n",
       "   'test_metrics': {'mse': 0.02754606515989599,\n",
       "    'mae': 0.12665806772650975,\n",
       "    'r2': 0.2726069368042884,\n",
       "    'pearson_corr': 0.5293168126205281,\n",
       "    'connectome_corr': 0.361093469044526,\n",
       "    'connectome_r2': -14.798010809249478,\n",
       "    'geodesic_distance': 6.868695882944916},\n",
       "   'y_true': array([ 0.71922  ,  0.71922  ,  0.48128  ,  0.48128  ,  0.43659  ,\n",
       "           0.43659  ,  0.32399  ,  0.32399  ,  0.44948  ,  0.44948  ,\n",
       "           0.33167  ,  0.33167  ,  0.43798  ,  0.43798  ,  0.50016  ,\n",
       "           0.50016  ,  0.1345   ,  0.1345   ,  0.26447  ,  0.26447  ,\n",
       "           0.29921  ,  0.29921  ,  0.30794  ,  0.30794  ,  0.3946   ,\n",
       "           0.3946   ,  0.19136  ,  0.19136  ,  0.23954  ,  0.23954  ,\n",
       "           0.92977  ,  0.92977  ,  0.59727  ,  0.59727  ,  0.49227  ,\n",
       "           0.49227  ,  0.42251  ,  0.42251  ,  0.39702  ,  0.39702  ,\n",
       "           0.32595  ,  0.32595  ,  0.12236  ,  0.12236  ,  0.21177  ,\n",
       "           0.21177  ,  0.16332  ,  0.16332  ,  0.2177   ,  0.2177   ,\n",
       "           0.051711 ,  0.051711 ,  0.045002 ,  0.045002 ,  0.027955 ,\n",
       "           0.027955 ,  0.4555   ,  0.4555   ,  0.70545  ,  0.70545  ,\n",
       "           0.35772  ,  0.35772  ,  0.40483  ,  0.40483  ,  0.31522  ,\n",
       "           0.31522  ,  0.3479   ,  0.3479   ,  0.47006  ,  0.47006  ,\n",
       "           0.11232  ,  0.11232  ,  0.21738  ,  0.21738  ,  0.2367   ,\n",
       "           0.2367   ,  0.19925  ,  0.19925  ,  0.42237  ,  0.42237  ,\n",
       "           0.15409  ,  0.15409  ,  0.12644  ,  0.12644  ,  0.76273  ,\n",
       "           0.76273  ,  1.4159   ,  1.4159   ,  0.53017  ,  0.53017  ,\n",
       "           0.48218  ,  0.48218  ,  0.39509  ,  0.39509  ,  0.46872  ,\n",
       "           0.46872  ,  0.11871  ,  0.11871  ,  0.1184   ,  0.1184   ,\n",
       "           0.088568 ,  0.088568 ,  0.10206  ,  0.10206  ,  0.068446 ,\n",
       "           0.068446 ,  0.033191 ,  0.033191 ,  0.02063  ,  0.02063  ,\n",
       "           0.4428   ,  0.4428   ,  0.41565  ,  0.41565  ,  0.34578  ,\n",
       "           0.34578  ,  0.39808  ,  0.39808  ,  0.44063  ,  0.44063  ,\n",
       "           0.46516  ,  0.46516  ,  0.15989  ,  0.15989  ,  0.26702  ,\n",
       "           0.26702  ,  0.33929  ,  0.33929  ,  0.21536  ,  0.21536  ,\n",
       "           0.56331  ,  0.56331  ,  0.25194  ,  0.25194  ,  0.25443  ,\n",
       "           0.25443  ,  0.49248  ,  0.49248  ,  0.39745  ,  0.39745  ,\n",
       "           0.54343  ,  0.54343  ,  0.4951   ,  0.4951   ,  0.54511  ,\n",
       "           0.54511  ,  0.32239  ,  0.32239  ,  0.14092  ,  0.14092  ,\n",
       "           0.23301  ,  0.23301  ,  0.21211  ,  0.21211  ,  0.23812  ,\n",
       "           0.23812  ,  0.096152 ,  0.096152 ,  0.046146 ,  0.046146 ,\n",
       "           0.034861 ,  0.034861 ,  0.61068  ,  0.61068  ,  0.48006  ,\n",
       "           0.48006  ,  0.34539  ,  0.34539  ,  0.37022  ,  0.37022  ,\n",
       "           0.43956  ,  0.43956  ,  0.17585  ,  0.17585  ,  0.49562  ,\n",
       "           0.49562  ,  0.50252  ,  0.50252  ,  0.18569  ,  0.18569  ,\n",
       "           0.54201  ,  0.54201  ,  0.2935   ,  0.2935   ,  0.13164  ,\n",
       "           0.13164  ,  0.45332  ,  0.45332  ,  0.7205   ,  0.7205   ,\n",
       "           0.38752  ,  0.38752  ,  0.43245  ,  0.43245  ,  0.35889  ,\n",
       "           0.35889  ,  0.67894  ,  0.67894  ,  0.1627   ,  0.1627   ,\n",
       "           0.20474  ,  0.20474  ,  0.23652  ,  0.23652  ,  0.10442  ,\n",
       "           0.10442  ,  0.09912  ,  0.09912  ,  0.036825 ,  0.036825 ,\n",
       "           0.020006 ,  0.020006 ,  0.45635  ,  0.45635  ,  0.45603  ,\n",
       "           0.45603  ,  0.38549  ,  0.38549  ,  0.44073  ,  0.44073  ,\n",
       "           0.1256   ,  0.1256   ,  0.40294  ,  0.40294  ,  0.42535  ,\n",
       "           0.42535  ,  0.1231   ,  0.1231   ,  0.41124  ,  0.41124  ,\n",
       "           0.17447  ,  0.17447  ,  0.044144 ,  0.044144 ,  0.34047  ,\n",
       "           0.34047  ,  0.32739  ,  0.32739  ,  0.37778  ,  0.37778  ,\n",
       "           0.49065  ,  0.49065  ,  0.36448  ,  0.36448  ,  0.58298  ,\n",
       "           0.58298  ,  0.11132  ,  0.11132  ,  0.10034  ,  0.10034  ,\n",
       "           0.073646 ,  0.073646 ,  0.013839 ,  0.013839 ,  0.076694 ,\n",
       "           0.076694 ,  0.037086 ,  0.037086 ,  0.0091754,  0.0091754,\n",
       "           0.34648  ,  0.34648  ,  0.43885  ,  0.43885  ,  0.63528  ,\n",
       "           0.63528  ,  0.1177   ,  0.1177   ,  0.44497  ,  0.44497  ,\n",
       "           0.42537  ,  0.42537  ,  0.4355   ,  0.4355   ,  0.39446  ,\n",
       "           0.39446  ,  0.095204 ,  0.095204 ,  0.14335  ,  0.14335  ,\n",
       "           0.43289  ,  0.43289  ,  0.36495  ,  0.36495  ,  0.2908   ,\n",
       "           0.2908   ,  0.36472  ,  0.36472  ,  0.28198  ,  0.28198  ,\n",
       "           0.47213  ,  0.47213  ,  0.10033  ,  0.10033  ,  0.24096  ,\n",
       "           0.24096  ,  0.15704  ,  0.15704  ,  0.14838  ,  0.14838  ,\n",
       "           0.039112 ,  0.039112 ,  0.039563 ,  0.039563 ,  0.023165 ,\n",
       "           0.023165 ,  0.55964  ,  0.55964  ,  0.4745   ,  0.4745   ,\n",
       "           0.12526  ,  0.12526  ,  0.30224  ,  0.30224  ,  0.42563  ,\n",
       "           0.42563  ,  0.22515  ,  0.22515  ,  0.38099  ,  0.38099  ,\n",
       "           0.20657  ,  0.20657  ,  0.032886 ,  0.032886 ,  0.3238   ,\n",
       "           0.3238   ,  0.27486  ,  0.27486  ,  0.3946   ,  0.3946   ,\n",
       "           0.29752  ,  0.29752  ,  0.26246  ,  0.26246  ,  0.36549  ,\n",
       "           0.36549  ,  0.08782  ,  0.08782  ,  0.1119   ,  0.1119   ,\n",
       "          -0.019675 , -0.019675 , -0.0045123, -0.0045123,  0.04498  ,\n",
       "           0.04498  ,  0.055842 ,  0.055842 ,  0.022106 ,  0.022106 ,\n",
       "           0.53023  ,  0.53023  ,  0.16353  ,  0.16353  ,  0.37731  ,\n",
       "           0.37731  ,  0.46037  ,  0.46037  ,  0.35026  ,  0.35026  ,\n",
       "           0.38297  ,  0.38297  ,  0.25752  ,  0.25752  ,  0.23182  ,\n",
       "           0.23182  ,  0.40406  ,  0.40406  ,  0.31176  ,  0.31176  ,\n",
       "           0.40604  ,  0.40604  ,  0.34375  ,  0.34375  ,  0.34606  ,\n",
       "           0.34606  ,  0.35101  ,  0.35101  ,  0.109    ,  0.109    ,\n",
       "           0.20861  ,  0.20861  ,  0.10234  ,  0.10234  ,  0.16967  ,\n",
       "           0.16967  ,  0.044379 ,  0.044379 ,  0.06157  ,  0.06157  ,\n",
       "           0.040003 ,  0.040003 ,  0.12983  ,  0.12983  ,  0.29599  ,\n",
       "           0.29599  ,  0.34645  ,  0.34645  ,  0.36782  ,  0.36782  ,\n",
       "           0.44099  ,  0.44099  ,  0.13153  ,  0.13153  ,  0.1545   ,\n",
       "           0.1545   ,  0.50111  ,  0.50111  ,  0.41965  ,  0.41965  ,\n",
       "           0.45112  ,  0.45112  ,  0.44269  ,  0.44269  ,  0.39682  ,\n",
       "           0.39682  ,  0.38445  ,  0.38445  ,  0.11951  ,  0.11951  ,\n",
       "           0.16574  ,  0.16574  ,  0.079648 ,  0.079648 ,  0.14578  ,\n",
       "           0.14578  ,  0.054895 ,  0.054895 ,  0.049641 ,  0.049641 ,\n",
       "           0.028757 ,  0.028757 ,  0.21889  ,  0.21889  ,  0.25943  ,\n",
       "           0.25943  ,  0.14335  ,  0.14335  ,  0.14767  ,  0.14767  ,\n",
       "           0.25599  ,  0.25599  ,  0.21192  ,  0.21192  ,  0.11635  ,\n",
       "           0.11635  ,  0.1081   ,  0.1081   ,  0.10367  ,  0.10367  ,\n",
       "           0.10679  ,  0.10679  ,  0.11532  ,  0.11532  ,  0.12951  ,\n",
       "           0.12951  ,  0.11851  ,  0.11851  ,  0.14697  ,  0.14697  ,\n",
       "           0.18269  ,  0.18269  ,  0.15903  ,  0.15903  ,  0.043178 ,\n",
       "           0.043178 ,  0.024286 ,  0.024286 ,  0.020426 ,  0.020426 ,\n",
       "           0.96592  ,  0.96592  ,  0.49741  ,  0.49741  ,  0.33026  ,\n",
       "           0.33026  ,  0.38577  ,  0.38577  ,  0.027429 ,  0.027429 ,\n",
       "           0.23037  ,  0.23037  ,  0.19739  ,  0.19739  ,  0.16974  ,\n",
       "           0.16974  ,  0.19702  ,  0.19702  ,  0.17137  ,  0.17137  ,\n",
       "           0.71622  ,  0.71622  ,  0.13794  ,  0.13794  ,  0.50895  ,\n",
       "           0.50895  ,  0.434    ,  0.434    ,  0.0098301,  0.0098301,\n",
       "           0.017404 ,  0.017404 ,  0.039726 ,  0.039726 ,  0.01181  ,\n",
       "           0.01181  ,  0.35224  ,  0.35224  ,  0.40969  ,  0.40969  ,\n",
       "           0.56554  ,  0.56554  ,  0.086979 ,  0.086979 ,  0.26136  ,\n",
       "           0.26136  ,  0.21452  ,  0.21452  ,  0.23154  ,  0.23154  ,\n",
       "           0.22368  ,  0.22368  ,  0.21647  ,  0.21647  ,  0.53709  ,\n",
       "           0.53709  ,  0.14928  ,  0.14928  ,  0.42003  ,  0.42003  ,\n",
       "           0.31317  ,  0.31317  ,  0.02881  ,  0.02881  ,  0.046027 ,\n",
       "           0.046027 ,  0.048635 ,  0.048635 ,  0.016227 ,  0.016227 ,\n",
       "           0.21579  ,  0.21579  ,  0.086568 ,  0.086568 ,  0.22599  ,\n",
       "           0.22599  ,  0.2586   ,  0.2586   ,  0.17552  ,  0.17552  ,\n",
       "           0.13738  ,  0.13738  ,  0.13292  ,  0.13292  ,  0.091804 ,\n",
       "           0.091804 ,  0.2714   ,  0.2714   ,  0.084917 ,  0.084917 ,\n",
       "           0.37345  ,  0.37345  ,  0.22315  ,  0.22315  ,  0.24222  ,\n",
       "           0.24222  , -0.021994 , -0.021994 ,  0.04059  ,  0.04059  ,\n",
       "           0.036565 ,  0.036565 ,  0.4397   ,  0.4397   ,  0.16518  ,\n",
       "           0.16518  ,  0.4114   ,  0.4114   ,  0.3826   ,  0.3826   ,\n",
       "           0.41368  ,  0.41368  ,  0.39789  ,  0.39789  ,  0.38326  ,\n",
       "           0.38326  ,  0.33313  ,  0.33313  ,  0.14608  ,  0.14608  ,\n",
       "           0.2563   ,  0.2563   ,  0.19318  ,  0.19318  ,  0.14108  ,\n",
       "           0.14108  ,  0.10198  ,  0.10198  ,  0.040245 ,  0.040245 ,\n",
       "           0.015939 ,  0.015939 ,  0.24942  ,  0.24942  ,  0.16531  ,\n",
       "           0.16531  ,  0.1696   ,  0.1696   ,  0.1745   ,  0.1745   ,\n",
       "           0.16606  ,  0.16606  ,  0.21919  ,  0.21919  ,  0.15928  ,\n",
       "           0.15928  ,  0.15385  ,  0.15385  ,  0.36641  ,  0.36641  ,\n",
       "           0.35634  ,  0.35634  ,  0.15194  ,  0.15194  ,  0.07519  ,\n",
       "           0.07519  ,  0.039233 ,  0.039233 ,  0.0028819,  0.0028819,\n",
       "           0.19999  ,  0.19999  ,  0.13245  ,  0.13245  ,  0.13954  ,\n",
       "           0.13954  ,  0.17444  ,  0.17444  ,  0.2211   ,  0.2211   ,\n",
       "          -0.040998 , -0.040998 ,  0.14267  ,  0.14267  ,  0.15903  ,\n",
       "           0.15903  ,  0.29571  ,  0.29571  ,  0.71477  ,  0.71477  ,\n",
       "           0.067065 ,  0.067065 ,  0.028937 ,  0.028937 ,  0.058216 ,\n",
       "           0.058216 ,  0.65175  ,  0.65175  ,  0.53779  ,  0.53779  ,\n",
       "           0.47837  ,  0.47837  ,  0.44458  ,  0.44458  ,  0.33217  ,\n",
       "           0.33217  ,  0.11667  ,  0.11667  ,  0.16692  ,  0.16692  ,\n",
       "           0.13881  ,  0.13881  ,  0.18278  ,  0.18278  ,  0.064966 ,\n",
       "           0.064966 ,  0.03759  ,  0.03759  ,  0.021881 ,  0.021881 ,\n",
       "           0.4799   ,  0.4799   ,  0.44247  ,  0.44247  ,  0.35893  ,\n",
       "           0.35893  ,  0.49644  ,  0.49644  ,  0.11726  ,  0.11726  ,\n",
       "           0.10522  ,  0.10522  ,  0.095924 ,  0.095924 ,  0.1036   ,\n",
       "           0.1036   ,  0.066691 ,  0.066691 ,  0.029888 ,  0.029888 ,\n",
       "           0.018513 ,  0.018513 ,  0.55442  ,  0.55442  ,  0.59529  ,\n",
       "           0.59529  ,  0.30399  ,  0.30399  ,  0.10286  ,  0.10286  ,\n",
       "           0.11277  ,  0.11277  ,  0.080131 ,  0.080131 ,  0.10934  ,\n",
       "           0.10934  ,  0.085419 ,  0.085419 ,  0.041231 ,  0.041231 ,\n",
       "           0.021797 ,  0.021797 ,  0.67374  ,  0.67374  ,  0.35077  ,\n",
       "           0.35077  ,  0.10422  ,  0.10422  ,  0.10999  ,  0.10999  ,\n",
       "           0.12321  ,  0.12321  ,  0.14327  ,  0.14327  ,  0.090271 ,\n",
       "           0.090271 ,  0.028932 ,  0.028932 ,  0.019377 ,  0.019377 ,\n",
       "           0.23682  ,  0.23682  ,  0.10727  ,  0.10727  ,  0.1266   ,\n",
       "           0.1266   ,  0.15259  ,  0.15259  ,  0.18575  ,  0.18575  ,\n",
       "           0.10293  ,  0.10293  ,  0.030834 ,  0.030834 ,  0.023963 ,\n",
       "           0.023963 ,  0.13622  ,  0.13622  ,  0.34866  ,  0.34866  ,\n",
       "           0.28463  ,  0.28463  , -0.024828 , -0.024828 ,  0.037177 ,\n",
       "           0.037177 ,  0.035488 ,  0.035488 ,  0.011733 ,  0.011733 ,\n",
       "           0.16488  ,  0.16488  ,  0.19003  ,  0.19003  ,  0.18605  ,\n",
       "           0.18605  ,  0.036091 ,  0.036091 ,  0.017743 ,  0.017743 ,\n",
       "           0.034771 ,  0.034771 ,  0.67479  ,  0.67479  ,  0.27241  ,\n",
       "           0.27241  ,  0.0089888,  0.0089888,  0.032533 ,  0.032533 ,\n",
       "           0.015969 ,  0.015969 ,  0.40099  ,  0.40099  ,  0.047716 ,\n",
       "           0.047716 ,  0.020583 ,  0.020583 ,  0.022499 ,  0.022499 ,\n",
       "           0.057452 ,  0.057452 ,  0.022605 ,  0.022605 ,  0.060679 ,\n",
       "           0.060679 ,  0.010376 ,  0.010376 ,  0.012433 ,  0.012433 ,\n",
       "           0.0095714,  0.0095714]),\n",
       "   'y_pred': array([0.33383442, 0.33383442, 0.26327354, 0.26327354, 0.26830736,\n",
       "          0.26830736, 0.3009464 , 0.3009464 , 0.35398218, 0.35398218,\n",
       "          0.23352746, 0.23352746, 0.26047877, 0.26047877, 0.31954589,\n",
       "          0.31954589, 0.18706004, 0.18706004, 0.33518372, 0.33518372,\n",
       "          0.23232352, 0.23232352, 0.34220387, 0.34220387, 0.29045029,\n",
       "          0.29045029, 0.2058157 , 0.2058157 , 0.19763146, 0.19763146,\n",
       "          0.29409446, 0.29409446, 0.32724963, 0.32724963, 0.30513352,\n",
       "          0.30513352, 0.34500245, 0.34500245, 0.33476924, 0.33476924,\n",
       "          0.33476507, 0.33476507, 0.18531243, 0.18531243, 0.27892126,\n",
       "          0.27892126, 0.23931349, 0.23931349, 0.1988106 , 0.1988106 ,\n",
       "          0.21739278, 0.21739278, 0.22477991, 0.22477991, 0.22038658,\n",
       "          0.22038658, 0.29901867, 0.29901867, 0.30405249, 0.30405249,\n",
       "          0.33669153, 0.33669153, 0.38972731, 0.38972731, 0.26927259,\n",
       "          0.26927259, 0.2962239 , 0.2962239 , 0.35529102, 0.35529102,\n",
       "          0.22280517, 0.22280517, 0.37092885, 0.37092885, 0.26806866,\n",
       "          0.26806866, 0.377949  , 0.377949  , 0.32619542, 0.32619542,\n",
       "          0.24156083, 0.24156083, 0.23337659, 0.23337659, 0.32983959,\n",
       "          0.32983959, 0.36299476, 0.36299476, 0.34087866, 0.34087866,\n",
       "          0.38074758, 0.38074758, 0.37051437, 0.37051437, 0.3705102 ,\n",
       "          0.3705102 , 0.22105756, 0.22105756, 0.3146664 , 0.3146664 ,\n",
       "          0.27505863, 0.27505863, 0.23455573, 0.23455573, 0.25313791,\n",
       "          0.25313791, 0.26052504, 0.26052504, 0.25613171, 0.25613171,\n",
       "          0.23349161, 0.23349161, 0.26613065, 0.26613065, 0.31916643,\n",
       "          0.31916643, 0.19871171, 0.19871171, 0.22566302, 0.22566302,\n",
       "          0.28473014, 0.28473014, 0.15224429, 0.15224429, 0.30036797,\n",
       "          0.30036797, 0.19750778, 0.19750778, 0.30738812, 0.30738812,\n",
       "          0.25563454, 0.25563454, 0.17099995, 0.17099995, 0.16281571,\n",
       "          0.16281571, 0.25927871, 0.25927871, 0.29243388, 0.29243388,\n",
       "          0.27031778, 0.27031778, 0.3101867 , 0.3101867 , 0.29995349,\n",
       "          0.29995349, 0.29994932, 0.29994932, 0.15049668, 0.15049668,\n",
       "          0.24410552, 0.24410552, 0.20449775, 0.20449775, 0.16399485,\n",
       "          0.16399485, 0.18257703, 0.18257703, 0.18996416, 0.18996416,\n",
       "          0.18557083, 0.18557083, 0.27116447, 0.27116447, 0.32420025,\n",
       "          0.32420025, 0.20374553, 0.20374553, 0.23069684, 0.23069684,\n",
       "          0.28976396, 0.28976396, 0.15727811, 0.15727811, 0.30540179,\n",
       "          0.30540179, 0.20254159, 0.20254159, 0.31242194, 0.31242194,\n",
       "          0.26066836, 0.26066836, 0.17603377, 0.17603377, 0.16784953,\n",
       "          0.16784953, 0.26431252, 0.26431252, 0.2974677 , 0.2974677 ,\n",
       "          0.27535159, 0.27535159, 0.31522052, 0.31522052, 0.30498731,\n",
       "          0.30498731, 0.30498314, 0.30498314, 0.1555305 , 0.1555305 ,\n",
       "          0.24913933, 0.24913933, 0.20953156, 0.20953156, 0.16902867,\n",
       "          0.16902867, 0.18761085, 0.18761085, 0.19499798, 0.19499798,\n",
       "          0.19060465, 0.19060465, 0.35683929, 0.35683929, 0.23638457,\n",
       "          0.23638457, 0.26333588, 0.26333588, 0.322403  , 0.322403  ,\n",
       "          0.18991715, 0.18991715, 0.33804083, 0.33804083, 0.23518063,\n",
       "          0.23518063, 0.34506098, 0.34506098, 0.2933074 , 0.2933074 ,\n",
       "          0.20867281, 0.20867281, 0.20048857, 0.20048857, 0.29695156,\n",
       "          0.29695156, 0.33010674, 0.33010674, 0.30799063, 0.30799063,\n",
       "          0.34785956, 0.34785956, 0.33762635, 0.33762635, 0.33762218,\n",
       "          0.33762218, 0.18816954, 0.18816954, 0.28177837, 0.28177837,\n",
       "          0.2421706 , 0.2421706 , 0.20166771, 0.20166771, 0.22024989,\n",
       "          0.22024989, 0.22763702, 0.22763702, 0.22324369, 0.22324369,\n",
       "          0.28942035, 0.28942035, 0.31637166, 0.31637166, 0.37543878,\n",
       "          0.37543878, 0.24295293, 0.24295293, 0.39107661, 0.39107661,\n",
       "          0.28821641, 0.28821641, 0.39809676, 0.39809676, 0.34634318,\n",
       "          0.34634318, 0.26170859, 0.26170859, 0.25352435, 0.25352435,\n",
       "          0.34998734, 0.34998734, 0.38314252, 0.38314252, 0.36102641,\n",
       "          0.36102641, 0.40089533, 0.40089533, 0.39066213, 0.39066213,\n",
       "          0.39065796, 0.39065796, 0.24120531, 0.24120531, 0.33481415,\n",
       "          0.33481415, 0.29520638, 0.29520638, 0.25470348, 0.25470348,\n",
       "          0.27328567, 0.27328567, 0.2806728 , 0.2806728 , 0.27627947,\n",
       "          0.27627947, 0.19591694, 0.19591694, 0.25498406, 0.25498406,\n",
       "          0.12249821, 0.12249821, 0.27062189, 0.27062189, 0.16776169,\n",
       "          0.16776169, 0.27764204, 0.27764204, 0.22588846, 0.22588846,\n",
       "          0.14125387, 0.14125387, 0.13306963, 0.13306963, 0.22953262,\n",
       "          0.22953262, 0.2626878 , 0.2626878 , 0.24057169, 0.24057169,\n",
       "          0.28044062, 0.28044062, 0.27020741, 0.27020741, 0.27020324,\n",
       "          0.27020324, 0.1207506 , 0.1207506 , 0.21435943, 0.21435943,\n",
       "          0.17475166, 0.17475166, 0.13424877, 0.13424877, 0.15283095,\n",
       "          0.15283095, 0.16021808, 0.16021808, 0.15582475, 0.15582475,\n",
       "          0.28193537, 0.28193537, 0.14944952, 0.14944952, 0.2975732 ,\n",
       "          0.2975732 , 0.194713  , 0.194713  , 0.30459335, 0.30459335,\n",
       "          0.25283977, 0.25283977, 0.16820518, 0.16820518, 0.16002094,\n",
       "          0.16002094, 0.25648393, 0.25648393, 0.28963911, 0.28963911,\n",
       "          0.267523  , 0.267523  , 0.30739192, 0.30739192, 0.29715872,\n",
       "          0.29715872, 0.29715455, 0.29715455, 0.1477019 , 0.1477019 ,\n",
       "          0.24131074, 0.24131074, 0.20170297, 0.20170297, 0.16120007,\n",
       "          0.16120007, 0.17978226, 0.17978226, 0.18716939, 0.18716939,\n",
       "          0.18277606, 0.18277606, 0.20851664, 0.20851664, 0.35664032,\n",
       "          0.35664032, 0.25378012, 0.25378012, 0.36366047, 0.36366047,\n",
       "          0.31190689, 0.31190689, 0.2272723 , 0.2272723 , 0.21908806,\n",
       "          0.21908806, 0.31555105, 0.31555105, 0.34870623, 0.34870623,\n",
       "          0.32659012, 0.32659012, 0.36645904, 0.36645904, 0.35622584,\n",
       "          0.35622584, 0.35622167, 0.35622167, 0.20676902, 0.20676902,\n",
       "          0.30037786, 0.30037786, 0.26077009, 0.26077009, 0.22026719,\n",
       "          0.22026719, 0.23884937, 0.23884937, 0.24623651, 0.24623651,\n",
       "          0.24184318, 0.24184318, 0.22415447, 0.22415447, 0.12129427,\n",
       "          0.12129427, 0.23117462, 0.23117462, 0.17942104, 0.17942104,\n",
       "          0.09478645, 0.09478645, 0.08660221, 0.08660221, 0.18306521,\n",
       "          0.18306521, 0.21622038, 0.21622038, 0.19410428, 0.19410428,\n",
       "          0.2339732 , 0.2339732 , 0.22373999, 0.22373999, 0.22373582,\n",
       "          0.22373582, 0.07428318, 0.07428318, 0.16789202, 0.16789202,\n",
       "          0.12828424, 0.12828424, 0.08778135, 0.08778135, 0.10636353,\n",
       "          0.10636353, 0.11375066, 0.11375066, 0.10935733, 0.10935733,\n",
       "          0.26941795, 0.26941795, 0.3792983 , 0.3792983 , 0.32754472,\n",
       "          0.32754472, 0.24291013, 0.24291013, 0.23472589, 0.23472589,\n",
       "          0.33118888, 0.33118888, 0.36434406, 0.36434406, 0.34222795,\n",
       "          0.34222795, 0.38209688, 0.38209688, 0.37186367, 0.37186367,\n",
       "          0.3718595 , 0.3718595 , 0.22240686, 0.22240686, 0.31601569,\n",
       "          0.31601569, 0.27640792, 0.27640792, 0.23590503, 0.23590503,\n",
       "          0.25448721, 0.25448721, 0.26187434, 0.26187434, 0.25748101,\n",
       "          0.25748101, 0.2764381 , 0.2764381 , 0.22468452, 0.22468452,\n",
       "          0.14004993, 0.14004993, 0.13186569, 0.13186569, 0.22832869,\n",
       "          0.22832869, 0.26148386, 0.26148386, 0.23936776, 0.23936776,\n",
       "          0.27923668, 0.27923668, 0.26900347, 0.26900347, 0.2689993 ,\n",
       "          0.2689993 , 0.11954666, 0.11954666, 0.2131555 , 0.2131555 ,\n",
       "          0.17354773, 0.17354773, 0.13304483, 0.13304483, 0.15162701,\n",
       "          0.15162701, 0.15901414, 0.15901414, 0.15462081, 0.15462081,\n",
       "          0.33456487, 0.33456487, 0.24993028, 0.24993028, 0.24174604,\n",
       "          0.24174604, 0.33820904, 0.33820904, 0.37136421, 0.37136421,\n",
       "          0.34924811, 0.34924811, 0.38911703, 0.38911703, 0.37888382,\n",
       "          0.37888382, 0.37887965, 0.37887965, 0.22942701, 0.22942701,\n",
       "          0.32303585, 0.32303585, 0.28342808, 0.28342808, 0.24292518,\n",
       "          0.24292518, 0.26150736, 0.26150736, 0.26889449, 0.26889449,\n",
       "          0.26450116, 0.26450116, 0.1981767 , 0.1981767 , 0.18999246,\n",
       "          0.18999246, 0.28645545, 0.28645545, 0.31961063, 0.31961063,\n",
       "          0.29749452, 0.29749452, 0.33736344, 0.33736344, 0.32713024,\n",
       "          0.32713024, 0.32712607, 0.32712607, 0.17767342, 0.17767342,\n",
       "          0.27128226, 0.27128226, 0.23167449, 0.23167449, 0.19117159,\n",
       "          0.19117159, 0.20975378, 0.20975378, 0.21714091, 0.21714091,\n",
       "          0.21274758, 0.21274758, 0.10535787, 0.10535787, 0.20182087,\n",
       "          0.20182087, 0.23497604, 0.23497604, 0.21285993, 0.21285993,\n",
       "          0.25272886, 0.25272886, 0.24249565, 0.24249565, 0.24249148,\n",
       "          0.24249148, 0.09303884, 0.09303884, 0.18664767, 0.18664767,\n",
       "          0.1470399 , 0.1470399 , 0.10653701, 0.10653701, 0.12511919,\n",
       "          0.12511919, 0.13250632, 0.13250632, 0.12811299, 0.12811299,\n",
       "          0.19363662, 0.19363662, 0.2267918 , 0.2267918 , 0.20467569,\n",
       "          0.20467569, 0.24454461, 0.24454461, 0.23431141, 0.23431141,\n",
       "          0.23430724, 0.23430724, 0.0848546 , 0.0848546 , 0.17846343,\n",
       "          0.17846343, 0.13885566, 0.13885566, 0.09835276, 0.09835276,\n",
       "          0.11693495, 0.11693495, 0.12432208, 0.12432208, 0.11992875,\n",
       "          0.11992875, 0.32325479, 0.32325479, 0.30113869, 0.30113869,\n",
       "          0.34100761, 0.34100761, 0.3307744 , 0.3307744 , 0.33077023,\n",
       "          0.33077023, 0.18131759, 0.18131759, 0.27492643, 0.27492643,\n",
       "          0.23531866, 0.23531866, 0.19481576, 0.19481576, 0.21339794,\n",
       "          0.21339794, 0.22078508, 0.22078508, 0.21639174, 0.21639174,\n",
       "          0.33429386, 0.33429386, 0.37416279, 0.37416279, 0.36392958,\n",
       "          0.36392958, 0.36392541, 0.36392541, 0.21447277, 0.21447277,\n",
       "          0.3080816 , 0.3080816 , 0.26847383, 0.26847383, 0.22797094,\n",
       "          0.22797094, 0.24655312, 0.24655312, 0.25394025, 0.25394025,\n",
       "          0.24954692, 0.24954692, 0.35204668, 0.35204668, 0.34181347,\n",
       "          0.34181347, 0.3418093 , 0.3418093 , 0.19235666, 0.19235666,\n",
       "          0.2859655 , 0.2859655 , 0.24635773, 0.24635773, 0.20585483,\n",
       "          0.20585483, 0.22443701, 0.22443701, 0.23182415, 0.23182415,\n",
       "          0.22743081, 0.22743081, 0.38168239, 0.38168239, 0.38167822,\n",
       "          0.38167822, 0.23222558, 0.23222558, 0.32583442, 0.32583442,\n",
       "          0.28622665, 0.28622665, 0.24572375, 0.24572375, 0.26430593,\n",
       "          0.26430593, 0.27169307, 0.27169307, 0.26729973, 0.26729973,\n",
       "          0.37144502, 0.37144502, 0.22199237, 0.22199237, 0.31560121,\n",
       "          0.31560121, 0.27599344, 0.27599344, 0.23549054, 0.23549054,\n",
       "          0.25407273, 0.25407273, 0.26145986, 0.26145986, 0.25706653,\n",
       "          0.25706653, 0.2219882 , 0.2219882 , 0.31559704, 0.31559704,\n",
       "          0.27598927, 0.27598927, 0.23548637, 0.23548637, 0.25406856,\n",
       "          0.25406856, 0.26145569, 0.26145569, 0.25706236, 0.25706236,\n",
       "          0.1661444 , 0.1661444 , 0.12653663, 0.12653663, 0.08603373,\n",
       "          0.08603373, 0.10461591, 0.10461591, 0.11200305, 0.11200305,\n",
       "          0.10760971, 0.10760971, 0.22014547, 0.22014547, 0.17964257,\n",
       "          0.17964257, 0.19822475, 0.19822475, 0.20561188, 0.20561188,\n",
       "          0.20121855, 0.20121855, 0.1400348 , 0.1400348 , 0.15861698,\n",
       "          0.15861698, 0.16600411, 0.16600411, 0.16161078, 0.16161078,\n",
       "          0.11811408, 0.11811408, 0.12550122, 0.12550122, 0.12110788,\n",
       "          0.12110788, 0.1440834 , 0.1440834 , 0.13969006, 0.13969006,\n",
       "          0.1470772 , 0.1470772 ]),\n",
       "   'feature_importances': array([ 4.77145776e-05, -8.31895214e-04,  1.16877302e-03,  4.77145776e-05,\n",
       "          -8.31895214e-04,  1.16877302e-03]),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.028683523809289957,\n",
       "    'mae': 0.13111115307900528,\n",
       "    'r2': 0.13913146401454346,\n",
       "    'pearson_corr': 0.37300330410324356},\n",
       "   'best_val_score': -0.027168831982130794,\n",
       "   'test_metrics': {'mse': 0.027077622998804823,\n",
       "    'mae': 0.12142571510515247,\n",
       "    'r2': 0.13593317511949277,\n",
       "    'pearson_corr': 0.39326999869588375,\n",
       "    'connectome_corr': 0.2099200137040555,\n",
       "    'connectome_r2': -0.2921565293186986,\n",
       "    'geodesic_distance': 6.5480889316219235},\n",
       "   'y_true': array([ 0.33148  ,  0.33148  ,  0.25656  ,  0.25656  ,  0.20587  ,\n",
       "           0.20587  ,  0.31174  ,  0.31174  ,  0.21878  ,  0.21878  ,\n",
       "           0.21532  ,  0.21532  ,  0.17031  ,  0.17031  ,  0.2146   ,\n",
       "           0.2146   ,  0.17066  ,  0.17066  ,  0.22792  ,  0.22792  ,\n",
       "           0.16084  ,  0.16084  ,  0.35445  ,  0.35445  ,  0.29327  ,\n",
       "           0.29327  ,  0.27628  ,  0.27628  ,  0.16877  ,  0.16877  ,\n",
       "           0.25129  ,  0.25129  ,  0.23922  ,  0.23922  ,  0.24166  ,\n",
       "           0.24166  ,  0.23041  ,  0.23041  ,  0.1454   ,  0.1454   ,\n",
       "           0.2518   ,  0.2518   ,  0.2819   ,  0.2819   ,  0.1944   ,\n",
       "           0.1944   ,  0.20706  ,  0.20706  ,  0.18931  ,  0.18931  ,\n",
       "           0.1788   ,  0.1788   ,  0.09972  ,  0.09972  ,  0.73677  ,\n",
       "           0.73677  ,  0.52629  ,  0.52629  ,  0.64905  ,  0.64905  ,\n",
       "           0.37377  ,  0.37377  ,  0.28208  ,  0.28208  ,  0.17852  ,\n",
       "           0.17852  ,  0.13445  ,  0.13445  ,  0.23099  ,  0.23099  ,\n",
       "           0.18371  ,  0.18371  ,  0.32211  ,  0.32211  ,  0.5291   ,\n",
       "           0.5291   ,  0.74704  ,  0.74704  ,  0.49516  ,  0.49516  ,\n",
       "           0.3926   ,  0.3926   ,  0.57732  ,  0.57732  ,  0.39736  ,\n",
       "           0.39736  ,  0.46903  ,  0.46903  ,  0.45497  ,  0.45497  ,\n",
       "           0.25314  ,  0.25314  ,  0.28381  ,  0.28381  ,  0.29232  ,\n",
       "           0.29232  ,  0.12649  ,  0.12649  ,  0.17147  ,  0.17147  ,\n",
       "           0.11213  ,  0.11213  ,  0.14714  ,  0.14714  ,  0.16611  ,\n",
       "           0.16611  ,  0.5164   ,  0.5164   ,  0.55261  ,  0.55261  ,\n",
       "           0.36462  ,  0.36462  ,  0.23562  ,  0.23562  ,  0.18814  ,\n",
       "           0.18814  ,  0.10769  ,  0.10769  ,  0.27835  ,  0.27835  ,\n",
       "           0.16852  ,  0.16852  ,  0.2832   ,  0.2832   ,  0.99059  ,\n",
       "           0.99059  ,  1.3686   ,  1.3686   ,  0.51897  ,  0.51897  ,\n",
       "           0.39011  ,  0.39011  ,  0.46333  ,  0.46333  ,  0.42757  ,\n",
       "           0.42757  ,  0.5324   ,  0.5324   ,  0.49968  ,  0.49968  ,\n",
       "           0.18134  ,  0.18134  ,  0.34009  ,  0.34009  ,  0.35677  ,\n",
       "           0.35677  ,  0.088168 ,  0.088168 ,  0.15894  ,  0.15894  ,\n",
       "           0.056894 ,  0.056894 ,  0.12265  ,  0.12265  ,  0.14579  ,\n",
       "           0.14579  ,  0.39599  ,  0.39599  ,  0.32793  ,  0.32793  ,\n",
       "           0.3222   ,  0.3222   ,  0.17706  ,  0.17706  ,  0.17992  ,\n",
       "           0.17992  ,  0.25264  ,  0.25264  ,  0.22991  ,  0.22991  ,\n",
       "           0.25632  ,  0.25632  ,  0.39837  ,  0.39837  ,  0.49432  ,\n",
       "           0.49432  ,  0.57933  ,  0.57933  ,  0.6152   ,  0.6152   ,\n",
       "           0.38098  ,  0.38098  ,  0.30403  ,  0.30403  ,  0.3698   ,\n",
       "           0.3698   ,  0.43567  ,  0.43567  ,  0.2435   ,  0.2435   ,\n",
       "           0.15002  ,  0.15002  ,  0.18289  ,  0.18289  ,  0.14929  ,\n",
       "           0.14929  ,  0.17282  ,  0.17282  ,  0.14255  ,  0.14255  ,\n",
       "           0.16803  ,  0.16803  ,  0.20912  ,  0.20912  ,  0.31918  ,\n",
       "           0.31918  ,  0.15832  ,  0.15832  ,  0.14124  ,  0.14124  ,\n",
       "           0.12168  ,  0.12168  ,  0.21204  ,  0.21204  ,  0.090976 ,\n",
       "           0.090976 ,  0.34417  ,  0.34417  ,  0.48272  ,  0.48272  ,\n",
       "           0.57398  ,  0.57398  ,  0.37793  ,  0.37793  ,  0.27984  ,\n",
       "           0.27984  ,  0.39158  ,  0.39158  ,  0.39081  ,  0.39081  ,\n",
       "           0.46012  ,  0.46012  ,  0.42524  ,  0.42524  ,  0.1816   ,\n",
       "           0.1816   ,  0.34134  ,  0.34134  ,  0.38275  ,  0.38275  ,\n",
       "           0.05783  ,  0.05783  ,  0.17506  ,  0.17506  ,  0.020916 ,\n",
       "           0.020916 ,  0.21375  ,  0.21375  ,  0.13493  ,  0.13493  ,\n",
       "           0.18505  ,  0.18505  ,  0.23936  ,  0.23936  ,  0.10311  ,\n",
       "           0.10311  ,  0.15239  ,  0.15239  ,  0.097861 ,  0.097861 ,\n",
       "           0.18874  ,  0.18874  ,  0.33161  ,  0.33161  ,  0.35783  ,\n",
       "           0.35783  ,  0.47651  ,  0.47651  ,  0.28464  ,  0.28464  ,\n",
       "           0.38285  ,  0.38285  ,  0.49751  ,  0.49751  ,  0.43599  ,\n",
       "           0.43599  ,  0.44142  ,  0.44142  ,  0.18     ,  0.18     ,\n",
       "           0.31861  ,  0.31861  ,  0.25083  ,  0.25083  ,  0.072256 ,\n",
       "           0.072256 ,  0.20171  ,  0.20171  ,  0.08234  ,  0.08234  ,\n",
       "           0.10307  ,  0.10307  ,  0.11407  ,  0.11407  ,  0.37818  ,\n",
       "           0.37818  ,  0.70079  ,  0.70079  ,  0.22818  ,  0.22818  ,\n",
       "           0.4826   ,  0.4826   ,  0.18201  ,  0.18201  ,  0.22923  ,\n",
       "           0.22923  ,  0.22029  ,  0.22029  ,  0.32249  ,  0.32249  ,\n",
       "           0.29678  ,  0.29678  ,  0.37948  ,  0.37948  ,  0.1018   ,\n",
       "           0.1018   ,  0.11987  ,  0.11987  ,  0.15403  ,  0.15403  ,\n",
       "           0.38225  ,  0.38225  ,  0.047428 ,  0.047428 ,  0.17087  ,\n",
       "           0.17087  ,  0.48353  ,  0.48353  ,  0.28801  ,  0.28801  ,\n",
       "           0.49612  ,  0.49612  ,  0.19856  ,  0.19856  ,  0.15518  ,\n",
       "           0.15518  ,  0.35209  ,  0.35209  ,  0.15824  ,  0.15824  ,\n",
       "           0.24511  ,  0.24511  ,  0.11831  ,  0.11831  ,  0.21464  ,\n",
       "           0.21464  ,  0.1765   ,  0.1765   ,  0.24353  ,  0.24353  ,\n",
       "           0.16734  ,  0.16734  ,  0.26438  ,  0.26438  ,  0.23999  ,\n",
       "           0.23999  ,  0.19193  ,  0.19193  ,  0.22554  ,  0.22554  ,\n",
       "           0.20534  ,  0.20534  ,  0.18197  ,  0.18197  ,  0.17313  ,\n",
       "           0.17313  ,  0.2402   ,  0.2402   ,  0.34958  ,  0.34958  ,\n",
       "           0.33265  ,  0.33265  ,  0.12881  ,  0.12881  ,  0.093283 ,\n",
       "           0.093283 ,  0.24612  ,  0.24612  ,  0.54612  ,  0.54612  ,\n",
       "           0.1315   ,  0.1315   ,  0.17622  ,  0.17622  ,  0.094095 ,\n",
       "           0.094095 ,  0.20863  ,  0.20863  ,  0.18872  ,  0.18872  ,\n",
       "           0.22455  ,  0.22455  ,  0.037041 ,  0.037041 ,  0.067595 ,\n",
       "           0.067595 ,  0.071538 ,  0.071538 ,  0.26982  ,  0.26982  ,\n",
       "           0.10338  ,  0.10338  ,  0.23899  ,  0.23899  ,  0.59564  ,\n",
       "           0.59564  ,  0.30835  ,  0.30835  ,  0.58107  ,  0.58107  ,\n",
       "           0.36845  ,  0.36845  ,  0.11855  ,  0.11855  ,  0.61204  ,\n",
       "           0.61204  ,  0.37042  ,  0.37042  ,  0.31008  ,  0.31008  ,\n",
       "           0.22684  ,  0.22684  ,  0.28242  ,  0.28242  ,  0.23092  ,\n",
       "           0.23092  ,  0.24189  ,  0.24189  ,  0.068368 ,  0.068368 ,\n",
       "           0.2144   ,  0.2144   ,  0.11238  ,  0.11238  ,  0.22456  ,\n",
       "           0.22456  ,  0.074981 ,  0.074981 ,  0.45418  ,  0.45418  ,\n",
       "           0.38823  ,  0.38823  ,  0.22955  ,  0.22955  ,  0.24779  ,\n",
       "           0.24779  ,  0.46636  ,  0.46636  ,  0.231    ,  0.231    ,\n",
       "           0.26248  ,  0.26248  ,  0.21753  ,  0.21753  ,  0.13705  ,\n",
       "           0.13705  ,  0.25632  ,  0.25632  ,  0.21373  ,  0.21373  ,\n",
       "           0.23771  ,  0.23771  , -0.029437 , -0.029437 ,  0.11596  ,\n",
       "           0.11596  ,  0.028802 ,  0.028802 ,  0.31914  ,  0.31914  ,\n",
       "           0.011954 ,  0.011954 ,  0.46983  ,  0.46983  ,  0.70364  ,\n",
       "           0.70364  ,  0.28605  ,  0.28605  ,  0.52316  ,  0.52316  ,\n",
       "           0.51308  ,  0.51308  ,  0.23119  ,  0.23119  ,  0.26831  ,\n",
       "           0.26831  ,  0.26953  ,  0.26953  ,  0.28503  ,  0.28503  ,\n",
       "           0.22905  ,  0.22905  ,  0.29339  ,  0.29339  ,  0.16203  ,\n",
       "           0.16203  ,  0.23115  ,  0.23115  ,  0.18003  ,  0.18003  ,\n",
       "           0.28613  ,  0.28613  ,  0.09553  ,  0.09553  ,  0.27519  ,\n",
       "           0.27519  ,  0.21045  ,  0.21045  ,  0.19372  ,  0.19372  ,\n",
       "           0.15535  ,  0.15535  ,  0.28494  ,  0.28494  ,  0.22675  ,\n",
       "           0.22675  ,  1.0155   ,  1.0155   ,  0.46736  ,  0.46736  ,\n",
       "           0.3253   ,  0.3253   ,  0.38513  ,  0.38513  ,  0.38889  ,\n",
       "           0.38889  ,  0.47048  ,  0.47048  ,  0.43624  ,  0.43624  ,\n",
       "           0.16823  ,  0.16823  ,  0.32943  ,  0.32943  ,  0.38119  ,\n",
       "           0.38119  ,  0.15107  ,  0.15107  ,  0.21734  ,  0.21734  ,\n",
       "           0.12138  ,  0.12138  ,  0.19254  ,  0.19254  ,  0.13583  ,\n",
       "           0.13583  ,  0.50327  ,  0.50327  ,  0.36506  ,  0.36506  ,\n",
       "           0.45718  ,  0.45718  ,  0.42915  ,  0.42915  ,  0.52831  ,\n",
       "           0.52831  ,  0.49068  ,  0.49068  ,  0.17272  ,  0.17272  ,\n",
       "           0.34581  ,  0.34581  ,  0.34235  ,  0.34235  ,  0.065032 ,\n",
       "           0.065032 ,  0.1468   ,  0.1468   ,  0.042703 ,  0.042703 ,\n",
       "           0.091022 ,  0.091022 ,  0.13416  ,  0.13416  ,  0.54402  ,\n",
       "           0.54402  ,  0.52982  ,  0.52982  ,  0.51557  ,  0.51557  ,\n",
       "           0.48936  ,  0.48936  ,  0.50885  ,  0.50885  ,  0.25358  ,\n",
       "           0.25358  ,  0.29956  ,  0.29956  ,  0.31516  ,  0.31516  ,\n",
       "           0.19706  ,  0.19706  ,  0.27752  ,  0.27752  ,  0.17393  ,\n",
       "           0.17393  ,  0.22682  ,  0.22682  ,  0.20172  ,  0.20172  ,\n",
       "           0.35103  ,  0.35103  ,  0.29129  ,  0.29129  ,  0.29662  ,\n",
       "           0.29662  ,  0.37846  ,  0.37846  ,  0.2243   ,  0.2243   ,\n",
       "           0.12522  ,  0.12522  ,  0.15747  ,  0.15747  ,  0.16605  ,\n",
       "           0.16605  ,  0.17666  ,  0.17666  ,  0.15819  ,  0.15819  ,\n",
       "           0.17849  ,  0.17849  ,  0.18453  ,  0.18453  ,  0.45635  ,\n",
       "           0.45635  ,  0.46468  ,  0.46468  ,  0.45726  ,  0.45726  ,\n",
       "           0.34839  ,  0.34839  ,  0.3866   ,  0.3866   ,  0.39813  ,\n",
       "           0.39813  ,  0.35559  ,  0.35559  ,  0.28283  ,  0.28283  ,\n",
       "           0.29687  ,  0.29687  ,  0.24929  ,  0.24929  ,  0.1526   ,\n",
       "           0.1526   ,  0.57962  ,  0.57962  ,  0.67896  ,  0.67896  ,\n",
       "           0.1415   ,  0.1415   ,  0.57955  ,  0.57955  ,  0.30799  ,\n",
       "           0.30799  , -0.0033665, -0.0033665,  0.27324  ,  0.27324  ,\n",
       "           0.027971 ,  0.027971 ,  0.077822 ,  0.077822 ,  0.087999 ,\n",
       "           0.087999 ,  0.60946  ,  0.60946  ,  0.15583  ,  0.15583  ,\n",
       "           0.53966  ,  0.53966  ,  0.44818  ,  0.44818  ,  0.10311  ,\n",
       "           0.10311  ,  0.19167  ,  0.19167  ,  0.071054 ,  0.071054 ,\n",
       "           0.18039  ,  0.18039  ,  0.11114  ,  0.11114  ,  0.1465   ,\n",
       "           0.1465   ,  0.50191  ,  0.50191  ,  0.27017  ,  0.27017  ,\n",
       "           0.0045791,  0.0045791,  0.18719  ,  0.18719  ,  0.047998 ,\n",
       "           0.047998 ,  0.084946 ,  0.084946 ,  0.096601 ,  0.096601 ,\n",
       "           0.067615 ,  0.067615 ,  0.15926  ,  0.15926  ,  0.37743  ,\n",
       "           0.37743  ,  0.27607  ,  0.27607  ,  0.41946  ,  0.41946  ,\n",
       "           0.21872  ,  0.21872  ,  0.2139   ,  0.2139   ,  0.5309   ,\n",
       "           0.5309   ,  0.17173  ,  0.17173  ,  0.1977   ,  0.1977   ,\n",
       "           0.15914  ,  0.15914  ,  0.31658  ,  0.31658  ,  0.027    ,\n",
       "           0.027    ,  0.4958   ,  0.4958   ,  0.25637  ,  0.25637  ,\n",
       "           0.25208  ,  0.25208  ,  0.60549  ,  0.60549  ,  0.13979  ,\n",
       "           0.13979  ,  0.36196  ,  0.36196  ,  0.85576  ,  0.85576  ,\n",
       "           0.71396  ,  0.71396  ,  0.17641  ,  0.17641  ,  0.43945  ,\n",
       "           0.43945  ,  0.28797  ,  0.28797  ,  0.15046  ,  0.15046  ,\n",
       "           0.48791  ,  0.48791  ,  0.14908  ,  0.14908  ,  0.18887  ,\n",
       "           0.18887  ]),\n",
       "   'y_pred': array([0.32381215, 0.32381215, 0.35152821, 0.35152821, 0.34154822,\n",
       "          0.34154822, 0.36672955, 0.36672955, 0.24352973, 0.24352973,\n",
       "          0.27592758, 0.27592758, 0.21814765, 0.21814765, 0.29592279,\n",
       "          0.29592279, 0.31334034, 0.31334034, 0.33491212, 0.33491212,\n",
       "          0.24933442, 0.24933442, 0.32029002, 0.32029002, 0.3580601 ,\n",
       "          0.3580601 , 0.26942619, 0.26942619, 0.29002242, 0.29002242,\n",
       "          0.30083982, 0.30083982, 0.24281736, 0.24281736, 0.33026808,\n",
       "          0.33026808, 0.30817908, 0.30817908, 0.20627405, 0.20627405,\n",
       "          0.24534753, 0.24534753, 0.35608146, 0.35608146, 0.32919199,\n",
       "          0.32919199, 0.20928147, 0.20928147, 0.25839953, 0.25839953,\n",
       "          0.28829403, 0.28829403, 0.25037865, 0.25037865, 0.35007241,\n",
       "          0.35007241, 0.34009242, 0.34009242, 0.36527375, 0.36527375,\n",
       "          0.24207394, 0.24207394, 0.27447178, 0.27447178, 0.21669185,\n",
       "          0.21669185, 0.294467  , 0.294467  , 0.31188455, 0.31188455,\n",
       "          0.33345632, 0.33345632, 0.24787862, 0.24787862, 0.31883423,\n",
       "          0.31883423, 0.3566043 , 0.3566043 , 0.26797039, 0.26797039,\n",
       "          0.28856662, 0.28856662, 0.29938402, 0.29938402, 0.24136156,\n",
       "          0.24136156, 0.32881229, 0.32881229, 0.30672329, 0.30672329,\n",
       "          0.20481825, 0.20481825, 0.24389173, 0.24389173, 0.35462567,\n",
       "          0.35462567, 0.3277362 , 0.3277362 , 0.20782567, 0.20782567,\n",
       "          0.25694373, 0.25694373, 0.28683823, 0.28683823, 0.24892285,\n",
       "          0.24892285, 0.36780848, 0.36780848, 0.39298981, 0.39298981,\n",
       "          0.26978999, 0.26978999, 0.30218784, 0.30218784, 0.24440791,\n",
       "          0.24440791, 0.32218305, 0.32218305, 0.3396006 , 0.3396006 ,\n",
       "          0.36117238, 0.36117238, 0.27559468, 0.27559468, 0.34655028,\n",
       "          0.34655028, 0.38432036, 0.38432036, 0.29568645, 0.29568645,\n",
       "          0.31628268, 0.31628268, 0.32710008, 0.32710008, 0.26907762,\n",
       "          0.26907762, 0.35652834, 0.35652834, 0.33443934, 0.33443934,\n",
       "          0.23253431, 0.23253431, 0.27160779, 0.27160779, 0.38234172,\n",
       "          0.38234172, 0.35545225, 0.35545225, 0.23554173, 0.23554173,\n",
       "          0.28465979, 0.28465979, 0.31455429, 0.31455429, 0.27663891,\n",
       "          0.27663891, 0.38300982, 0.38300982, 0.25981001, 0.25981001,\n",
       "          0.29220785, 0.29220785, 0.23442792, 0.23442792, 0.31220307,\n",
       "          0.31220307, 0.32962062, 0.32962062, 0.35119239, 0.35119239,\n",
       "          0.26561469, 0.26561469, 0.3365703 , 0.3365703 , 0.37434037,\n",
       "          0.37434037, 0.28570646, 0.28570646, 0.30630269, 0.30630269,\n",
       "          0.31712009, 0.31712009, 0.25909763, 0.25909763, 0.34654835,\n",
       "          0.34654835, 0.32445935, 0.32445935, 0.22255432, 0.22255432,\n",
       "          0.2616278 , 0.2616278 , 0.37236174, 0.37236174, 0.34547227,\n",
       "          0.34547227, 0.22556174, 0.22556174, 0.2746798 , 0.2746798 ,\n",
       "          0.3045743 , 0.3045743 , 0.26665892, 0.26665892, 0.28499133,\n",
       "          0.28499133, 0.31738918, 0.31738918, 0.25960925, 0.25960925,\n",
       "          0.33738439, 0.33738439, 0.35480194, 0.35480194, 0.37637372,\n",
       "          0.37637372, 0.29079602, 0.29079602, 0.36175162, 0.36175162,\n",
       "          0.3995217 , 0.3995217 , 0.31088779, 0.31088779, 0.33148402,\n",
       "          0.33148402, 0.34230142, 0.34230142, 0.28427896, 0.28427896,\n",
       "          0.37172968, 0.37172968, 0.34964068, 0.34964068, 0.24773565,\n",
       "          0.24773565, 0.28680913, 0.28680913, 0.39754306, 0.39754306,\n",
       "          0.37065359, 0.37065359, 0.25074307, 0.25074307, 0.29986113,\n",
       "          0.29986113, 0.32975563, 0.32975563, 0.29184025, 0.29184025,\n",
       "          0.19418937, 0.19418937, 0.13640943, 0.13640943, 0.21418458,\n",
       "          0.21418458, 0.23160213, 0.23160213, 0.25317391, 0.25317391,\n",
       "          0.16759621, 0.16759621, 0.23855181, 0.23855181, 0.27632189,\n",
       "          0.27632189, 0.18768797, 0.18768797, 0.2082842 , 0.2082842 ,\n",
       "          0.2191016 , 0.2191016 , 0.16107914, 0.16107914, 0.24852987,\n",
       "          0.24852987, 0.22644087, 0.22644087, 0.12453583, 0.12453583,\n",
       "          0.16360931, 0.16360931, 0.27434325, 0.27434325, 0.24745378,\n",
       "          0.24745378, 0.12754326, 0.12754326, 0.17666132, 0.17666132,\n",
       "          0.20655582, 0.20655582, 0.16864043, 0.16864043, 0.16880728,\n",
       "          0.16880728, 0.24658242, 0.24658242, 0.26399997, 0.26399997,\n",
       "          0.28557175, 0.28557175, 0.19999405, 0.19999405, 0.27094966,\n",
       "          0.27094966, 0.30871973, 0.30871973, 0.22008582, 0.22008582,\n",
       "          0.24068205, 0.24068205, 0.25149945, 0.25149945, 0.19347699,\n",
       "          0.19347699, 0.28092771, 0.28092771, 0.25883871, 0.25883871,\n",
       "          0.15693368, 0.15693368, 0.19600716, 0.19600716, 0.3067411 ,\n",
       "          0.3067411 , 0.27985162, 0.27985162, 0.1599411 , 0.1599411 ,\n",
       "          0.20905916, 0.20905916, 0.23895366, 0.23895366, 0.20103828,\n",
       "          0.20103828, 0.18880249, 0.18880249, 0.20622004, 0.20622004,\n",
       "          0.22779182, 0.22779182, 0.14221412, 0.14221412, 0.21316972,\n",
       "          0.21316972, 0.2509398 , 0.2509398 , 0.16230588, 0.16230588,\n",
       "          0.18290211, 0.18290211, 0.19371951, 0.19371951, 0.13569706,\n",
       "          0.13569706, 0.22314778, 0.22314778, 0.20105878, 0.20105878,\n",
       "          0.09915374, 0.09915374, 0.13822722, 0.13822722, 0.24896116,\n",
       "          0.24896116, 0.22207169, 0.22207169, 0.10216117, 0.10216117,\n",
       "          0.15127923, 0.15127923, 0.18117373, 0.18117373, 0.14325835,\n",
       "          0.14325835, 0.28399519, 0.28399519, 0.30556696, 0.30556696,\n",
       "          0.21998927, 0.21998927, 0.29094487, 0.29094487, 0.32871495,\n",
       "          0.32871495, 0.24008103, 0.24008103, 0.26067726, 0.26067726,\n",
       "          0.27149466, 0.27149466, 0.2134722 , 0.2134722 , 0.30092293,\n",
       "          0.30092293, 0.27883393, 0.27883393, 0.17692889, 0.17692889,\n",
       "          0.21600237, 0.21600237, 0.32673631, 0.32673631, 0.29984684,\n",
       "          0.29984684, 0.17993632, 0.17993632, 0.22905437, 0.22905437,\n",
       "          0.25894887, 0.25894887, 0.22103349, 0.22103349, 0.32298452,\n",
       "          0.32298452, 0.23740682, 0.23740682, 0.30836242, 0.30836242,\n",
       "          0.3461325 , 0.3461325 , 0.25749858, 0.25749858, 0.27809481,\n",
       "          0.27809481, 0.28891221, 0.28891221, 0.23088975, 0.23088975,\n",
       "          0.31834048, 0.31834048, 0.29625148, 0.29625148, 0.19434644,\n",
       "          0.19434644, 0.23341992, 0.23341992, 0.34415386, 0.34415386,\n",
       "          0.31726439, 0.31726439, 0.19735387, 0.19735387, 0.24647193,\n",
       "          0.24647193, 0.27636642, 0.27636642, 0.23845104, 0.23845104,\n",
       "          0.25897859, 0.25897859, 0.3299342 , 0.3299342 , 0.36770427,\n",
       "          0.36770427, 0.27907036, 0.27907036, 0.29966659, 0.29966659,\n",
       "          0.31048399, 0.31048399, 0.25246153, 0.25246153, 0.33991225,\n",
       "          0.33991225, 0.31782325, 0.31782325, 0.21591822, 0.21591822,\n",
       "          0.2549917 , 0.2549917 , 0.36572564, 0.36572564, 0.33883617,\n",
       "          0.33883617, 0.21892564, 0.21892564, 0.2680437 , 0.2680437 ,\n",
       "          0.2979382 , 0.2979382 , 0.26002282, 0.26002282, 0.2443565 ,\n",
       "          0.2443565 , 0.28212657, 0.28212657, 0.19349266, 0.19349266,\n",
       "          0.21408889, 0.21408889, 0.22490629, 0.22490629, 0.16688383,\n",
       "          0.16688383, 0.25433456, 0.25433456, 0.23224556, 0.23224556,\n",
       "          0.13034052, 0.13034052, 0.169414  , 0.169414  , 0.28014794,\n",
       "          0.28014794, 0.25325847, 0.25325847, 0.13334794, 0.13334794,\n",
       "          0.182466  , 0.182466  , 0.2123605 , 0.2123605 , 0.17444512,\n",
       "          0.17444512, 0.35308218, 0.35308218, 0.26444826, 0.26444826,\n",
       "          0.28504449, 0.28504449, 0.29586189, 0.29586189, 0.23783943,\n",
       "          0.23783943, 0.32529016, 0.32529016, 0.30320116, 0.30320116,\n",
       "          0.20129612, 0.20129612, 0.2403696 , 0.2403696 , 0.35110354,\n",
       "          0.35110354, 0.32421407, 0.32421407, 0.20430355, 0.20430355,\n",
       "          0.25342161, 0.25342161, 0.2833161 , 0.2833161 , 0.24540072,\n",
       "          0.24540072, 0.30221834, 0.30221834, 0.32281457, 0.32281457,\n",
       "          0.33363197, 0.33363197, 0.27560951, 0.27560951, 0.36306024,\n",
       "          0.36306024, 0.34097124, 0.34097124, 0.2390662 , 0.2390662 ,\n",
       "          0.27813968, 0.27813968, 0.38887362, 0.38887362, 0.36198415,\n",
       "          0.36198415, 0.24207362, 0.24207362, 0.29119168, 0.29119168,\n",
       "          0.32108618, 0.32108618, 0.2831708 , 0.2831708 , 0.23418065,\n",
       "          0.23418065, 0.24499805, 0.24499805, 0.1869756 , 0.1869756 ,\n",
       "          0.27442632, 0.27442632, 0.25233732, 0.25233732, 0.15043228,\n",
       "          0.15043228, 0.18950576, 0.18950576, 0.3002397 , 0.3002397 ,\n",
       "          0.27335023, 0.27335023, 0.15343971, 0.15343971, 0.20255777,\n",
       "          0.20255777, 0.23245227, 0.23245227, 0.19453688, 0.19453688,\n",
       "          0.26559428, 0.26559428, 0.20757183, 0.20757183, 0.29502255,\n",
       "          0.29502255, 0.27293355, 0.27293355, 0.17102851, 0.17102851,\n",
       "          0.21010199, 0.21010199, 0.32083593, 0.32083593, 0.29394646,\n",
       "          0.29394646, 0.17403594, 0.17403594, 0.223154  , 0.223154  ,\n",
       "          0.2530485 , 0.2530485 , 0.21513311, 0.21513311, 0.21838923,\n",
       "          0.21838923, 0.30583995, 0.30583995, 0.28375095, 0.28375095,\n",
       "          0.18184591, 0.18184591, 0.22091939, 0.22091939, 0.33165333,\n",
       "          0.33165333, 0.30476386, 0.30476386, 0.18485334, 0.18485334,\n",
       "          0.2339714 , 0.2339714 , 0.2638659 , 0.2638659 , 0.22595051,\n",
       "          0.22595051, 0.24781749, 0.24781749, 0.22572849, 0.22572849,\n",
       "          0.12382346, 0.12382346, 0.16289694, 0.16289694, 0.27363087,\n",
       "          0.27363087, 0.2467414 , 0.2467414 , 0.12683088, 0.12683088,\n",
       "          0.17594894, 0.17594894, 0.20584344, 0.20584344, 0.16792806,\n",
       "          0.16792806, 0.31317922, 0.31317922, 0.21127418, 0.21127418,\n",
       "          0.25034766, 0.25034766, 0.3610816 , 0.3610816 , 0.33419213,\n",
       "          0.33419213, 0.21428161, 0.21428161, 0.26339966, 0.26339966,\n",
       "          0.29329416, 0.29329416, 0.25537878, 0.25537878, 0.18918518,\n",
       "          0.18918518, 0.22825866, 0.22825866, 0.3389926 , 0.3389926 ,\n",
       "          0.31210313, 0.31210313, 0.1921926 , 0.1921926 , 0.24131066,\n",
       "          0.24131066, 0.27120516, 0.27120516, 0.23328978, 0.23328978,\n",
       "          0.12635362, 0.12635362, 0.23708756, 0.23708756, 0.21019809,\n",
       "          0.21019809, 0.09028757, 0.09028757, 0.13940563, 0.13940563,\n",
       "          0.16930013, 0.16930013, 0.13138475, 0.13138475, 0.27616104,\n",
       "          0.27616104, 0.24927157, 0.24927157, 0.12936105, 0.12936105,\n",
       "          0.17847911, 0.17847911, 0.20837361, 0.20837361, 0.17045822,\n",
       "          0.17045822, 0.36000551, 0.36000551, 0.24009499, 0.24009499,\n",
       "          0.28921305, 0.28921305, 0.31910755, 0.31910755, 0.28119216,\n",
       "          0.28119216, 0.21320552, 0.21320552, 0.26232358, 0.26232358,\n",
       "          0.29221807, 0.29221807, 0.25430269, 0.25430269, 0.14241305,\n",
       "          0.14241305, 0.17230755, 0.17230755, 0.13439217, 0.13439217,\n",
       "          0.22142561, 0.22142561, 0.18351023, 0.18351023, 0.21340473,\n",
       "          0.21340473]),\n",
       "   'feature_importances': array([-1.68313985e-05, -8.57501781e-04,  1.21270061e-03, -1.68313985e-05,\n",
       "          -8.57501781e-04,  1.21270061e-03]),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.028742649467198428,\n",
       "    'mae': 0.1312452749281661,\n",
       "    'r2': 0.13804170515059944,\n",
       "    'pearson_corr': 0.3715396425479373},\n",
       "   'best_val_score': -0.027513927619089013,\n",
       "   'test_metrics': {'mse': 0.026180488551267433,\n",
       "    'mae': 0.12353691054843409,\n",
       "    'r2': 0.14913020873577676,\n",
       "    'pearson_corr': 0.3871272882847764,\n",
       "    'connectome_corr': 0.16044218073695365,\n",
       "    'connectome_r2': -3.767436413630827,\n",
       "    'geodesic_distance': 5.767893088793565},\n",
       "   'y_true': array([ 0.81838 ,  0.81838 ,  0.44134 ,  0.44134 ,  0.3922  ,  0.3922  ,\n",
       "           0.33937 ,  0.33937 ,  0.31612 ,  0.31612 ,  0.34407 ,  0.34407 ,\n",
       "           0.1723  ,  0.1723  ,  0.26053 ,  0.26053 ,  0.9718  ,  0.9718  ,\n",
       "           0.733   ,  0.733   ,  0.40486 ,  0.40486 ,  0.3934  ,  0.3934  ,\n",
       "           0.39635 ,  0.39635 ,  0.39357 ,  0.39357 ,  0.3708  ,  0.3708  ,\n",
       "           0.17249 ,  0.17249 ,  0.13774 ,  0.13774 ,  0.24104 ,  0.24104 ,\n",
       "           0.34322 ,  0.34322 ,  0.22774 ,  0.22774 ,  0.14728 ,  0.14728 ,\n",
       "           0.018301,  0.018301,  0.097647,  0.097647,  0.11349 ,  0.11349 ,\n",
       "           0.14274 ,  0.14274 ,  0.087701,  0.087701,  0.14287 ,  0.14287 ,\n",
       "           0.4315  ,  0.4315  ,  0.4144  ,  0.4144  ,  0.30494 ,  0.30494 ,\n",
       "           0.33556 ,  0.33556 ,  0.35054 ,  0.35054 ,  0.13225 ,  0.13225 ,\n",
       "           0.20896 ,  0.20896 ,  0.88472 ,  0.88472 ,  0.56087 ,  0.56087 ,\n",
       "           0.40877 ,  0.40877 ,  0.40643 ,  0.40643 ,  0.37281 ,  0.37281 ,\n",
       "           0.35164 ,  0.35164 ,  0.39117 ,  0.39117 ,  0.12708 ,  0.12708 ,\n",
       "           0.12771 ,  0.12771 ,  0.23802 ,  0.23802 ,  0.30795 ,  0.30795 ,\n",
       "           0.18453 ,  0.18453 ,  0.11813 ,  0.11813 ,  0.014342,  0.014342,\n",
       "           0.081532,  0.081532,  0.10419 ,  0.10419 ,  0.14018 ,  0.14018 ,\n",
       "           0.070059,  0.070059,  0.13455 ,  0.13455 ,  0.652   ,  0.652   ,\n",
       "           0.31912 ,  0.31912 ,  0.3282  ,  0.3282  ,  0.52059 ,  0.52059 ,\n",
       "           0.16183 ,  0.16183 ,  0.29156 ,  0.29156 ,  0.44222 ,  0.44222 ,\n",
       "           0.4727  ,  0.4727  ,  0.61856 ,  0.61856 ,  0.57925 ,  0.57925 ,\n",
       "           0.70341 ,  0.70341 ,  0.31806 ,  0.31806 ,  0.39637 ,  0.39637 ,\n",
       "           0.12368 ,  0.12368 ,  0.10866 ,  0.10866 ,  0.21268 ,  0.21268 ,\n",
       "           0.37674 ,  0.37674 ,  0.19332 ,  0.19332 ,  0.17454 ,  0.17454 ,\n",
       "           0.025401,  0.025401,  0.10078 ,  0.10078 ,  0.1409  ,  0.1409  ,\n",
       "           0.18789 ,  0.18789 ,  0.083905,  0.083905,  0.15993 ,  0.15993 ,\n",
       "           0.31714 ,  0.31714 ,  0.47498 ,  0.47498 ,  0.46442 ,  0.46442 ,\n",
       "           0.108   ,  0.108   ,  0.17035 ,  0.17035 ,  0.40173 ,  0.40173 ,\n",
       "           0.45769 ,  0.45769 ,  0.49706 ,  0.49706 ,  0.49691 ,  0.49691 ,\n",
       "           0.45997 ,  0.45997 ,  0.32508 ,  0.32508 ,  0.57744 ,  0.57744 ,\n",
       "           0.08625 ,  0.08625 ,  0.11305 ,  0.11305 ,  0.20143 ,  0.20143 ,\n",
       "           0.26651 ,  0.26651 ,  0.12242 ,  0.12242 ,  0.10453 ,  0.10453 ,\n",
       "           0.016032,  0.016032,  0.07877 ,  0.07877 ,  0.1242  ,  0.1242  ,\n",
       "           0.1604  ,  0.1604  ,  0.065636,  0.065636,  0.14085 ,  0.14085 ,\n",
       "           0.61055 ,  0.61055 ,  0.48032 ,  0.48032 ,  0.34818 ,  0.34818 ,\n",
       "           0.13401 ,  0.13401 ,  0.31784 ,  0.31784 ,  0.41468 ,  0.41468 ,\n",
       "           0.31134 ,  0.31134 ,  0.3635  ,  0.3635  ,  0.26366 ,  0.26366 ,\n",
       "           0.72974 ,  0.72974 ,  0.50262 ,  0.50262 ,  0.1968  ,  0.1968  ,\n",
       "           0.063801,  0.063801,  0.31384 ,  0.31384 ,  0.27739 ,  0.27739 ,\n",
       "           0.32239 ,  0.32239 , -0.05647 , -0.05647 ,  0.010482,  0.010482,\n",
       "           0.16089 ,  0.16089 ,  0.18213 ,  0.18213 ,  0.15926 ,  0.15926 ,\n",
       "           0.13811 ,  0.13811 ,  0.13602 ,  0.13602 ,  0.51535 ,  0.51535 ,\n",
       "           0.19514 ,  0.19514 ,  0.026434,  0.026434,  0.31163 ,  0.31163 ,\n",
       "           0.33739 ,  0.33739 ,  0.36784 ,  0.36784 ,  0.48796 ,  0.48796 ,\n",
       "           0.17221 ,  0.17221 ,  0.59915 ,  0.59915 ,  0.9465  ,  0.9465  ,\n",
       "           0.1477  ,  0.1477  ,  0.16485 ,  0.16485 ,  0.36548 ,  0.36548 ,\n",
       "           0.28938 ,  0.28938 ,  0.34738 ,  0.34738 , -0.069929, -0.069929,\n",
       "           0.012004,  0.012004,  0.13788 ,  0.13788 ,  0.19761 ,  0.19761 ,\n",
       "           0.19086 ,  0.19086 ,  0.126   ,  0.126   ,  0.17964 ,  0.17964 ,\n",
       "           0.11124 ,  0.11124 ,  0.17191 ,  0.17191 ,  0.34755 ,  0.34755 ,\n",
       "           0.35721 ,  0.35721 ,  0.43137 ,  0.43137 ,  0.53786 ,  0.53786 ,\n",
       "           0.3672  ,  0.3672  ,  0.43597 ,  0.43597 ,  0.51059 ,  0.51059 ,\n",
       "           0.086054,  0.086054,  0.086132,  0.086132,  0.28085 ,  0.28085 ,\n",
       "           0.31346 ,  0.31346 ,  0.24337 ,  0.24337 ,  0.022592,  0.022592,\n",
       "           0.018079,  0.018079,  0.13688 ,  0.13688 ,  0.18395 ,  0.18395 ,\n",
       "           0.16099 ,  0.16099 ,  0.11672 ,  0.11672 ,  0.14472 ,  0.14472 ,\n",
       "           0.2988  ,  0.2988  ,  0.14685 ,  0.14685 ,  0.20041 ,  0.20041 ,\n",
       "           0.14503 ,  0.14503 ,  0.1079  ,  0.1079  ,  0.1484  ,  0.1484  ,\n",
       "           0.21373 ,  0.21373 ,  0.092073,  0.092073,  0.3208  ,  0.3208  ,\n",
       "           0.15637 ,  0.15637 ,  0.19179 ,  0.19179 ,  0.2362  ,  0.2362  ,\n",
       "           0.23041 ,  0.23041 ,  0.22145 ,  0.22145 ,  0.018007,  0.018007,\n",
       "           0.13568 ,  0.13568 ,  0.097995,  0.097995,  0.13314 ,  0.13314 ,\n",
       "           0.11861 ,  0.11861 ,  0.11165 ,  0.11165 ,  0.25668 ,  0.25668 ,\n",
       "           0.27928 ,  0.27928 ,  0.25164 ,  0.25164 ,  0.17766 ,  0.17766 ,\n",
       "           0.30654 ,  0.30654 ,  0.04025 ,  0.04025 , -0.019243, -0.019243,\n",
       "           0.10886 ,  0.10886 ,  0.079629,  0.079629,  0.037198,  0.037198,\n",
       "           0.55236 ,  0.55236 ,  0.31815 ,  0.31815 ,  0.47026 ,  0.47026 ,\n",
       "           0.023071,  0.023071,  0.096659,  0.096659,  0.07033 ,  0.07033 ,\n",
       "           0.088388,  0.088388,  0.06929 ,  0.06929 ,  0.063706,  0.063706,\n",
       "           0.75899 ,  0.75899 ,  0.4182  ,  0.4182  ,  0.41105 ,  0.41105 ,\n",
       "           0.40217 ,  0.40217 ,  0.38907 ,  0.38907 ,  0.38795 ,  0.38795 ,\n",
       "           0.13881 ,  0.13881 ,  0.1268  ,  0.1268  ,  0.22659 ,  0.22659 ,\n",
       "           0.34833 ,  0.34833 ,  0.20942 ,  0.20942 ,  0.13749 ,  0.13749 ,\n",
       "           0.015259,  0.015259,  0.08026 ,  0.08026 ,  0.1011  ,  0.1011  ,\n",
       "           0.13007 ,  0.13007 ,  0.068697,  0.068697,  0.13054 ,  0.13054 ,\n",
       "           0.41689 ,  0.41689 ,  0.41408 ,  0.41408 ,  0.4566  ,  0.4566  ,\n",
       "           0.52749 ,  0.52749 ,  0.47462 ,  0.47462 ,  0.15207 ,  0.15207 ,\n",
       "           0.10558 ,  0.10558 ,  0.20897 ,  0.20897 ,  0.38513 ,  0.38513 ,\n",
       "           0.22266 ,  0.22266 ,  0.13338 ,  0.13338 ,  0.023153,  0.023153,\n",
       "           0.10571 ,  0.10571 ,  0.12122 ,  0.12122 ,  0.14254 ,  0.14254 ,\n",
       "           0.095759,  0.095759,  0.14333 ,  0.14333 ,  0.68306 ,  0.68306 ,\n",
       "           0.5435  ,  0.5435  ,  0.33372 ,  0.33372 ,  0.48485 ,  0.48485 ,\n",
       "           0.13303 ,  0.13303 ,  0.13193 ,  0.13193 ,  0.24759 ,  0.24759 ,\n",
       "           0.35667 ,  0.35667 ,  0.20544 ,  0.20544 ,  0.16099 ,  0.16099 ,\n",
       "           0.023318,  0.023318,  0.097558,  0.097558,  0.14163 ,  0.14163 ,\n",
       "           0.1726  ,  0.1726  ,  0.084644,  0.084644,  0.1691  ,  0.1691  ,\n",
       "           0.48128 ,  0.48128 ,  0.43514 ,  0.43514 ,  0.67583 ,  0.67583 ,\n",
       "           0.10531 ,  0.10531 ,  0.1171  ,  0.1171  ,  0.25078 ,  0.25078 ,\n",
       "           0.34208 ,  0.34208 ,  0.20078 ,  0.20078 ,  0.068998,  0.068998,\n",
       "           0.018846,  0.018846,  0.09443 ,  0.09443 ,  0.16398 ,  0.16398 ,\n",
       "           0.15269 ,  0.15269 ,  0.082883,  0.082883,  0.14856 ,  0.14856 ,\n",
       "           0.25675 ,  0.25675 ,  0.25601 ,  0.25601 ,  0.10158 ,  0.10158 ,\n",
       "           0.055389,  0.055389,  0.12263 ,  0.12263 ,  0.35918 ,  0.35918 ,\n",
       "           0.16189 ,  0.16189 ,  0.1744  ,  0.1744  ,  0.019853,  0.019853,\n",
       "           0.069583,  0.069583,  0.076881,  0.076881,  0.11079 ,  0.11079 ,\n",
       "           0.053991,  0.053991,  0.10556 ,  0.10556 ,  0.70754 ,  0.70754 ,\n",
       "           0.28536 ,  0.28536 ,  0.12229 ,  0.12229 ,  0.40747 ,  0.40747 ,\n",
       "           0.34977 ,  0.34977 ,  0.45087 ,  0.45087 , -0.029604, -0.029604,\n",
       "           0.012842,  0.012842,  0.14106 ,  0.14106 ,  0.17495 ,  0.17495 ,\n",
       "           0.15435 ,  0.15435 ,  0.14986 ,  0.14986 ,  0.16964 ,  0.16964 ,\n",
       "           0.11954 ,  0.11954 ,  0.15531 ,  0.15531 ,  0.34743 ,  0.34743 ,\n",
       "           0.25542 ,  0.25542 ,  0.2543  ,  0.2543  , -0.076543, -0.076543,\n",
       "           0.01228 ,  0.01228 ,  0.10778 ,  0.10778 ,  0.1859  ,  0.1859  ,\n",
       "           0.1875  ,  0.1875  ,  0.10722 ,  0.10722 ,  0.18352 ,  0.18352 ,\n",
       "           0.32773 ,  0.32773 ,  0.43278 ,  0.43278 ,  0.28433 ,  0.28433 ,\n",
       "           0.34246 ,  0.34246 ,  0.31868 ,  0.31868 ,  0.021865,  0.021865,\n",
       "           0.15572 ,  0.15572 ,  0.10647 ,  0.10647 ,  0.14874 ,  0.14874 ,\n",
       "           0.19075 ,  0.19075 ,  0.17451 ,  0.17451 ,  0.33589 ,  0.33589 ,\n",
       "           0.15461 ,  0.15461 ,  0.12699 ,  0.12699 ,  0.38227 ,  0.38227 ,\n",
       "           0.031151,  0.031151,  0.12646 ,  0.12646 ,  0.10322 ,  0.10322 ,\n",
       "           0.16502 ,  0.16502 ,  0.13441 ,  0.13441 ,  0.17819 ,  0.17819 ,\n",
       "           0.23785 ,  0.23785 ,  0.30948 ,  0.30948 ,  0.17583 ,  0.17583 ,\n",
       "           0.038642,  0.038642,  0.19682 ,  0.19682 ,  0.17838 ,  0.17838 ,\n",
       "           0.21146 ,  0.21146 ,  0.21378 ,  0.21378 ,  0.22637 ,  0.22637 ,\n",
       "           0.60657 ,  0.60657 ,  0.33179 ,  0.33179 ,  0.019819,  0.019819,\n",
       "           0.1191  ,  0.1191  ,  0.10777 ,  0.10777 ,  0.11949 ,  0.11949 ,\n",
       "           0.11785 ,  0.11785 ,  0.12269 ,  0.12269 ,  0.19324 ,  0.19324 ,\n",
       "           0.010799,  0.010799,  0.14439 ,  0.14439 ,  0.12377 ,  0.12377 ,\n",
       "           0.12771 ,  0.12771 ,  0.15287 ,  0.15287 ,  0.13668 ,  0.13668 ,\n",
       "           0.040826,  0.040826,  0.11812 ,  0.11812 ,  0.062458,  0.062458,\n",
       "           0.14295 ,  0.14295 ,  0.1203  ,  0.1203  ,  0.14825 ,  0.14825 ,\n",
       "           0.07886 ,  0.07886 ,  0.060065,  0.060065,  0.04385 ,  0.04385 ,\n",
       "           0.06811 ,  0.06811 ,  0.045071,  0.045071,  0.28453 ,  0.28453 ,\n",
       "           0.25202 ,  0.25202 ,  0.40311 ,  0.40311 ,  0.23479 ,  0.23479 ,\n",
       "           0.22615 ,  0.22615 ,  0.27356 ,  0.27356 ,  0.21588 ,  0.21588 ,\n",
       "           0.23607 ,  0.23607 ,  0.35804 ,  0.35804 ,  0.24593 ,  0.24593 ]),\n",
       "   'y_pred': array([0.26930156, 0.26930156, 0.32496047, 0.32496047, 0.34859782,\n",
       "          0.34859782, 0.25543898, 0.25543898, 0.29562512, 0.29562512,\n",
       "          0.29749913, 0.29749913, 0.22899395, 0.22899395, 0.20567463,\n",
       "          0.20567463, 0.26537238, 0.26537238, 0.26117402, 0.26117402,\n",
       "          0.25517716, 0.25517716, 0.24092811, 0.24092811, 0.31416648,\n",
       "          0.31416648, 0.24822953, 0.24822953, 0.27862457, 0.27862457,\n",
       "          0.17307109, 0.17307109, 0.28771864, 0.28771864, 0.24072996,\n",
       "          0.24072996, 0.24281942, 0.24281942, 0.20000102, 0.20000102,\n",
       "          0.30602495, 0.30602495, 0.2075831 , 0.2075831 , 0.23118812,\n",
       "          0.23118812, 0.22669859, 0.22669859, 0.24372972, 0.24372972,\n",
       "          0.23050955, 0.23050955, 0.24116869, 0.24116869, 0.319636  ,\n",
       "          0.319636  , 0.34327336, 0.34327336, 0.25011452, 0.25011452,\n",
       "          0.29030066, 0.29030066, 0.29217466, 0.29217466, 0.22366949,\n",
       "          0.22366949, 0.20035017, 0.20035017, 0.26004792, 0.26004792,\n",
       "          0.25584955, 0.25584955, 0.24985269, 0.24985269, 0.23560365,\n",
       "          0.23560365, 0.30884202, 0.30884202, 0.24290507, 0.24290507,\n",
       "          0.27330011, 0.27330011, 0.16774662, 0.16774662, 0.28239418,\n",
       "          0.28239418, 0.2354055 , 0.2354055 , 0.23749495, 0.23749495,\n",
       "          0.19467655, 0.19467655, 0.30070049, 0.30070049, 0.20225864,\n",
       "          0.20225864, 0.22586366, 0.22586366, 0.22137413, 0.22137413,\n",
       "          0.23840526, 0.23840526, 0.22518509, 0.22518509, 0.23584423,\n",
       "          0.23584423, 0.39893227, 0.39893227, 0.30577343, 0.30577343,\n",
       "          0.34595957, 0.34595957, 0.34783357, 0.34783357, 0.2793284 ,\n",
       "          0.2793284 , 0.25600908, 0.25600908, 0.31570682, 0.31570682,\n",
       "          0.31150846, 0.31150846, 0.3055116 , 0.3055116 , 0.29126256,\n",
       "          0.29126256, 0.36450093, 0.36450093, 0.29856398, 0.29856398,\n",
       "          0.32895902, 0.32895902, 0.22340553, 0.22340553, 0.33805309,\n",
       "          0.33805309, 0.29106441, 0.29106441, 0.29315386, 0.29315386,\n",
       "          0.25033546, 0.25033546, 0.3563594 , 0.3563594 , 0.25791754,\n",
       "          0.25791754, 0.28152257, 0.28152257, 0.27703304, 0.27703304,\n",
       "          0.29406417, 0.29406417, 0.280844  , 0.280844  , 0.29150314,\n",
       "          0.29150314, 0.32941079, 0.32941079, 0.36959693, 0.36959693,\n",
       "          0.37147093, 0.37147093, 0.30296575, 0.30296575, 0.27964643,\n",
       "          0.27964643, 0.33934418, 0.33934418, 0.33514582, 0.33514582,\n",
       "          0.32914896, 0.32914896, 0.31489992, 0.31489992, 0.38813828,\n",
       "          0.38813828, 0.32220134, 0.32220134, 0.35259637, 0.35259637,\n",
       "          0.24704289, 0.24704289, 0.36169045, 0.36169045, 0.31470177,\n",
       "          0.31470177, 0.31679122, 0.31679122, 0.27397282, 0.27397282,\n",
       "          0.37999675, 0.37999675, 0.2815549 , 0.2815549 , 0.30515992,\n",
       "          0.30515992, 0.30067039, 0.30067039, 0.31770152, 0.31770152,\n",
       "          0.30448135, 0.30448135, 0.31514049, 0.31514049, 0.27643809,\n",
       "          0.27643809, 0.27831209, 0.27831209, 0.20980691, 0.20980691,\n",
       "          0.18648759, 0.18648759, 0.24618534, 0.24618534, 0.24198698,\n",
       "          0.24198698, 0.23599012, 0.23599012, 0.22174108, 0.22174108,\n",
       "          0.29497944, 0.29497944, 0.2290425 , 0.2290425 , 0.25943753,\n",
       "          0.25943753, 0.15388405, 0.15388405, 0.26853161, 0.26853161,\n",
       "          0.22154293, 0.22154293, 0.22363238, 0.22363238, 0.18081398,\n",
       "          0.18081398, 0.28683791, 0.28683791, 0.18839606, 0.18839606,\n",
       "          0.21200108, 0.21200108, 0.20751155, 0.20751155, 0.22454268,\n",
       "          0.22454268, 0.21132251, 0.21132251, 0.22198165, 0.22198165,\n",
       "          0.31849823, 0.31849823, 0.24999305, 0.24999305, 0.22667373,\n",
       "          0.22667373, 0.28637148, 0.28637148, 0.28217312, 0.28217312,\n",
       "          0.27617626, 0.27617626, 0.26192722, 0.26192722, 0.33516558,\n",
       "          0.33516558, 0.26922864, 0.26922864, 0.29962367, 0.29962367,\n",
       "          0.19407019, 0.19407019, 0.30871775, 0.30871775, 0.26172907,\n",
       "          0.26172907, 0.26381852, 0.26381852, 0.22100012, 0.22100012,\n",
       "          0.32702405, 0.32702405, 0.2285822 , 0.2285822 , 0.25218722,\n",
       "          0.25218722, 0.24769769, 0.24769769, 0.26472882, 0.26472882,\n",
       "          0.25150865, 0.25150865, 0.26216779, 0.26216779, 0.25186706,\n",
       "          0.25186706, 0.22854774, 0.22854774, 0.28824548, 0.28824548,\n",
       "          0.28404712, 0.28404712, 0.27805026, 0.27805026, 0.26380122,\n",
       "          0.26380122, 0.33703959, 0.33703959, 0.27110264, 0.27110264,\n",
       "          0.30149768, 0.30149768, 0.19594419, 0.19594419, 0.31059175,\n",
       "          0.31059175, 0.26360307, 0.26360307, 0.26569252, 0.26569252,\n",
       "          0.22287412, 0.22287412, 0.32889806, 0.32889806, 0.2304562 ,\n",
       "          0.2304562 , 0.25406123, 0.25406123, 0.2495717 , 0.2495717 ,\n",
       "          0.26660283, 0.26660283, 0.25338266, 0.25338266, 0.2640418 ,\n",
       "          0.2640418 , 0.16004256, 0.16004256, 0.21974031, 0.21974031,\n",
       "          0.21554195, 0.21554195, 0.20954509, 0.20954509, 0.19529604,\n",
       "          0.19529604, 0.26853441, 0.26853441, 0.20259746, 0.20259746,\n",
       "          0.2329925 , 0.2329925 , 0.12743902, 0.12743902, 0.24208657,\n",
       "          0.24208657, 0.19509789, 0.19509789, 0.19718735, 0.19718735,\n",
       "          0.15436895, 0.15436895, 0.26039288, 0.26039288, 0.16195103,\n",
       "          0.16195103, 0.18555605, 0.18555605, 0.18106652, 0.18106652,\n",
       "          0.19809765, 0.19809765, 0.18487748, 0.18487748, 0.19553662,\n",
       "          0.19553662, 0.19642099, 0.19642099, 0.19222263, 0.19222263,\n",
       "          0.18622576, 0.18622576, 0.17197672, 0.17197672, 0.24521509,\n",
       "          0.24521509, 0.17927814, 0.17927814, 0.20967318, 0.20967318,\n",
       "          0.10411969, 0.10411969, 0.21876725, 0.21876725, 0.17177857,\n",
       "          0.17177857, 0.17386802, 0.17386802, 0.13104963, 0.13104963,\n",
       "          0.23707356, 0.23707356, 0.13863171, 0.13863171, 0.16223673,\n",
       "          0.16223673, 0.1577472 , 0.1577472 , 0.17477833, 0.17477833,\n",
       "          0.16155816, 0.16155816, 0.1722173 , 0.1722173 , 0.25192037,\n",
       "          0.25192037, 0.24592351, 0.24592351, 0.23167447, 0.23167447,\n",
       "          0.30491284, 0.30491284, 0.23897589, 0.23897589, 0.26937093,\n",
       "          0.26937093, 0.16381744, 0.16381744, 0.278465  , 0.278465  ,\n",
       "          0.23147632, 0.23147632, 0.23356577, 0.23356577, 0.19074737,\n",
       "          0.19074737, 0.29677131, 0.29677131, 0.19832946, 0.19832946,\n",
       "          0.22193448, 0.22193448, 0.21744495, 0.21744495, 0.23447608,\n",
       "          0.23447608, 0.22125591, 0.22125591, 0.23191505, 0.23191505,\n",
       "          0.24172515, 0.24172515, 0.22747611, 0.22747611, 0.30071448,\n",
       "          0.30071448, 0.23477753, 0.23477753, 0.26517257, 0.26517257,\n",
       "          0.15961908, 0.15961908, 0.27426664, 0.27426664, 0.22727796,\n",
       "          0.22727796, 0.22936741, 0.22936741, 0.18654901, 0.18654901,\n",
       "          0.29257295, 0.29257295, 0.1941311 , 0.1941311 , 0.21773612,\n",
       "          0.21773612, 0.21324659, 0.21324659, 0.23027772, 0.23027772,\n",
       "          0.21705755, 0.21705755, 0.22771669, 0.22771669, 0.22147925,\n",
       "          0.22147925, 0.29471762, 0.29471762, 0.22878067, 0.22878067,\n",
       "          0.2591757 , 0.2591757 , 0.15362222, 0.15362222, 0.26826978,\n",
       "          0.26826978, 0.2212811 , 0.2212811 , 0.22337055, 0.22337055,\n",
       "          0.18055215, 0.18055215, 0.28657609, 0.28657609, 0.18813423,\n",
       "          0.18813423, 0.21173926, 0.21173926, 0.20724973, 0.20724973,\n",
       "          0.22428085, 0.22428085, 0.21106068, 0.21106068, 0.22171982,\n",
       "          0.22171982, 0.28046857, 0.28046857, 0.21453163, 0.21453163,\n",
       "          0.24492666, 0.24492666, 0.13937318, 0.13937318, 0.25402074,\n",
       "          0.25402074, 0.20703206, 0.20703206, 0.20912151, 0.20912151,\n",
       "          0.16630311, 0.16630311, 0.27232704, 0.27232704, 0.17388519,\n",
       "          0.17388519, 0.19749021, 0.19749021, 0.19300068, 0.19300068,\n",
       "          0.21003181, 0.21003181, 0.19681164, 0.19681164, 0.20747078,\n",
       "          0.20747078, 0.28776999, 0.28776999, 0.31816503, 0.31816503,\n",
       "          0.21261155, 0.21261155, 0.3272591 , 0.3272591 , 0.28027042,\n",
       "          0.28027042, 0.28235988, 0.28235988, 0.23954148, 0.23954148,\n",
       "          0.34556541, 0.34556541, 0.24712356, 0.24712356, 0.27072858,\n",
       "          0.27072858, 0.26623905, 0.26623905, 0.28327018, 0.28327018,\n",
       "          0.27005001, 0.27005001, 0.28070915, 0.28070915, 0.25222808,\n",
       "          0.25222808, 0.1466746 , 0.1466746 , 0.26132215, 0.26132215,\n",
       "          0.21433348, 0.21433348, 0.21642293, 0.21642293, 0.17360453,\n",
       "          0.17360453, 0.27962846, 0.27962846, 0.18118661, 0.18118661,\n",
       "          0.20479163, 0.20479163, 0.2003021 , 0.2003021 , 0.21733323,\n",
       "          0.21733323, 0.20411306, 0.20411306, 0.2147722 , 0.2147722 ,\n",
       "          0.17706963, 0.17706963, 0.29171719, 0.29171719, 0.24472851,\n",
       "          0.24472851, 0.24681797, 0.24681797, 0.20399957, 0.20399957,\n",
       "          0.3100235 , 0.3100235 , 0.21158165, 0.21158165, 0.23518667,\n",
       "          0.23518667, 0.23069714, 0.23069714, 0.24772827, 0.24772827,\n",
       "          0.2345081 , 0.2345081 , 0.24516724, 0.24516724, 0.18616371,\n",
       "          0.18616371, 0.13917503, 0.13917503, 0.14126448, 0.14126448,\n",
       "          0.09844608, 0.09844608, 0.20447002, 0.20447002, 0.10602816,\n",
       "          0.10602816, 0.12963319, 0.12963319, 0.12514366, 0.12514366,\n",
       "          0.14217478, 0.14217478, 0.12895461, 0.12895461, 0.13961375,\n",
       "          0.13961375, 0.25382259, 0.25382259, 0.25591204, 0.25591204,\n",
       "          0.21309364, 0.21309364, 0.31911757, 0.31911757, 0.22067572,\n",
       "          0.22067572, 0.24428074, 0.24428074, 0.23979121, 0.23979121,\n",
       "          0.25682234, 0.25682234, 0.24360217, 0.24360217, 0.25426131,\n",
       "          0.25426131, 0.20892336, 0.20892336, 0.16610496, 0.16610496,\n",
       "          0.27212889, 0.27212889, 0.17368704, 0.17368704, 0.19729206,\n",
       "          0.19729206, 0.19280253, 0.19280253, 0.20983366, 0.20983366,\n",
       "          0.19661349, 0.19661349, 0.20727263, 0.20727263, 0.16819441,\n",
       "          0.16819441, 0.27421835, 0.27421835, 0.17577649, 0.17577649,\n",
       "          0.19938152, 0.19938152, 0.19489199, 0.19489199, 0.21192312,\n",
       "          0.21192312, 0.19870295, 0.19870295, 0.20936208, 0.20936208,\n",
       "          0.23139995, 0.23139995, 0.13295809, 0.13295809, 0.15656312,\n",
       "          0.15656312, 0.15207359, 0.15207359, 0.16910472, 0.16910472,\n",
       "          0.15588455, 0.15588455, 0.16654369, 0.16654369, 0.23898203,\n",
       "          0.23898203, 0.26258705, 0.26258705, 0.25809752, 0.25809752,\n",
       "          0.27512865, 0.27512865, 0.26190848, 0.26190848, 0.27256762,\n",
       "          0.27256762, 0.1641452 , 0.1641452 , 0.15965567, 0.15965567,\n",
       "          0.1766868 , 0.1766868 , 0.16346663, 0.16346663, 0.17412577,\n",
       "          0.17412577, 0.18326069, 0.18326069, 0.20029182, 0.20029182,\n",
       "          0.18707165, 0.18707165, 0.19773079, 0.19773079, 0.19580229,\n",
       "          0.19580229, 0.18258212, 0.18258212, 0.19324126, 0.19324126,\n",
       "          0.19961325, 0.19961325, 0.21027239, 0.21027239, 0.19705222,\n",
       "          0.19705222]),\n",
       "   'feature_importances': array([-3.40664206e-05, -8.43965454e-04,  1.20654100e-03, -3.40664206e-05,\n",
       "          -8.43965454e-04,  1.20654100e-03]),\n",
       "   'model_json': None}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='ridge',\n",
    "              feature_type=[{'euclidean': None}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=False,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('grid', 'mse'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9968256-2e65-4681-84f7-8c22d52a187a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name euclidean\n",
      "processing_type None\n",
      "X shape (114, 3)\n",
      "\n",
      " Test fold num: 1\n",
      "(12070, 6) (12070,) (812, 6) (812,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "2\n",
      "3\n",
      "4\n",
      "Create sweep with ID: gx3d4mmj\n",
      "Sweep URL: https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ro1qgbl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [512, 256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_094809-9ro1qgbl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/9ro1qgbl' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold0_run123</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/9ro1qgbl' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/9ro1qgbl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0294, Val Loss: 0.0285\n",
      "Epoch 20/100, Train Loss: 0.0289, Val Loss: 0.0226\n",
      "Epoch 30/100, Train Loss: 0.0276, Val Loss: 0.0232\n",
      "Epoch 40/100, Train Loss: 0.0271, Val Loss: 0.0205\n",
      "Epoch 50/100, Train Loss: 0.0266, Val Loss: 0.0221\n",
      "Epoch 60/100, Train Loss: 0.0269, Val Loss: 0.0197\n",
      "Epoch 70/100, Train Loss: 0.0266, Val Loss: 0.0196\n",
      "Epoch 80/100, Train Loss: 0.0269, Val Loss: 0.0240\n",
      "Epoch 90/100, Train Loss: 0.0266, Val Loss: 0.0227\n",
      "Epoch 100/100, Train Loss: 0.0267, Val Loss: 0.0204\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0307, Val Loss: 0.0290\n",
      "Epoch 20/100, Train Loss: 0.0305, Val Loss: 0.0275\n",
      "Epoch 30/100, Train Loss: 0.0291, Val Loss: 0.0261\n",
      "Epoch 40/100, Train Loss: 0.0293, Val Loss: 0.0274\n",
      "Epoch 50/100, Train Loss: 0.0284, Val Loss: 0.0272\n",
      "Epoch 60/100, Train Loss: 0.0287, Val Loss: 0.0262\n",
      "Epoch 70/100, Train Loss: 0.0286, Val Loss: 0.0258\n",
      "Epoch 80/100, Train Loss: 0.0293, Val Loss: 0.0254\n",
      "Epoch 90/100, Train Loss: 0.0288, Val Loss: 0.0267\n",
      "Epoch 100/100, Train Loss: 0.0285, Val Loss: 0.0255\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0301, Val Loss: 0.0230\n",
      "Epoch 20/100, Train Loss: 0.0296, Val Loss: 0.0211\n",
      "Epoch 30/100, Train Loss: 0.0288, Val Loss: 0.0200\n",
      "Epoch 40/100, Train Loss: 0.0281, Val Loss: 0.0200\n",
      "Epoch 50/100, Train Loss: 0.0277, Val Loss: 0.0203\n",
      "Epoch 60/100, Train Loss: 0.0278, Val Loss: 0.0171\n",
      "Epoch 70/100, Train Loss: 0.0270, Val Loss: 0.0172\n",
      "Epoch 80/100, Train Loss: 0.0278, Val Loss: 0.0190\n",
      "Epoch 90/100, Train Loss: 0.0267, Val Loss: 0.0175\n",
      "Epoch 100/100, Train Loss: 0.0271, Val Loss: 0.0159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>fold0_train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▃▄▄▅▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇▇▇█▇█████▇█▇▇</td></tr><tr><td>fold0_val_loss</td><td>█▆▅▅▄▄▃▃▃▂▂▃▂▂▂▂▃▂▂▂▁▃▂▂▃▂▂▂▂▂▃▂▂▂▂▁▃▁▂▂</td></tr><tr><td>fold0_val_pearson</td><td>▁▃▄▅▇▆▇▇▇████▇██████▇████▇███▇▇███▇██▇██</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>fold1_train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███▇███▇▇████▇▇▇▇█▇</td></tr><tr><td>fold1_val_loss</td><td>▇█▇▆▆▄▄▄▄▃▃▂▅▃▅▂▂▄▃▂▅▇▁▆▃▁▂▂▂▃▁▁▁▄▃▁▂▁▃▁</td></tr><tr><td>fold1_val_pearson</td><td>▄▁▃▃▃▆▆▅▇▆▇▇▇▆▇▄█▇▇▃▇▆█▇▅▆▆▆▅█▇█▇▆██▆█▆▇</td></tr><tr><td>fold2_epoch</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>fold2_train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▃▃▃▃▄▄▅▅▅▅▅▆▅▆▆▆▆▇▆▇▆▇▇▇█▇▇▇▇██▇▇▇▇███</td></tr><tr><td>fold2_val_loss</td><td>█▅▅▅▄▄▄▄▄▃▃▂▃▂▃▃▂▂▂▄▁▅▂▃▂▂▂▂▂▃▁▁▂▂▃▁▂▂▃▁</td></tr><tr><td>fold2_val_pearson</td><td>▂▁▃▂▃▅▄▆▅▅▆▆▅▅▅▅▆▅▇▇▇▅▇▆▇▆▅▆▆▇▇▆▇▆▇▅▆▆▅█</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.02665</td></tr><tr><td>fold0_train_pearson</td><td>0.52034</td></tr><tr><td>fold0_val_loss</td><td>0.02036</td></tr><tr><td>fold0_val_pearson</td><td>0.68359</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.02848</td></tr><tr><td>fold1_train_pearson</td><td>0.46326</td></tr><tr><td>fold1_val_loss</td><td>0.02554</td></tr><tr><td>fold1_val_pearson</td><td>0.3856</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.02709</td></tr><tr><td>fold2_train_pearson</td><td>0.4916</td></tr><tr><td>fold2_val_loss</td><td>0.01587</td></tr><tr><td>fold2_val_pearson</td><td>0.6977</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.02741</td></tr><tr><td>mean_train_pearson</td><td>0.49173</td></tr><tr><td>mean_val_loss</td><td>0.02059</td></tr><tr><td>mean_val_pearson</td><td>0.58897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold0_run123</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/9ro1qgbl' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/9ro1qgbl</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_094809-9ro1qgbl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6t6ntc1a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_095403-6t6ntc1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/6t6ntc1a' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold0_run795</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/6t6ntc1a' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/6t6ntc1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0308, Val Loss: 0.0221\n",
      "Epoch 20/100, Train Loss: 0.0283, Val Loss: 0.0208\n",
      "Epoch 30/100, Train Loss: 0.0255, Val Loss: 0.0243\n",
      "Epoch 40/100, Train Loss: 0.0240, Val Loss: 0.0232\n",
      "Epoch 50/100, Train Loss: 0.0226, Val Loss: 0.0223\n",
      "Epoch 60/100, Train Loss: 0.0215, Val Loss: 0.0251\n",
      "Epoch 70/100, Train Loss: 0.0203, Val Loss: 0.0231\n",
      "Epoch 80/100, Train Loss: 0.0194, Val Loss: 0.0246\n",
      "Epoch 90/100, Train Loss: 0.0199, Val Loss: 0.0246\n",
      "Epoch 100/100, Train Loss: 0.0190, Val Loss: 0.0230\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0304, Val Loss: 0.0292\n",
      "Epoch 20/100, Train Loss: 0.0279, Val Loss: 0.0275\n",
      "Epoch 30/100, Train Loss: 0.0277, Val Loss: 0.0269\n",
      "Epoch 40/100, Train Loss: 0.0244, Val Loss: 0.0265\n",
      "Epoch 50/100, Train Loss: 0.0226, Val Loss: 0.0276\n",
      "Epoch 60/100, Train Loss: 0.0224, Val Loss: 0.0277\n",
      "Epoch 70/100, Train Loss: 0.0219, Val Loss: 0.0281\n",
      "Epoch 80/100, Train Loss: 0.0215, Val Loss: 0.0277\n",
      "Epoch 90/100, Train Loss: 0.0207, Val Loss: 0.0285\n",
      "Epoch 100/100, Train Loss: 0.0202, Val Loss: 0.0285\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0312, Val Loss: 0.0328\n",
      "Epoch 20/100, Train Loss: 0.0292, Val Loss: 0.0308\n",
      "Epoch 30/100, Train Loss: 0.0265, Val Loss: 0.0278\n",
      "Epoch 40/100, Train Loss: 0.0241, Val Loss: 0.0276\n",
      "Epoch 50/100, Train Loss: 0.0238, Val Loss: 0.0268\n",
      "Epoch 60/100, Train Loss: 0.0222, Val Loss: 0.0264\n",
      "Epoch 70/100, Train Loss: 0.0216, Val Loss: 0.0267\n",
      "Epoch 80/100, Train Loss: 0.0209, Val Loss: 0.0302\n",
      "Epoch 90/100, Train Loss: 0.0210, Val Loss: 0.0251\n",
      "Epoch 100/100, Train Loss: 0.0204, Val Loss: 0.0242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>fold0_train_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>fold0_val_loss</td><td>█▂▂▂▂▁▁▁▁▁▁▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>fold0_val_pearson</td><td>▁▁▄▁▄▆▆███▇▇▇▅▆▆▇██▇███▇▇▆▇▆▆▆▇▆▇▇▅▆▅▆▆▆</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>fold1_train_loss</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▃▃▃▄▄▄▅▅▅▅▆▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██▇▇█████████</td></tr><tr><td>fold1_val_loss</td><td>█▅▃▃▃▃▃▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>fold1_val_pearson</td><td>▁▄▄▅▅▅▆▇▇▆▆▇██▇▇▇▇▇▆▆▆▇▇▆▆▆▆▆▆▆█▇▇▇▇█▇█▇</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>fold2_train_loss</td><td>█▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▂▃▃▄▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>fold2_val_loss</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_val_pearson</td><td>▁▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.01897</td></tr><tr><td>fold0_train_pearson</td><td>0.81816</td></tr><tr><td>fold0_val_loss</td><td>0.02296</td></tr><tr><td>fold0_val_pearson</td><td>0.6453</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.02023</td></tr><tr><td>fold1_train_pearson</td><td>0.78201</td></tr><tr><td>fold1_val_loss</td><td>0.02854</td></tr><tr><td>fold1_val_pearson</td><td>0.35323</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.02044</td></tr><tr><td>fold2_train_pearson</td><td>0.78002</td></tr><tr><td>fold2_val_loss</td><td>0.02425</td></tr><tr><td>fold2_val_pearson</td><td>0.535</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.01988</td></tr><tr><td>mean_train_pearson</td><td>0.7934</td></tr><tr><td>mean_val_loss</td><td>0.02525</td></tr><tr><td>mean_val_pearson</td><td>0.51118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold0_run795</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/6t6ntc1a' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/6t6ntc1a</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_095403-6t6ntc1a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5n6ruea5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_100028-5n6ruea5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5n6ruea5' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold0_run825</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5n6ruea5' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5n6ruea5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0245, Val Loss: 0.0369\n",
      "Epoch 20/100, Train Loss: 0.0188, Val Loss: 0.0380\n",
      "Epoch 30/100, Train Loss: 0.0161, Val Loss: 0.0384\n",
      "Epoch 40/100, Train Loss: 0.0142, Val Loss: 0.0396\n",
      "Epoch 50/100, Train Loss: 0.0127, Val Loss: 0.0405\n",
      "Epoch 60/100, Train Loss: 0.0120, Val Loss: 0.0401\n",
      "Epoch 70/100, Train Loss: 0.0119, Val Loss: 0.0362\n",
      "Epoch 80/100, Train Loss: 0.0113, Val Loss: 0.0386\n",
      "Epoch 90/100, Train Loss: 0.0135, Val Loss: 0.0530\n",
      "Epoch 100/100, Train Loss: 0.0141, Val Loss: 0.0407\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0255, Val Loss: 0.0323\n",
      "Epoch 20/100, Train Loss: 0.0203, Val Loss: 0.0313\n",
      "Epoch 30/100, Train Loss: 0.0179, Val Loss: 0.0309\n",
      "Epoch 40/100, Train Loss: 0.0163, Val Loss: 0.0306\n",
      "Epoch 50/100, Train Loss: 0.0152, Val Loss: 0.0311\n",
      "Epoch 60/100, Train Loss: 0.0145, Val Loss: 0.0356\n",
      "Epoch 70/100, Train Loss: 0.0196, Val Loss: 0.0412\n",
      "Epoch 80/100, Train Loss: 0.0150, Val Loss: 0.0365\n",
      "Epoch 90/100, Train Loss: 0.0134, Val Loss: 0.0383\n",
      "Epoch 100/100, Train Loss: 0.0125, Val Loss: 0.0386\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0254, Val Loss: 0.0371\n",
      "Epoch 20/100, Train Loss: 0.0200, Val Loss: 0.0330\n",
      "Epoch 30/100, Train Loss: 0.0173, Val Loss: 0.0331\n",
      "Epoch 40/100, Train Loss: 0.0154, Val Loss: 0.0333\n",
      "Epoch 50/100, Train Loss: 0.0140, Val Loss: 0.0327\n",
      "Epoch 60/100, Train Loss: 0.0136, Val Loss: 0.0318\n",
      "Epoch 70/100, Train Loss: 0.0144, Val Loss: 0.0313\n",
      "Epoch 80/100, Train Loss: 0.0131, Val Loss: 0.0276\n",
      "Epoch 90/100, Train Loss: 0.0124, Val Loss: 0.0287\n",
      "Epoch 100/100, Train Loss: 0.0130, Val Loss: 0.0324\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>fold0_train_loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▂▃▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇███████▇█████████████▇</td></tr><tr><td>fold0_val_loss</td><td>▃▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▁▂▂▂▂▂▃▄▅▇█▃▁▁▂</td></tr><tr><td>fold0_val_pearson</td><td>▁▂▄▅▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▆▆▇█████▇▇▇▇▆▃▆</td></tr><tr><td>fold1_epoch</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>fold1_train_loss</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████▇▇▆▇▇▇▇█████████</td></tr><tr><td>fold1_val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▃▂▃▂▂▂▂▂▂▂▂</td></tr><tr><td>fold1_val_pearson</td><td>▁▇▇▇▇▇▇▇▇▇████████████▇▇▇▆▇▇▇▇▇█████████</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>fold2_train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▃▄▄▅▅▆▆▇▇▇▇▇▇█████▇▇▇▇▇███████████████</td></tr><tr><td>fold2_val_loss</td><td>█▅▅▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▄▂▂▁▁▁▁▁▁▁▂</td></tr><tr><td>fold2_val_pearson</td><td>▁▂▃▄▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▆▆▇▇████████▇▇▇</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅██████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.01412</td></tr><tr><td>fold0_train_pearson</td><td>0.80264</td></tr><tr><td>fold0_val_loss</td><td>0.0407</td></tr><tr><td>fold0_val_pearson</td><td>0.40988</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.01246</td></tr><tr><td>fold1_train_pearson</td><td>0.85056</td></tr><tr><td>fold1_val_loss</td><td>0.03855</td></tr><tr><td>fold1_val_pearson</td><td>0.34863</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.01298</td></tr><tr><td>fold2_train_pearson</td><td>0.82748</td></tr><tr><td>fold2_val_loss</td><td>0.03244</td></tr><tr><td>fold2_val_pearson</td><td>0.44706</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.01319</td></tr><tr><td>mean_train_pearson</td><td>0.82689</td></tr><tr><td>mean_val_loss</td><td>0.03723</td></tr><tr><td>mean_val_pearson</td><td>0.40185</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold0_run825</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5n6ruea5' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5n6ruea5</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_100028-5n6ruea5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: phf0qkbb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [512, 256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_100351-phf0qkbb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/phf0qkbb' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold0_run114</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/phf0qkbb' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/phf0qkbb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0301, Val Loss: 0.0229\n",
      "Epoch 20/100, Train Loss: 0.0270, Val Loss: 0.0235\n",
      "Epoch 30/100, Train Loss: 0.0248, Val Loss: 0.0229\n",
      "Epoch 40/100, Train Loss: 0.0233, Val Loss: 0.0217\n",
      "Epoch 50/100, Train Loss: 0.0217, Val Loss: 0.0221\n",
      "Epoch 60/100, Train Loss: 0.0207, Val Loss: 0.0208\n",
      "Epoch 70/100, Train Loss: 0.0198, Val Loss: 0.0209\n",
      "Epoch 80/100, Train Loss: 0.0191, Val Loss: 0.0210\n",
      "Epoch 90/100, Train Loss: 0.0185, Val Loss: 0.0210\n",
      "Epoch 100/100, Train Loss: 0.0182, Val Loss: 0.0200\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0332, Val Loss: 0.0266\n",
      "Epoch 20/100, Train Loss: 0.0286, Val Loss: 0.0257\n",
      "Epoch 30/100, Train Loss: 0.0264, Val Loss: 0.0254\n",
      "Epoch 40/100, Train Loss: 0.0249, Val Loss: 0.0246\n",
      "Epoch 50/100, Train Loss: 0.0236, Val Loss: 0.0245\n",
      "Epoch 60/100, Train Loss: 0.0224, Val Loss: 0.0240\n",
      "Epoch 70/100, Train Loss: 0.0217, Val Loss: 0.0240\n",
      "Epoch 80/100, Train Loss: 0.0209, Val Loss: 0.0240\n",
      "Epoch 90/100, Train Loss: 0.0203, Val Loss: 0.0234\n",
      "Epoch 100/100, Train Loss: 0.0200, Val Loss: 0.0234\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0317, Val Loss: 0.0242\n",
      "Epoch 20/100, Train Loss: 0.0282, Val Loss: 0.0250\n",
      "Epoch 30/100, Train Loss: 0.0267, Val Loss: 0.0216\n",
      "Epoch 40/100, Train Loss: 0.0250, Val Loss: 0.0228\n",
      "Epoch 50/100, Train Loss: 0.0235, Val Loss: 0.0236\n",
      "Epoch 60/100, Train Loss: 0.0228, Val Loss: 0.0203\n",
      "Epoch 70/100, Train Loss: 0.0218, Val Loss: 0.0215\n",
      "Epoch 80/100, Train Loss: 0.0214, Val Loss: 0.0204\n",
      "Epoch 90/100, Train Loss: 0.0206, Val Loss: 0.0184\n",
      "Epoch 100/100, Train Loss: 0.0201, Val Loss: 0.0203\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>fold0_train_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>fold0_val_loss</td><td>█▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_val_pearson</td><td>▁▂▄▃▃▅▄▆▅▆▅▆▆▆▆█▆▆▇▇▆▅▅▇▅▅▆▅▆▆▄▆▅▅▅▄▃▆▄▄</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>fold1_train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>fold1_val_loss</td><td>█▆▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>fold1_val_pearson</td><td>▁▂▂▃▃▂▄▃▂▄▄▄▄▅▅▅▆▅▅▆▇▆▇▆▆▇▇▇▆▆█▇▇▆█▆███▇</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>fold2_train_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>fold2_val_loss</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>fold2_val_pearson</td><td>▁▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█▇█▇████▇█▇</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅██████████████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.01818</td></tr><tr><td>fold0_train_pearson</td><td>0.81857</td></tr><tr><td>fold0_val_loss</td><td>0.02002</td></tr><tr><td>fold0_val_pearson</td><td>0.61591</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.01996</td></tr><tr><td>fold1_train_pearson</td><td>0.79004</td></tr><tr><td>fold1_val_loss</td><td>0.02341</td></tr><tr><td>fold1_val_pearson</td><td>0.42891</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.02009</td></tr><tr><td>fold2_train_pearson</td><td>0.78127</td></tr><tr><td>fold2_val_loss</td><td>0.02034</td></tr><tr><td>fold2_val_pearson</td><td>0.54038</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.01941</td></tr><tr><td>mean_train_pearson</td><td>0.79663</td></tr><tr><td>mean_val_loss</td><td>0.02126</td></tr><tr><td>mean_val_pearson</td><td>0.5284</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold0_run114</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/phf0qkbb' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/phf0qkbb</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_100351-phf0qkbb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dny2h08x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_101055-dny2h08x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold0_run42</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0286, Val Loss: 0.0240\n",
      "Epoch 20/100, Train Loss: 0.0262, Val Loss: 0.0236\n",
      "Epoch 30/100, Train Loss: 0.0248, Val Loss: 0.0230\n",
      "Epoch 40/100, Train Loss: 0.0236, Val Loss: 0.0221\n",
      "Epoch 50/100, Train Loss: 0.0233, Val Loss: 0.0218\n",
      "Epoch 60/100, Train Loss: 0.0230, Val Loss: 0.0207\n",
      "Epoch 70/100, Train Loss: 0.0226, Val Loss: 0.0200\n",
      "Epoch 80/100, Train Loss: 0.0226, Val Loss: 0.0236\n",
      "Epoch 90/100, Train Loss: 0.0222, Val Loss: 0.0211\n",
      "Epoch 100/100, Train Loss: 0.0222, Val Loss: 0.0191\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0295, Val Loss: 0.0279\n",
      "Epoch 20/100, Train Loss: 0.0275, Val Loss: 0.0266\n",
      "Epoch 30/100, Train Loss: 0.0259, Val Loss: 0.0249\n",
      "Epoch 40/100, Train Loss: 0.0249, Val Loss: 0.0241\n",
      "Epoch 50/100, Train Loss: 0.0244, Val Loss: 0.0259\n",
      "Epoch 60/100, Train Loss: 0.0242, Val Loss: 0.0248\n",
      "Epoch 70/100, Train Loss: 0.0240, Val Loss: 0.0280\n",
      "Epoch 80/100, Train Loss: 0.0239, Val Loss: 0.0250\n",
      "Epoch 90/100, Train Loss: 0.0238, Val Loss: 0.0252\n",
      "Epoch 100/100, Train Loss: 0.0234, Val Loss: 0.0262\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0291, Val Loss: 0.0237\n",
      "Epoch 20/100, Train Loss: 0.0272, Val Loss: 0.0229\n",
      "Epoch 30/100, Train Loss: 0.0256, Val Loss: 0.0218\n",
      "Epoch 40/100, Train Loss: 0.0250, Val Loss: 0.0278\n",
      "Epoch 50/100, Train Loss: 0.0245, Val Loss: 0.0316\n",
      "Epoch 60/100, Train Loss: 0.0241, Val Loss: 0.0338\n",
      "Epoch 70/100, Train Loss: 0.0237, Val Loss: 0.0304\n",
      "Epoch 80/100, Train Loss: 0.0239, Val Loss: 0.0324\n",
      "Epoch 90/100, Train Loss: 0.0239, Val Loss: 0.0349\n",
      "Epoch 100/100, Train Loss: 0.0239, Val Loss: 0.0328\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>fold0_train_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇█▇█▇████████████████████</td></tr><tr><td>fold0_val_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>fold0_val_pearson</td><td>▁▃▃▅▄▄▄▅▄▅▆▆▆▆▇▆▇█▇▇▆▇▇▆▇▇▇███▇▇█▆██▇▇▇█</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>fold1_train_loss</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▂▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>fold1_val_loss</td><td>█▄▄▃▃▃▃▃▃▃▃▂▂▃▂▁▁▁▂▁▂▁▁▂▃▃▂▂▄▃▃▃▂▃▄▅▄▂▃▂</td></tr><tr><td>fold1_val_pearson</td><td>▁▃▃▄▅▅▅▆▅▅▆▆▆▆▆▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>fold2_train_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>fold2_val_loss</td><td>▇▁▁▁▁▂▂▂▁▁▁▁▁▁▂▂▄▃▄▃▄▄▄▃▅▇▅▆▆▄▆▄▅▆▃▅█▇▆▃</td></tr><tr><td>fold2_val_pearson</td><td>▁▃▃▃▃▃▄▄▄▅▅▅▆▆▅▆▆▆▆▆▆▆▆▆▇▆▇█▇██▇███▇▇▇▇█</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅██████████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.02224</td></tr><tr><td>fold0_train_pearson</td><td>0.60679</td></tr><tr><td>fold0_val_loss</td><td>0.01906</td></tr><tr><td>fold0_val_pearson</td><td>0.68675</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.02336</td></tr><tr><td>fold1_train_pearson</td><td>0.56854</td></tr><tr><td>fold1_val_loss</td><td>0.02619</td></tr><tr><td>fold1_val_pearson</td><td>0.43282</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.02392</td></tr><tr><td>fold2_train_pearson</td><td>0.54836</td></tr><tr><td>fold2_val_loss</td><td>0.03278</td></tr><tr><td>fold2_val_pearson</td><td>0.64187</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.02317</td></tr><tr><td>mean_train_pearson</td><td>0.57456</td></tr><tr><td>mean_val_loss</td><td>0.02601</td></tr><tr><td>mean_val_pearson</td><td>0.58715</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold0_run42</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_101055-dny2h08x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by +summary_metrics.mean_val_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.0222, Val Loss: 0.0199\n",
      "Epoch 20/100, Train Loss: 0.0210, Val Loss: 0.0179\n",
      "Epoch 30/100, Train Loss: 0.0208, Val Loss: 0.0174\n",
      "Epoch 40/100, Train Loss: 0.0209, Val Loss: 0.0197\n",
      "Epoch 50/100, Train Loss: 0.0208, Val Loss: 0.0180\n",
      "Epoch 60/100, Train Loss: 0.0204, Val Loss: 0.0177\n",
      "Epoch 70/100, Train Loss: 0.0207, Val Loss: 0.0173\n",
      "Epoch 80/100, Train Loss: 0.0205, Val Loss: 0.0191\n",
      "Epoch 90/100, Train Loss: 0.0209, Val Loss: 0.0182\n",
      "Epoch 100/100, Train Loss: 0.0207, Val Loss: 0.0178\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_102147-dny2h08x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x' target=\"_blank\">dynamic_nn_euclidean_predFC_random_fold0_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx3d4mmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.02059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_predFC_random_fold0_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/dny2h08x</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_102147-dny2h08x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "\n",
      "Train Metrics: {'mse': 0.017537426891609007, 'mae': 0.09857991266631086, 'r2': 0.47537510106497527, 'pearson_corr': 0.6983204092384502}\n",
      "Test Metrics: {'mse': 0.017225016828970697, 'mae': 0.09528311769857292, 'r2': 0.42059102126662085, 'pearson_corr': 0.6577709443519555, 'connectome_corr': 0.5402742835543543, 'connectome_r2': -0.3665845184153881, 'geodesic_distance': 6.044226139647817}\n",
      "BEST VAL SCORE 0.02059142927825252\n",
      "BEST MODEL PARAMS {'input_dim': 6, 'hidden_dims': [512, 256, 128], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.001, 'batch_size': 32, 'symmetry_weight': 0, 'epochs': 100, 'device': 'cuda'}\n",
      "CPU Usage: 43.1%\n",
      "RAM Usage: 4.4%\n",
      "Available RAM: 962.8G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  1% |\n",
      "\n",
      " Test fold num: 2\n",
      "(12070, 6) (12070,) (812, 6) (812,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "3\n",
      "4\n",
      "Create sweep with ID: htd1c8yj\n",
      "Sweep URL: https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5by0wmoi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [512, 256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_102152-5by0wmoi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5by0wmoi' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold1_run82</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5by0wmoi' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5by0wmoi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0298, Val Loss: 0.0228\n",
      "Epoch 20/100, Train Loss: 0.0279, Val Loss: 0.0229\n",
      "Epoch 30/100, Train Loss: 0.0264, Val Loss: 0.0223\n",
      "Epoch 40/100, Train Loss: 0.0254, Val Loss: 0.0218\n",
      "Epoch 50/100, Train Loss: 0.0242, Val Loss: 0.0213\n",
      "Epoch 60/100, Train Loss: 0.0234, Val Loss: 0.0218\n",
      "Epoch 70/100, Train Loss: 0.0225, Val Loss: 0.0221\n",
      "Epoch 80/100, Train Loss: 0.0217, Val Loss: 0.0216\n",
      "Epoch 90/100, Train Loss: 0.0211, Val Loss: 0.0212\n",
      "Epoch 100/100, Train Loss: 0.0208, Val Loss: 0.0213\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0303, Val Loss: 0.0270\n",
      "Epoch 20/100, Train Loss: 0.0290, Val Loss: 0.0271\n",
      "Epoch 30/100, Train Loss: 0.0276, Val Loss: 0.0263\n",
      "Epoch 40/100, Train Loss: 0.0262, Val Loss: 0.0256\n",
      "Epoch 50/100, Train Loss: 0.0253, Val Loss: 0.0251\n",
      "Epoch 60/100, Train Loss: 0.0245, Val Loss: 0.0250\n",
      "Epoch 70/100, Train Loss: 0.0237, Val Loss: 0.0244\n",
      "Epoch 80/100, Train Loss: 0.0231, Val Loss: 0.0254\n",
      "Epoch 90/100, Train Loss: 0.0225, Val Loss: 0.0240\n",
      "Epoch 100/100, Train Loss: 0.0224, Val Loss: 0.0244\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0306, Val Loss: 0.0217\n",
      "Epoch 20/100, Train Loss: 0.0288, Val Loss: 0.0224\n",
      "Epoch 30/100, Train Loss: 0.0278, Val Loss: 0.0214\n",
      "Epoch 40/100, Train Loss: 0.0267, Val Loss: 0.0202\n",
      "Epoch 50/100, Train Loss: 0.0258, Val Loss: 0.0195\n",
      "Epoch 60/100, Train Loss: 0.0251, Val Loss: 0.0194\n",
      "Epoch 70/100, Train Loss: 0.0243, Val Loss: 0.0189\n",
      "Epoch 80/100, Train Loss: 0.0238, Val Loss: 0.0179\n",
      "Epoch 90/100, Train Loss: 0.0232, Val Loss: 0.0180\n",
      "Epoch 100/100, Train Loss: 0.0225, Val Loss: 0.0191\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>fold0_train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▂▃▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>fold0_val_loss</td><td>█▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_val_pearson</td><td>▁▆▆▆▆▆▇▇▇▇▇▇▇██▇▇███████████████████████</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>fold1_train_loss</td><td>█▆▅▅▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▂▃▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>fold1_val_loss</td><td>▇█▆▆▆▆▅▆▆▅▅▅▆▅▄▅▄▄▅▃▃▃▃▄▄▂▃▄▂▂▂▃▁▁▂▁▂▂▁▂</td></tr><tr><td>fold1_val_pearson</td><td>▁▂▃▄▄▅▅▅▅▆▅▅▅▅▅▆▅▇▆▇▆▇▇▆▇▆▇▇▇▆▇▇▇▇████▇█</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>fold2_train_loss</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▂▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>fold2_val_loss</td><td>█▄▄▄▄▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▃▂▂▂▁▂▂▂▃▂▂▂▂▁▂▂▁▁▂</td></tr><tr><td>fold2_val_pearson</td><td>▃▁▂▂▁▂▁▂▂▂▃▃▃▄▄▅▅▅▅▆▆▆▆▆▇▆▆▇▇▇▇▇█████▇█▇</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.02084</td></tr><tr><td>fold0_train_pearson</td><td>0.70754</td></tr><tr><td>fold0_val_loss</td><td>0.02131</td></tr><tr><td>fold0_val_pearson</td><td>0.60133</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.02238</td></tr><tr><td>fold1_train_pearson</td><td>0.67314</td></tr><tr><td>fold1_val_loss</td><td>0.02438</td></tr><tr><td>fold1_val_pearson</td><td>0.45373</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.02251</td></tr><tr><td>fold2_train_pearson</td><td>0.67482</td></tr><tr><td>fold2_val_loss</td><td>0.01909</td></tr><tr><td>fold2_val_pearson</td><td>0.59811</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.02191</td></tr><tr><td>mean_train_pearson</td><td>0.68517</td></tr><tr><td>mean_val_loss</td><td>0.02159</td></tr><tr><td>mean_val_pearson</td><td>0.55105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold1_run82</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5by0wmoi' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/5by0wmoi</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_102152-5by0wmoi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yrxdcwim with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [512, 256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_102907-yrxdcwim</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/yrxdcwim' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold1_run269</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/yrxdcwim' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/yrxdcwim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0294, Val Loss: 0.0228\n",
      "Epoch 20/100, Train Loss: 0.0269, Val Loss: 0.0234\n",
      "Epoch 30/100, Train Loss: 0.0258, Val Loss: 0.0230\n",
      "Epoch 40/100, Train Loss: 0.0251, Val Loss: 0.0245\n",
      "Epoch 50/100, Train Loss: 0.0246, Val Loss: 0.0237\n",
      "Epoch 60/100, Train Loss: 0.0240, Val Loss: 0.0221\n",
      "Epoch 70/100, Train Loss: 0.0239, Val Loss: 0.0237\n",
      "Epoch 80/100, Train Loss: 0.0238, Val Loss: 0.0239\n",
      "Epoch 90/100, Train Loss: 0.0235, Val Loss: 0.0253\n",
      "Epoch 100/100, Train Loss: 0.0237, Val Loss: 0.0240\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0308, Val Loss: 0.0264\n",
      "Epoch 20/100, Train Loss: 0.0279, Val Loss: 0.0254\n",
      "Epoch 30/100, Train Loss: 0.0271, Val Loss: 0.0243\n",
      "Epoch 40/100, Train Loss: 0.0267, Val Loss: 0.0232\n",
      "Epoch 50/100, Train Loss: 0.0261, Val Loss: 0.0243\n",
      "Epoch 60/100, Train Loss: 0.0259, Val Loss: 0.0230\n",
      "Epoch 70/100, Train Loss: 0.0256, Val Loss: 0.0240\n",
      "Epoch 80/100, Train Loss: 0.0255, Val Loss: 0.0217\n",
      "Epoch 90/100, Train Loss: 0.0253, Val Loss: 0.0226\n",
      "Epoch 100/100, Train Loss: 0.0251, Val Loss: 0.0233\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0298, Val Loss: 0.0226\n",
      "Epoch 20/100, Train Loss: 0.0276, Val Loss: 0.0209\n",
      "Epoch 30/100, Train Loss: 0.0269, Val Loss: 0.0187\n",
      "Epoch 40/100, Train Loss: 0.0263, Val Loss: 0.0220\n",
      "Epoch 50/100, Train Loss: 0.0263, Val Loss: 0.0265\n",
      "Epoch 60/100, Train Loss: 0.0260, Val Loss: 0.0229\n",
      "Epoch 70/100, Train Loss: 0.0256, Val Loss: 0.0228\n",
      "Epoch 80/100, Train Loss: 0.0255, Val Loss: 0.0233\n",
      "Epoch 90/100, Train Loss: 0.0258, Val Loss: 0.0211\n",
      "Epoch 100/100, Train Loss: 0.0252, Val Loss: 0.0202\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>fold0_train_loss</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████▇███████████████</td></tr><tr><td>fold0_val_loss</td><td>▃▃▂▂▂▂▃▂▂▃▃▃▄▂▂▃▂▂▂▃▂▂▂▁█▆▄▂▅▄▇▅▃▆▅▅▅▃▅▃</td></tr><tr><td>fold0_val_pearson</td><td>▁▆▆▆▆▇▇▇▇▇▇▇▇▇███▇█▇▇█▇▇▇█▇▇▇█▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>fold1_train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>fold1_val_loss</td><td>█▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_val_pearson</td><td>▁▃▄▃▄▅▆▇▇▆▇▇▇▇█▇█▇▇▇▇██▇▇▇▇███▇▇▇▇▇▇▇▇▆▇</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>fold2_train_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▅▅▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█████████████████████</td></tr><tr><td>fold2_val_loss</td><td>█▅▆▄▄▄▃▃▃▃▂▂▂▂▂▃▅▄▃▇▅▄▅▃▄▂▆▄▄▁▂▆▄▂▃▂▃▅▂▂</td></tr><tr><td>fold2_val_pearson</td><td>▁▁▂▂▃▃▄▃▄▅▅▆▇▆▆▆▆▇▇▇▇▇▇▆▇▇▇█▇███▇█▇██▇█▇</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅███████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.02375</td></tr><tr><td>fold0_train_pearson</td><td>0.5886</td></tr><tr><td>fold0_val_loss</td><td>0.02398</td></tr><tr><td>fold0_val_pearson</td><td>0.52183</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.02514</td></tr><tr><td>fold1_train_pearson</td><td>0.5593</td></tr><tr><td>fold1_val_loss</td><td>0.02334</td></tr><tr><td>fold1_val_pearson</td><td>0.44337</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.02523</td></tr><tr><td>fold2_train_pearson</td><td>0.53701</td></tr><tr><td>fold2_val_loss</td><td>0.0202</td></tr><tr><td>fold2_val_pearson</td><td>0.6184</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.02471</td></tr><tr><td>mean_train_pearson</td><td>0.56164</td></tr><tr><td>mean_val_loss</td><td>0.02251</td></tr><tr><td>mean_val_pearson</td><td>0.52787</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold1_run269</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/yrxdcwim' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/yrxdcwim</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_102907-yrxdcwim/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zd302azh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_103632-zd302azh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/zd302azh' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold1_run39</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/zd302azh' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/zd302azh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0232, Val Loss: 0.0311\n",
      "Epoch 20/100, Train Loss: 0.0177, Val Loss: 0.0288\n",
      "Epoch 30/100, Train Loss: 0.0153, Val Loss: 0.0288\n",
      "Epoch 40/100, Train Loss: 0.0136, Val Loss: 0.0300\n",
      "Epoch 50/100, Train Loss: 0.0127, Val Loss: 0.0335\n",
      "Epoch 60/100, Train Loss: 0.0145, Val Loss: 0.0464\n",
      "Epoch 70/100, Train Loss: 0.0144, Val Loss: 0.0331\n",
      "Epoch 80/100, Train Loss: 0.0114, Val Loss: 0.0276\n",
      "Epoch 90/100, Train Loss: 0.0102, Val Loss: 0.0279\n",
      "Epoch 100/100, Train Loss: 0.0094, Val Loss: 0.0282\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0251, Val Loss: 0.0326\n",
      "Epoch 20/100, Train Loss: 0.0200, Val Loss: 0.0316\n",
      "Epoch 30/100, Train Loss: 0.0174, Val Loss: 0.0315\n",
      "Epoch 40/100, Train Loss: 0.0156, Val Loss: 0.0312\n",
      "Epoch 50/100, Train Loss: 0.0142, Val Loss: 0.0333\n",
      "Epoch 60/100, Train Loss: 0.0141, Val Loss: 0.0318\n",
      "Epoch 70/100, Train Loss: 0.0153, Val Loss: 0.0449\n",
      "Epoch 80/100, Train Loss: 0.0144, Val Loss: 0.0300\n",
      "Epoch 90/100, Train Loss: 0.0125, Val Loss: 0.0309\n",
      "Epoch 100/100, Train Loss: 0.0115, Val Loss: 0.0313\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0238, Val Loss: 0.0251\n",
      "Epoch 20/100, Train Loss: 0.0190, Val Loss: 0.0230\n",
      "Epoch 30/100, Train Loss: 0.0167, Val Loss: 0.0225\n",
      "Epoch 40/100, Train Loss: 0.0150, Val Loss: 0.0224\n",
      "Epoch 50/100, Train Loss: 0.0138, Val Loss: 0.0229\n",
      "Epoch 60/100, Train Loss: 0.0132, Val Loss: 0.0248\n",
      "Epoch 70/100, Train Loss: 0.0164, Val Loss: 0.0287\n",
      "Epoch 80/100, Train Loss: 0.0155, Val Loss: 0.0289\n",
      "Epoch 90/100, Train Loss: 0.0135, Val Loss: 0.0229\n",
      "Epoch 100/100, Train Loss: 0.0117, Val Loss: 0.0227\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>fold0_train_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▂▃▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>fold0_val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>fold0_val_pearson</td><td>▁▃▄▅▆▆▆▆▆▆▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▅▅▆▇▇████████▇▇</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>fold1_train_loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▂▃▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█████</td></tr><tr><td>fold1_val_loss</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▅▄▅▇▄▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_val_pearson</td><td>▁▂▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▇▆▄▄▆▆▇███████</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>fold2_train_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▆▆▇▇▇▆▆▇▇▇▇▇███</td></tr><tr><td>fold2_val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_val_pearson</td><td>▁▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▅▅▆▆▆▆▆▇▇█████</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.00943</td></tr><tr><td>fold0_train_pearson</td><td>0.87195</td></tr><tr><td>fold0_val_loss</td><td>0.02824</td></tr><tr><td>fold0_val_pearson</td><td>0.47477</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.01153</td></tr><tr><td>fold1_train_pearson</td><td>0.85081</td></tr><tr><td>fold1_val_loss</td><td>0.03134</td></tr><tr><td>fold1_val_pearson</td><td>0.35712</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.01166</td></tr><tr><td>fold2_train_pearson</td><td>0.83695</td></tr><tr><td>fold2_val_loss</td><td>0.02273</td></tr><tr><td>fold2_val_pearson</td><td>0.55162</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.01087</td></tr><tr><td>mean_train_pearson</td><td>0.85324</td></tr><tr><td>mean_val_loss</td><td>0.02744</td></tr><tr><td>mean_val_pearson</td><td>0.46117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold1_run39</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/zd302azh' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/zd302azh</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_103632-zd302azh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r0gvxago with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [512, 256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_104000-r0gvxago</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/r0gvxago' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold1_run978</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/r0gvxago' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/r0gvxago</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0337, Val Loss: 0.0271\n",
      "Epoch 20/100, Train Loss: 0.0247, Val Loss: 0.0273\n",
      "Epoch 30/100, Train Loss: 0.0262, Val Loss: 0.0270\n",
      "Epoch 40/100, Train Loss: 0.0183, Val Loss: 0.0240\n",
      "Epoch 50/100, Train Loss: 0.0192, Val Loss: 0.0254\n",
      "Epoch 60/100, Train Loss: 0.0165, Val Loss: 0.0253\n",
      "Epoch 70/100, Train Loss: 0.0186, Val Loss: 0.0240\n",
      "Epoch 80/100, Train Loss: 0.0160, Val Loss: 0.0228\n",
      "Epoch 90/100, Train Loss: 0.0151, Val Loss: 0.0234\n",
      "Epoch 100/100, Train Loss: 0.0151, Val Loss: 0.0236\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0364, Val Loss: 0.0330\n",
      "Epoch 20/100, Train Loss: 0.0234, Val Loss: 0.0301\n",
      "Epoch 30/100, Train Loss: 0.0292, Val Loss: 0.0318\n",
      "Epoch 40/100, Train Loss: 0.0239, Val Loss: 0.0300\n",
      "Epoch 50/100, Train Loss: 0.0223, Val Loss: 0.0285\n",
      "Epoch 60/100, Train Loss: 0.0203, Val Loss: 0.0288\n",
      "Epoch 70/100, Train Loss: 0.0203, Val Loss: 0.0283\n",
      "Epoch 80/100, Train Loss: 0.0187, Val Loss: 0.0263\n",
      "Epoch 90/100, Train Loss: 0.0187, Val Loss: 0.0274\n",
      "Epoch 100/100, Train Loss: 0.0167, Val Loss: 0.0276\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0356, Val Loss: 0.0286\n",
      "Epoch 20/100, Train Loss: 0.0254, Val Loss: 0.0236\n",
      "Epoch 30/100, Train Loss: 0.0224, Val Loss: 0.0214\n",
      "Epoch 40/100, Train Loss: 0.0204, Val Loss: 0.0236\n",
      "Epoch 50/100, Train Loss: 0.0202, Val Loss: 0.0228\n",
      "Epoch 60/100, Train Loss: 0.0191, Val Loss: 0.0217\n",
      "Epoch 70/100, Train Loss: 0.0174, Val Loss: 0.0265\n",
      "Epoch 80/100, Train Loss: 0.0181, Val Loss: 0.0217\n",
      "Epoch 90/100, Train Loss: 0.0173, Val Loss: 0.0224\n",
      "Epoch 100/100, Train Loss: 0.0166, Val Loss: 0.0263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>fold0_train_loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▂▃▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████▇█████████</td></tr><tr><td>fold0_val_loss</td><td>█▂▂▂▂▂▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_val_pearson</td><td>▁▂▃▄▄▄▄▄▆▆▅▅▅▆▆▆▇▇▇▇▅▅█▇▆█▆▇▆▇▇█▆▇▇▇▇██▇</td></tr><tr><td>fold1_epoch</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>fold1_train_loss</td><td>█▆▆▄▄▄▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>fold1_val_loss</td><td>█▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_val_pearson</td><td>▁▃▃▄▄▄▆▆▆▅▅▆▆▆▅▅▆▇▆▅▆▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇██</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>fold2_train_loss</td><td>█▄▄▃▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▂▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇████████████████</td></tr><tr><td>fold2_val_loss</td><td>█▅▅▅▃▃▃▃▂▂▁▃▃▂▃▂▁▃▂▃▁▁▁▂▃▃▃▂▂▁▂▂▂▂▂▃▂▂▂▃</td></tr><tr><td>fold2_val_pearson</td><td>▁▄▅▃▅▅▄▇▅▅▆▆▅▄▅▆▅▆▇▇██▇▅█▇▅▆▆▆▇██▆▇▇▆▆▆▅</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅██████████████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.01515</td></tr><tr><td>fold0_train_pearson</td><td>0.85971</td></tr><tr><td>fold0_val_loss</td><td>0.02358</td></tr><tr><td>fold0_val_pearson</td><td>0.52698</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.01669</td></tr><tr><td>fold1_train_pearson</td><td>0.83559</td></tr><tr><td>fold1_val_loss</td><td>0.02764</td></tr><tr><td>fold1_val_pearson</td><td>0.40318</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.0166</td></tr><tr><td>fold2_train_pearson</td><td>0.85242</td></tr><tr><td>fold2_val_loss</td><td>0.02632</td></tr><tr><td>fold2_val_pearson</td><td>0.48096</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.01614</td></tr><tr><td>mean_train_pearson</td><td>0.84924</td></tr><tr><td>mean_val_loss</td><td>0.02585</td></tr><tr><td>mean_val_pearson</td><td>0.47037</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold1_run978</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/r0gvxago' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/r0gvxago</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_104000-r0gvxago/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hnjfl2sb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_104347-hnjfl2sb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold1_run457</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0215, Val Loss: 0.0310\n",
      "Epoch 20/100, Train Loss: 0.0173, Val Loss: 0.0301\n",
      "Epoch 30/100, Train Loss: 0.0151, Val Loss: 0.0339\n",
      "Epoch 40/100, Train Loss: 0.0153, Val Loss: 0.0370\n",
      "Epoch 50/100, Train Loss: 0.0153, Val Loss: 0.0419\n",
      "Epoch 60/100, Train Loss: 0.0135, Val Loss: 0.0411\n",
      "Epoch 70/100, Train Loss: 0.0120, Val Loss: 0.0372\n",
      "Epoch 80/100, Train Loss: 0.0133, Val Loss: 0.0380\n",
      "Epoch 90/100, Train Loss: 0.0135, Val Loss: 0.0412\n",
      "Epoch 100/100, Train Loss: 0.0104, Val Loss: 0.0426\n",
      "Processing inner fold 1\n",
      "Epoch 10/100, Train Loss: 0.0234, Val Loss: 0.0358\n",
      "Epoch 20/100, Train Loss: 0.0200, Val Loss: 0.0361\n",
      "Epoch 30/100, Train Loss: 0.0180, Val Loss: 0.0332\n",
      "Epoch 40/100, Train Loss: 0.0177, Val Loss: 0.0389\n",
      "Epoch 50/100, Train Loss: 0.0160, Val Loss: 0.0370\n",
      "Epoch 60/100, Train Loss: 0.0145, Val Loss: 0.0376\n",
      "Epoch 70/100, Train Loss: 0.0160, Val Loss: 0.0411\n",
      "Epoch 80/100, Train Loss: 0.0149, Val Loss: 0.0387\n",
      "Epoch 90/100, Train Loss: 0.0135, Val Loss: 0.0391\n",
      "Epoch 100/100, Train Loss: 0.0131, Val Loss: 0.0385\n",
      "Processing inner fold 2\n",
      "Epoch 10/100, Train Loss: 0.0229, Val Loss: 0.0324\n",
      "Epoch 20/100, Train Loss: 0.0187, Val Loss: 0.0325\n",
      "Epoch 30/100, Train Loss: 0.0166, Val Loss: 0.0349\n",
      "Epoch 40/100, Train Loss: 0.0173, Val Loss: 0.0436\n",
      "Epoch 50/100, Train Loss: 0.0145, Val Loss: 0.0304\n",
      "Epoch 60/100, Train Loss: 0.0143, Val Loss: 0.0390\n",
      "Epoch 70/100, Train Loss: 0.0153, Val Loss: 0.0249\n",
      "Epoch 80/100, Train Loss: 0.0132, Val Loss: 0.0230\n",
      "Epoch 90/100, Train Loss: 0.0118, Val Loss: 0.0266\n",
      "Epoch 100/100, Train Loss: 0.0145, Val Loss: 0.0292\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>fold0_train_loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold0_train_pearson</td><td>▁▁▃▄▄▅▅▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▆▆▆▇███</td></tr><tr><td>fold0_val_loss</td><td>▂▂▂▂▂▁▁▁▂▂▃▃▄▅▅▅▅▇████▇▇▆▅▄▄▄▅▆▇██▇▇▇▇▇▇</td></tr><tr><td>fold0_val_pearson</td><td>▁▃▄▄▅▆▇███▇▇▇▇▆▅▆▇▆▆▆▅▄▄▄▅▅▅▅▆▆▇▇▇▆▆████</td></tr><tr><td>fold1_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>fold1_train_loss</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fold1_train_pearson</td><td>▁▂▃▄▄▄▅▅▆▆▆▆▆▆▆▇▆▆▆▆▇▇▇▇█████▇▇▇▇▇██████</td></tr><tr><td>fold1_val_loss</td><td>█▃▃▃▃▃▄▄▄▄▁▁▁▂▃▅▅▄▄▄▅▅▅▆▇▄▄▄▄▄▆▆▆▆▅▅▅▅▅▅</td></tr><tr><td>fold1_val_pearson</td><td>▁▂▂▂▃▄▅▅▅▅▆▆▇██▆▆▅▅▄▆▆▆▅▅▅▄▄▃▁▃▄▃▃▄▅▄▄▄▅</td></tr><tr><td>fold2_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>fold2_train_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>fold2_train_pearson</td><td>▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████▇▇▇▇▇▇█████████</td></tr><tr><td>fold2_val_loss</td><td>▄▅▄▄▄▄▄▄▄▄▄▅▆▅█▆▅▄▃▃▄▄▅▆▂▂▂▂▂▁▁▁▂▂▂▂▃▂▂▃</td></tr><tr><td>fold2_val_pearson</td><td>▁▅▅▃▄▄▃▃▄▄▄▃▃▃▃▄▅▅▅▅▆▅▅▄▅▇▇▇▇▇███▇▇▇▇▇▆▆</td></tr><tr><td>inner_fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅███████████████</td></tr><tr><td>mean_train_loss</td><td>▁</td></tr><tr><td>mean_train_pearson</td><td>▁</td></tr><tr><td>mean_val_loss</td><td>▁</td></tr><tr><td>mean_val_pearson</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold0_epoch</td><td>99</td></tr><tr><td>fold0_train_loss</td><td>0.01039</td></tr><tr><td>fold0_train_pearson</td><td>0.85736</td></tr><tr><td>fold0_val_loss</td><td>0.04259</td></tr><tr><td>fold0_val_pearson</td><td>0.4245</td></tr><tr><td>fold1_epoch</td><td>99</td></tr><tr><td>fold1_train_loss</td><td>0.01306</td></tr><tr><td>fold1_train_pearson</td><td>0.81638</td></tr><tr><td>fold1_val_loss</td><td>0.03849</td></tr><tr><td>fold1_val_pearson</td><td>0.29568</td></tr><tr><td>fold2_epoch</td><td>99</td></tr><tr><td>fold2_train_loss</td><td>0.01447</td></tr><tr><td>fold2_train_pearson</td><td>0.77807</td></tr><tr><td>fold2_val_loss</td><td>0.02918</td></tr><tr><td>fold2_val_pearson</td><td>0.48975</td></tr><tr><td>inner_fold</td><td>2</td></tr><tr><td>mean_train_loss</td><td>0.01264</td></tr><tr><td>mean_train_pearson</td><td>0.81727</td></tr><tr><td>mean_val_loss</td><td>0.03675</td></tr><tr><td>mean_val_pearson</td><td>0.40331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_FC_random_fold1_run457</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_104347-hnjfl2sb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by +summary_metrics.mean_val_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.0313, Val Loss: 0.0259\n",
      "Epoch 20/100, Train Loss: 0.0299, Val Loss: 0.0255\n",
      "Epoch 30/100, Train Loss: 0.0288, Val Loss: 0.0209\n",
      "Epoch 40/100, Train Loss: 0.0282, Val Loss: 0.0202\n",
      "Epoch 50/100, Train Loss: 0.0277, Val Loss: 0.0200\n",
      "Epoch 60/100, Train Loss: 0.0275, Val Loss: 0.0210\n",
      "Epoch 70/100, Train Loss: 0.0273, Val Loss: 0.0182\n",
      "Epoch 80/100, Train Loss: 0.0268, Val Loss: 0.0181\n",
      "Epoch 90/100, Train Loss: 0.0267, Val Loss: 0.0183\n",
      "Epoch 100/100, Train Loss: 0.0263, Val Loss: 0.0180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_105447-hnjfl2sb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb' target=\"_blank\">dynamic_nn_euclidean_predFC_random_fold1_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/htd1c8yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.02159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynamic_nn_euclidean_predFC_random_fold1_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/hnjfl2sb</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_105447-hnjfl2sb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "\n",
      "Train Metrics: {'mse': 0.016370828932268964, 'mae': 0.09973268346674312, 'r2': 0.5023446928247877, 'pearson_corr': 0.7873918083388076}\n",
      "Test Metrics: {'mse': 0.01731252240470004, 'mae': 0.107680718659563, 'r2': 0.5428382010098062, 'pearson_corr': 0.8162463038473294, 'connectome_corr': 0.7478454458674666, 'connectome_r2': -7.112940323753401, 'geodesic_distance': 6.578642500028277}\n",
      "BEST VAL SCORE 0.02159226180699009\n",
      "BEST MODEL PARAMS {'input_dim': 6, 'hidden_dims': [512, 256, 128], 'dropout_rate': 0.2, 'learning_rate': 0.0003, 'weight_decay': 0, 'batch_size': 32, 'symmetry_weight': 0.1, 'epochs': 100, 'device': 'cuda'}\n",
      "CPU Usage: 42.3%\n",
      "RAM Usage: 4.4%\n",
      "Available RAM: 963.2G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  1% |\n",
      "\n",
      " Test fold num: 3\n",
      "(12126, 6) (12126,) (756, 6) (756,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "2\n",
      "4\n",
      "Create sweep with ID: gx2ldexj\n",
      "Sweep URL: https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v4ird0tn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: [256, 128]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_dim: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsymmetry_weight: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_105454-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">dynamic_nn_euclidean_FC_random_fold2_run752</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inner fold 0\n",
      "Epoch 10/100, Train Loss: 0.0437, Val Loss: 0.0259\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/agents/pyagent.py:219\u001b[0m, in \u001b[0;36mAgent._run_jobs_from_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_status[run_id] \u001b[38;5;241m=\u001b[39m RunStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m--> 219\u001b[0m thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    220\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThread joined for run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m single_sim_run(\n\u001b[1;32m      2\u001b[0m               cv_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m               random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      4\u001b[0m               model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_nn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m               feature_type\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}],\n\u001b[1;32m      6\u001b[0m               connectome_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m               use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m               use_shared_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m               test_shared_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m               save_sim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m               search_method\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m               track_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m               )\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim_run.py:211\u001b[0m, in \u001b[0;36msingle_sim_run\u001b[0;34m(feature_type, cv_type, model_type, use_gpu, connectome_target, feature_interactions, use_shared_regions, test_shared_regions, resolution, random_seed, save_sim, search_method, save_model_json, track_wandb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Structural\u001b[39;00m\n\u001b[1;32m    195\u001b[0m sim \u001b[38;5;241m=\u001b[39m Simulation(\n\u001b[1;32m    196\u001b[0m                 feature_type\u001b[38;5;241m=\u001b[39mfeature_type,\n\u001b[1;32m    197\u001b[0m                 cv_type\u001b[38;5;241m=\u001b[39mcv_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 save_model_json\u001b[38;5;241m=\u001b[39msave_model_json\n\u001b[1;32m    209\u001b[0m             )\n\u001b[0;32m--> 211\u001b[0m sim\u001b[38;5;241m.\u001b[39mrun_sim(search_method, track_wandb)\n\u001b[1;32m    212\u001b[0m single_model_results\u001b[38;5;241m.\u001b[39mappend(sim\u001b[38;5;241m.\u001b[39mresults)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Save sim data\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim.py:330\u001b[0m, in \u001b[0;36mSimulation.run_sim\u001b[0;34m(self, search_method, track_wandb)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m track_wandb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_nn\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;66;03m# need to change this to run for any epoch based model\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m--> 330\u001b[0m     best_model, best_val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_innercv_wandb(X_train, Y_train, X_test, Y_test, train_indices, test_indices, train_network_dict, fold_idx, search_method\u001b[38;5;241m=\u001b[39msearch_method, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)                \n\u001b[1;32m    331\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, val_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test))\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim.py:232\u001b[0m, in \u001b[0;36mSimulation.run_innercv_wandb\u001b[0;34m(self, X_train, Y_train, X_test, Y_test, train_indices, test_indices, train_network_dict, outer_fold_idx, search_method, n_iter)\u001b[0m\n\u001b[1;32m    229\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msweep(sweep\u001b[38;5;241m=\u001b[39msweep_config, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgx2conn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Run sweep\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, function\u001b[38;5;241m=\u001b[39mtrain_sweep_wrapper, count\u001b[38;5;241m=\u001b[39mn_iter)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Get best run from sweep\u001b[39;00m\n\u001b[1;32m    235\u001b[0m api \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mApi()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/wandb_agent.py:570\u001b[0m, in \u001b[0;36magent\u001b[0;34m(sweep_id, function, entity, project, count)\u001b[0m\n\u001b[1;32m    568\u001b[0m wandb_sdk\u001b[38;5;241m.\u001b[39mwandb_login\u001b[38;5;241m.\u001b[39m_login(_silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function:\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyagent(sweep_id, function, entity, project, count)\n\u001b[1;32m    571\u001b[0m in_jupyter \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mwandb_sdk\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mipython\u001b[38;5;241m.\u001b[39m_get_python_type() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_agent(\n\u001b[1;32m    573\u001b[0m     sweep_id,\n\u001b[1;32m    574\u001b[0m     function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     count\u001b[38;5;241m=\u001b[39mcount,\n\u001b[1;32m    579\u001b[0m )\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/agents/pyagent.py:356\u001b[0m, in \u001b[0;36mpyagent\u001b[0;34m(sweep_id, function, entity, project, count)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction parameter must be callable!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    349\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[1;32m    350\u001b[0m     sweep_id,\n\u001b[1;32m    351\u001b[0m     function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m     count\u001b[38;5;241m=\u001b[39mcount,\n\u001b[1;32m    355\u001b[0m )\n\u001b[0;32m--> 356\u001b[0m agent\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/agents/pyagent.py:334\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_heartbeat_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# self._main_thread.join()\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_jobs_from_queue()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/wandb/agents/pyagent.py:271\u001b[0m, in \u001b[0;36mAgent._run_jobs_from_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCtrl + C detected. Stopping sweep.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    272\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCtrl + C detected. Stopping sweep.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/logging/__init__.py:1467\u001b[0m, in \u001b[0;36mLogger.debug\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m=\u001b[39m _checkLevel(level)\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39m_clear_cache()\n\u001b[0;32m-> 1467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdebug\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03m    Log 'msg % args' with severity 'DEBUG'.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m    logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(DEBUG):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='dynamic_nn',\n",
    "              feature_type=[{'euclidean': None}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('grid', 'mse'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77e6c276-85cb-450a-86fe-06cb788116cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name euclidean\n",
      "processing_type None\n",
      "X shape (114, 3)\n",
      "\n",
      " Test fold num: 1\n",
      "(12070, 6) (12070,) (812, 6) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "2\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.841, test=0.810) total time=   0.3s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.857, test=0.605) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.845, test=0.763) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.749, test=0.782) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.758, test=0.579) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.743, test=0.712) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.748, test=0.787) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.762, test=0.557) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.747, test=0.713) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.709, test=0.771) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.730, test=0.509) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.707, test=0.712) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.594, test=0.680) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.621, test=0.394) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.601, test=0.578) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.986, test=0.825) total time=   0.3s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.988, test=0.669) total time=   0.3s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.987, test=0.794) total time=   0.3s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.814, test=0.813) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.828, test=0.614) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.811, test=0.751) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.796, test=0.803) total time=   0.3s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.805, test=0.622) total time=   0.3s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.791, test=0.752) total time=   0.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m single_sim_run(\n\u001b[1;32m      2\u001b[0m               cv_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m               random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      4\u001b[0m               model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m               feature_type\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}],\n\u001b[1;32m      6\u001b[0m               connectome_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m               use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m               use_shared_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m               test_shared_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m               save_sim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m               search_method\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbayes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m               track_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m               )\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim_run.py:211\u001b[0m, in \u001b[0;36msingle_sim_run\u001b[0;34m(feature_type, cv_type, model_type, use_gpu, connectome_target, feature_interactions, use_shared_regions, test_shared_regions, resolution, random_seed, save_sim, search_method, save_model_json, track_wandb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Structural\u001b[39;00m\n\u001b[1;32m    195\u001b[0m sim \u001b[38;5;241m=\u001b[39m Simulation(\n\u001b[1;32m    196\u001b[0m                 feature_type\u001b[38;5;241m=\u001b[39mfeature_type,\n\u001b[1;32m    197\u001b[0m                 cv_type\u001b[38;5;241m=\u001b[39mcv_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 save_model_json\u001b[38;5;241m=\u001b[39msave_model_json\n\u001b[1;32m    209\u001b[0m             )\n\u001b[0;32m--> 211\u001b[0m sim\u001b[38;5;241m.\u001b[39mrun_sim(search_method, track_wandb)\n\u001b[1;32m    212\u001b[0m single_model_results\u001b[38;5;241m.\u001b[39mappend(sim\u001b[38;5;241m.\u001b[39mresults)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Save sim data\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim.py:334\u001b[0m, in \u001b[0;36mSimulation.run_sim\u001b[0;34m(self, search_method, track_wandb)\u001b[0m\n\u001b[1;32m    332\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, val_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     best_model, best_val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_innercv(train_indices, test_indices, train_network_dict, search_method\u001b[38;5;241m=\u001b[39msearch_method, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    335\u001b[0m     best_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m    336\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim.py:285\u001b[0m, in \u001b[0;36mSimulation.run_innercv\u001b[0;34m(self, train_indices, test_indices, train_network_dict, search_method, n_iter)\u001b[0m\n\u001b[1;32m    282\u001b[0m     param_search, X_combined, Y_combined \u001b[38;5;241m=\u001b[39m bayes_search_init(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_acceleration, model, X_combined, Y_combined, param_dist, train_test_indices, n_iter\u001b[38;5;241m=\u001b[39mn_iter, metric\u001b[38;5;241m=\u001b[39mmetric)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV on the combined data\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m param_search\u001b[38;5;241m.\u001b[39mfit(X_combined, Y_combined)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Display comprehensive results\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mParameter Search CV Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/skopt/searchcv.py:542\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/skopt/searchcv.py:599\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 599\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(\n\u001b[1;32m    600\u001b[0m         search_space,\n\u001b[1;32m    601\u001b[0m         optimizer,\n\u001b[1;32m    602\u001b[0m         score_name,\n\u001b[1;32m    603\u001b[0m         evaluate_candidates,\n\u001b[1;32m    604\u001b[0m         n_points\u001b[38;5;241m=\u001b[39mn_points_adjusted,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/skopt/searchcv.py:453\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    451\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m--> 453\u001b[0m all_results \u001b[38;5;241m=\u001b[39m evaluate_candidates(params_dict)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1090\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m (\n\u001b[1;32m   1082\u001b[0m     model,\n\u001b[1;32m   1083\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1089\u001b[0m )\n\u001b[0;32m-> 1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1091\u001b[0m     params,\n\u001b[1;32m   1092\u001b[0m     train_dmatrix,\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1094\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1095\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1096\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1097\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1098\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1099\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1100\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1101\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[0;32m-> 2051\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2052\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2053\u001b[0m         )\n\u001b[1;32m   2054\u001b[0m     )\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='xgboost',\n",
    "              feature_type=[{'euclidean': None}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('bayes', 'pearson'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84982726-b743-4aee-b817-a0b77fd64b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name structural\n",
      "processing_type spectral_A_20\n",
      "X shape (114, 20)\n",
      "\n",
      " Test fold num: 1\n",
      "(12070, 40) (12070,) (812, 40) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "2\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.910, test=0.809) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.921, test=0.648) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.912, test=0.804) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.776, test=0.781) total time=   0.5s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.784, test=0.560) total time=   0.5s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.774, test=0.785) total time=   0.5s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.782, test=0.776) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.796, test=0.584) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.787, test=0.794) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.715, test=0.710) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.734, test=0.480) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.714, test=0.773) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.610, test=0.632) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.634, test=0.346) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.598, test=0.682) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.830) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.696) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.797) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.857, test=0.802) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.878, test=0.631) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.861, test=0.802) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.831, test=0.818) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.833, test=0.630) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.827, test=0.816) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.682, test=0.696) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.702, test=0.448) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.659, test=0.721) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.677, test=0.708) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.708, test=0.452) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.680, test=0.725) total time=   0.2s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.7744875649955252\n",
      "\n",
      "Train Metrics: {'mse': 0.0006210155204882609, 'mae': 0.019405889051554683, 'r2': 0.9814225766022084, 'pearson_corr': 0.9909086196217815}\n",
      "Test Metrics: {'mse': 0.007662449339002795, 'mae': 0.05990534182458078, 'r2': 0.7422532593035983, 'pearson_corr': 0.8615837644755121, 'connectome_corr': 0.7622451456005819, 'connectome_r2': 0.5059352280174134, 'geodesic_distance': 5.432658729166449}\n",
      "BEST VAL SCORE 0.7744875649955252\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_112435-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_structural_spectral_A_20_predFC_random_fold0_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.77449</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_structural_spectral_A_20_predFC_random_fold0_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_112435-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 43.6%\n",
      "RAM Usage: 6.3%\n",
      "Available RAM: 943.5G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 34% |\n",
      "\n",
      " Test fold num: 2\n",
      "(12070, 40) (12070,) (812, 40) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.902, test=0.751) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.900, test=0.636) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.897, test=0.809) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.746, test=0.713) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.750, test=0.546) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.735, test=0.790) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.763, test=0.724) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.769, test=0.574) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.754, test=0.791) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.710, test=0.681) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.725, test=0.499) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.697, test=0.768) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.560, test=0.546) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.578, test=0.263) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.546, test=0.705) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.818) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.679) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.831) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.854, test=0.747) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.853, test=0.661) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.844, test=0.794) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.798, test=0.752) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.798, test=0.613) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.787, test=0.813) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.649, test=0.626) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.659, test=0.393) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.631, test=0.741) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.639, test=0.610) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.650, test=0.385) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.628, test=0.735) total time=   0.2s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.7758276289206564\n",
      "\n",
      "Train Metrics: {'mse': 0.0006355905673706079, 'mae': 0.019500724921117082, 'r2': 0.9806787414155302, 'pearson_corr': 0.9905485842217123}\n",
      "Test Metrics: {'mse': 0.007440853559069646, 'mae': 0.057219163406732894, 'r2': 0.8035136694948986, 'pearson_corr': 0.8972587856334614, 'connectome_corr': 0.7958709725995459, 'connectome_r2': 0.2795932707747635, 'geodesic_distance': 5.071492299022182}\n",
      "BEST VAL SCORE 0.7758276289206564\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_112446-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_structural_spectral_A_20_predFC_random_fold1_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.77583</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_structural_spectral_A_20_predFC_random_fold1_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_112446-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 42.6%\n",
      "RAM Usage: 6.2%\n",
      "Available RAM: 944.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 34% |\n",
      "\n",
      " Test fold num: 3\n",
      "(12126, 40) (12126,) (756, 40) (756,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "2\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.916, test=0.703) total time=   0.3s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.910, test=0.808) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.908, test=0.806) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.785, test=0.680) total time=   0.5s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.776, test=0.736) total time=   0.5s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.769, test=0.787) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.799, test=0.704) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.792, test=0.746) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.792, test=0.777) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.749, test=0.666) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.748, test=0.689) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.754, test=0.801) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.617, test=0.581) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.614, test=0.545) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.630, test=0.668) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.752) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.818) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.996, test=0.826) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.869, test=0.710) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.871, test=0.800) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.868, test=0.790) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.832, test=0.717) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.825, test=0.771) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.818, test=0.814) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.698, test=0.599) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.690, test=0.657) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.675, test=0.730) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.703, test=0.613) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.694, test=0.672) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.691, test=0.693) total time=   0.2s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.7986328158873616\n",
      "\n",
      "Train Metrics: {'mse': 0.000662425106155722, 'mae': 0.0198908579608497, 'r2': 0.9801188677120769, 'pearson_corr': 0.9902897102431489}\n",
      "Test Metrics: {'mse': 0.009469663746600977, 'mae': 0.06739890985781175, 'r2': 0.697816079108099, 'pearson_corr': 0.836533644830454, 'connectome_corr': 0.7607976107729945, 'connectome_r2': 0.46163836953733445, 'geodesic_distance': 6.059807663602764}\n",
      "BEST VAL SCORE 0.7986328158873616\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_112457-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_structural_spectral_A_20_predFC_random_fold2_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.79863</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_structural_spectral_A_20_predFC_random_fold2_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_112457-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 42.5%\n",
      "RAM Usage: 6.3%\n",
      "Available RAM: 944.0G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 34% |\n",
      "\n",
      " Test fold num: 4\n",
      "(12126, 40) (12126,) (756, 40) (756,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "2\n",
      "3\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.899, test=0.770) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.890, test=0.803) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.897, test=0.704) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.705, test=0.675) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.701, test=0.743) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.704, test=0.580) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.727, test=0.689) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.713, test=0.757) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.729, test=0.607) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.550, test=0.569) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.563, test=0.646) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.562, test=0.270) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.508, test=0.550) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.479, test=0.530) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.521, test=0.258) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.996, test=0.811) total time=   0.4s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.996, test=0.836) total time=   0.4s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.997, test=0.763) total time=   0.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.844, test=0.737) total time=   0.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.834, test=0.777) total time=   0.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.848, test=0.711) total time=   0.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.755, test=0.713) total time=   0.3s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.755, test=0.782) total time=   0.3s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.756, test=0.656) total time=   0.3s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.594, test=0.595) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.583, test=0.673) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.599, test=0.391) total time=   0.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.641, test=0.630) total time=   0.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.624, test=0.698) total time=   0.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.648, test=0.466) total time=   0.2s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8031158911648472\n",
      "\n",
      "Train Metrics: {'mse': 0.0006791631959356874, 'mae': 0.0203094279969541, 'r2': 0.9796326935357412, 'pearson_corr': 0.9900604947612802}\n",
      "Test Metrics: {'mse': 0.0074071602736818995, 'mae': 0.057016760852192136, 'r2': 0.7592661839145378, 'pearson_corr': 0.8715399935088349, 'connectome_corr': 0.7213574264931617, 'connectome_r2': 0.34033994270403944, 'geodesic_distance': 5.613401736792768}\n",
      "BEST VAL SCORE 0.8031158911648472\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_112508-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_structural_spectral_A_20_predFC_random_fold3_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.80312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_structural_spectral_A_20_predFC_random_fold3_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_112508-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 43.1%\n",
      "RAM Usage: 6.2%\n",
      "Available RAM: 944.1G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 34% |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 0.0006210155204882609,\n",
       "    'mae': 0.019405889051554683,\n",
       "    'r2': 0.9814225766022084,\n",
       "    'pearson_corr': 0.9909086196217815},\n",
       "   'best_val_score': 0.7744875649955252,\n",
       "   'test_metrics': {'mse': 0.007662449339002795,\n",
       "    'mae': 0.05990534182458078,\n",
       "    'r2': 0.7422532593035983,\n",
       "    'pearson_corr': 0.8615837644755121,\n",
       "    'connectome_corr': 0.7622451456005819,\n",
       "    'connectome_r2': 0.5059352280174134,\n",
       "    'geodesic_distance': 5.432658729166449},\n",
       "   'y_true': array([ 0.25718  ,  0.25718  ,  0.21614  ,  0.21614  ,  0.17879  ,\n",
       "           0.17879  ,  0.20003  ,  0.20003  ,  0.18572  ,  0.18572  ,\n",
       "           0.25026  ,  0.25026  ,  0.11669  ,  0.11669  ,  0.24455  ,\n",
       "           0.24455  ,  0.11424  ,  0.11424  ,  0.29925  ,  0.29925  ,\n",
       "           0.18083  ,  0.18083  ,  0.18606  ,  0.18606  ,  0.32224  ,\n",
       "           0.32224  ,  0.18932  ,  0.18932  ,  0.40234  ,  0.40234  ,\n",
       "           0.18223  ,  0.18223  ,  0.26495  ,  0.26495  ,  0.23553  ,\n",
       "           0.23553  ,  0.21608  ,  0.21608  ,  0.059681 ,  0.059681 ,\n",
       "           0.21966  ,  0.21966  ,  0.14268  ,  0.14268  ,  0.22835  ,\n",
       "           0.22835  ,  0.42801  ,  0.42801  ,  0.21521  ,  0.21521  ,\n",
       "           0.057658 ,  0.057658 ,  0.024612 ,  0.024612 ,  0.063759 ,\n",
       "           0.063759 ,  0.37308  ,  0.37308  ,  0.34132  ,  0.34132  ,\n",
       "           0.32791  ,  0.32791  ,  0.27542  ,  0.27542  ,  0.36759  ,\n",
       "           0.36759  ,  0.32945  ,  0.32945  ,  0.19856  ,  0.19856  ,\n",
       "           0.24324  ,  0.24324  ,  0.28955  ,  0.28955  ,  0.30675  ,\n",
       "           0.30675  ,  0.23096  ,  0.23096  ,  0.26071  ,  0.26071  ,\n",
       "           0.73698  ,  0.73698  ,  0.4264   ,  0.4264   ,  0.27143  ,\n",
       "           0.27143  ,  0.45758  ,  0.45758  ,  0.36727  ,  0.36727  ,\n",
       "           0.35085  ,  0.35085  ,  0.25663  ,  0.25663  ,  0.27071  ,\n",
       "           0.27071  ,  0.33455  ,  0.33455  ,  0.20266  ,  0.20266  ,\n",
       "           0.31722  ,  0.31722  ,  0.14571  ,  0.14571  ,  0.027941 ,\n",
       "           0.027941 ,  0.060493 ,  0.060493 ,  0.15441  ,  0.15441  ,\n",
       "           0.6665   ,  0.6665   ,  0.57117  ,  0.57117  ,  0.38161  ,\n",
       "           0.38161  ,  0.37302  ,  0.37302  ,  0.28516  ,  0.28516  ,\n",
       "           0.21945  ,  0.21945  ,  0.17296  ,  0.17296  ,  0.20472  ,\n",
       "           0.20472  ,  0.21497  ,  0.21497  ,  0.19057  ,  0.19057  ,\n",
       "           0.18716  ,  0.18716  ,  0.31581  ,  0.31581  ,  0.36647  ,\n",
       "           0.36647  ,  0.39506  ,  0.39506  ,  0.30871  ,  0.30871  ,\n",
       "           0.41039  ,  0.41039  ,  0.3606   ,  0.3606   ,  0.1418   ,\n",
       "           0.1418   ,  0.13914  ,  0.13914  ,  0.22276  ,  0.22276  ,\n",
       "           0.27225  ,  0.27225  ,  0.24408  ,  0.24408  ,  0.17506  ,\n",
       "           0.17506  ,  0.072852 ,  0.072852 ,  0.051264 ,  0.051264 ,\n",
       "           0.14285  ,  0.14285  ,  0.71373  ,  0.71373  ,  0.51352  ,\n",
       "           0.51352  ,  0.50611  ,  0.50611  ,  0.40024  ,  0.40024  ,\n",
       "           0.21241  ,  0.21241  ,  0.14927  ,  0.14927  ,  0.11245  ,\n",
       "           0.11245  ,  0.17597  ,  0.17597  ,  0.10735  ,  0.10735  ,\n",
       "           0.1207   ,  0.1207   ,  0.29097  ,  0.29097  ,  0.40723  ,\n",
       "           0.40723  ,  0.53582  ,  0.53582  ,  0.33953  ,  0.33953  ,\n",
       "           0.51564  ,  0.51564  ,  0.49405  ,  0.49405  ,  0.15535  ,\n",
       "           0.15535  ,  0.054372 ,  0.054372 ,  0.27296  ,  0.27296  ,\n",
       "           0.26516  ,  0.26516  ,  0.17728  ,  0.17728  ,  0.14978  ,\n",
       "           0.14978  ,  0.081124 ,  0.081124 ,  0.059407 ,  0.059407 ,\n",
       "           0.17772  ,  0.17772  ,  0.41042  ,  0.41042  ,  0.37986  ,\n",
       "           0.37986  ,  0.2326   ,  0.2326   ,  0.23302  ,  0.23302  ,\n",
       "           0.1023   ,  0.1023   ,  0.22117  ,  0.22117  ,  0.18856  ,\n",
       "           0.18856  ,  0.22485  ,  0.22485  ,  0.20126  ,  0.20126  ,\n",
       "           0.27932  ,  0.27932  ,  0.36743  ,  0.36743  ,  0.46038  ,\n",
       "           0.46038  ,  0.30677  ,  0.30677  ,  0.36399  ,  0.36399  ,\n",
       "           0.35942  ,  0.35942  ,  0.081346 ,  0.081346 ,  0.12832  ,\n",
       "           0.12832  ,  0.16343  ,  0.16343  ,  0.30423  ,  0.30423  ,\n",
       "           0.22562  ,  0.22562  ,  0.18438  ,  0.18438  ,  0.10238  ,\n",
       "           0.10238  ,  0.046463 ,  0.046463 ,  0.12617  ,  0.12617  ,\n",
       "           0.57758  ,  0.57758  ,  0.25516  ,  0.25516  ,  0.20396  ,\n",
       "           0.20396  ,  0.082235 ,  0.082235 ,  0.15964  ,  0.15964  ,\n",
       "           0.16099  ,  0.16099  ,  0.13542  ,  0.13542  ,  0.16695  ,\n",
       "           0.16695  ,  0.23593  ,  0.23593  ,  0.40179  ,  0.40179  ,\n",
       "           0.83487  ,  0.83487  ,  0.50918  ,  0.50918  ,  0.49254  ,\n",
       "           0.49254  ,  0.53114  ,  0.53114  ,  0.10013  ,  0.10013  ,\n",
       "           0.10591  ,  0.10591  ,  0.22573  ,  0.22573  ,  0.22525  ,\n",
       "           0.22525  ,  0.1804   ,  0.1804   ,  0.15122  ,  0.15122  ,\n",
       "           0.077285 ,  0.077285 ,  0.032009 ,  0.032009 ,  0.088393 ,\n",
       "           0.088393 ,  0.52482  ,  0.52482  ,  0.17916  ,  0.17916  ,\n",
       "           0.12961  ,  0.12961  ,  0.15284  ,  0.15284  ,  0.23742  ,\n",
       "           0.23742  ,  0.075633 ,  0.075633 ,  0.29751  ,  0.29751  ,\n",
       "           0.26412  ,  0.26412  ,  0.5749   ,  0.5749   ,  0.531    ,\n",
       "           0.531    ,  0.64141  ,  0.64141  ,  0.65289  ,  0.65289  ,\n",
       "           0.82631  ,  0.82631  ,  0.26751  ,  0.26751  ,  0.14327  ,\n",
       "           0.14327  ,  0.41185  ,  0.41185  ,  0.18529  ,  0.18529  ,\n",
       "           0.23687  ,  0.23687  ,  0.12537  ,  0.12537  ,  0.061412 ,\n",
       "           0.061412 ,  0.05069  ,  0.05069  ,  0.16083  ,  0.16083  ,\n",
       "           0.099214 ,  0.099214 ,  0.28009  ,  0.28009  ,  0.14229  ,\n",
       "           0.14229  ,  0.43305  ,  0.43305  ,  0.14596  ,  0.14596  ,\n",
       "           0.24134  ,  0.24134  ,  0.30743  ,  0.30743  ,  0.3538   ,\n",
       "           0.3538   ,  0.22765  ,  0.22765  ,  0.33891  ,  0.33891  ,\n",
       "           0.46497  ,  0.46497  ,  0.45604  ,  0.45604  ,  0.41889  ,\n",
       "           0.41889  ,  0.078087 ,  0.078087 ,  0.38251  ,  0.38251  ,\n",
       "           0.11138  ,  0.11138  ,  0.11192  ,  0.11192  ,  0.049193 ,\n",
       "           0.049193 ,  0.0029833,  0.0029833,  0.064237 ,  0.064237 ,\n",
       "           0.19875  ,  0.19875  ,  0.088349 ,  0.088349 ,  0.31613  ,\n",
       "           0.31613  ,  0.17825  ,  0.17825  ,  0.31984  ,  0.31984  ,\n",
       "           0.20939  ,  0.20939  ,  0.18911  ,  0.18911  ,  0.27698  ,\n",
       "           0.27698  ,  0.20825  ,  0.20825  ,  0.19395  ,  0.19395  ,\n",
       "           0.16338  ,  0.16338  ,  0.15413  ,  0.15413  ,  0.031288 ,\n",
       "           0.031288 ,  0.14008  ,  0.14008  ,  0.090333 ,  0.090333 ,\n",
       "           0.3232   ,  0.3232   ,  0.19489  ,  0.19489  ,  0.21312  ,\n",
       "           0.21312  ,  0.089073 ,  0.089073 ,  0.019102 ,  0.019102 ,\n",
       "           0.057632 ,  0.057632 ,  0.3293   ,  0.3293   ,  0.3667   ,\n",
       "           0.3667   ,  0.21927  ,  0.21927  ,  0.25507  ,  0.25507  ,\n",
       "           0.24062  ,  0.24062  ,  0.15685  ,  0.15685  ,  0.063974 ,\n",
       "           0.063974 ,  0.12795  ,  0.12795  ,  0.14111  ,  0.14111  ,\n",
       "           0.10713  ,  0.10713  ,  0.29691  ,  0.29691  ,  0.31763  ,\n",
       "           0.31763  ,  0.21632  ,  0.21632  ,  0.088533 ,  0.088533 ,\n",
       "           0.19251  ,  0.19251  ,  0.09435  ,  0.09435  , -0.0097822,\n",
       "          -0.0097822,  0.042749 ,  0.042749 ,  0.1148   ,  0.1148   ,\n",
       "           0.63295  ,  0.63295  ,  0.89729  ,  0.89729  ,  0.78426  ,\n",
       "           0.78426  ,  0.25748  ,  0.25748  ,  0.35261  ,  0.35261  ,\n",
       "           0.13637  ,  0.13637  ,  0.12896  ,  0.12896  ,  0.10413  ,\n",
       "           0.10413  ,  0.069807 ,  0.069807 ,  0.29382  ,  0.29382  ,\n",
       "           0.66414  ,  0.66414  ,  0.11436  ,  0.11436  ,  0.33009  ,\n",
       "           0.33009  ,  0.40708  ,  0.40708  ,  0.21692  ,  0.21692  ,\n",
       "           0.046517 ,  0.046517 ,  0.024411 ,  0.024411 ,  0.074275 ,\n",
       "           0.074275 ,  0.49719  ,  0.49719  ,  0.55071  ,  0.55071  ,\n",
       "           0.29181  ,  0.29181  ,  0.22733  ,  0.22733  ,  0.14105  ,\n",
       "           0.14105  ,  0.18604  ,  0.18604  ,  0.15466  ,  0.15466  ,\n",
       "           0.16411  ,  0.16411  ,  0.35593  ,  0.35593  ,  0.45398  ,\n",
       "           0.45398  ,  0.2662   ,  0.2662   ,  0.15843  ,  0.15843  ,\n",
       "           0.23967  ,  0.23967  ,  0.13648  ,  0.13648  ,  0.01001  ,\n",
       "           0.01001  ,  0.041901 ,  0.041901 ,  0.12886  ,  0.12886  ,\n",
       "           0.54538  ,  0.54538  ,  0.23193  ,  0.23193  ,  0.16253  ,\n",
       "           0.16253  ,  0.14732  ,  0.14732  ,  0.037314 ,  0.037314 ,\n",
       "           0.029983 ,  0.029983 ,  0.019508 ,  0.019508 ,  0.039895 ,\n",
       "           0.039895 ,  0.26181  ,  0.26181  , -0.036992 , -0.036992 ,\n",
       "           0.32256  ,  0.32256  ,  0.2233   ,  0.2233   ,  0.18918  ,\n",
       "           0.18918  ,  0.062411 ,  0.062411 ,  0.021565 ,  0.021565 ,\n",
       "           0.072701 ,  0.072701 ,  0.20146  ,  0.20146  ,  0.34138  ,\n",
       "           0.34138  ,  0.14711  ,  0.14711  ,  0.16088  ,  0.16088  ,\n",
       "           0.1457   ,  0.1457   ,  0.16897  ,  0.16897  ,  0.22849  ,\n",
       "           0.22849  ,  0.52415  ,  0.52415  ,  0.19247  ,  0.19247  ,\n",
       "           0.1996   ,  0.1996   ,  0.42029  ,  0.42029  ,  0.19304  ,\n",
       "           0.19304  ,  0.035691 ,  0.035691 ,  0.02462  ,  0.02462  ,\n",
       "           0.0827   ,  0.0827   ,  0.35579  ,  0.35579  ,  0.24985  ,\n",
       "           0.24985  ,  0.44664  ,  0.44664  ,  0.28575  ,  0.28575  ,\n",
       "           0.27349  ,  0.27349  ,  0.22864  ,  0.22864  ,  0.21489  ,\n",
       "           0.21489  ,  0.29219  ,  0.29219  ,  0.20292  ,  0.20292  ,\n",
       "           0.21713  ,  0.21713  ,  0.13414  ,  0.13414  ,  0.033316 ,\n",
       "           0.033316 ,  0.055339 ,  0.055339 ,  0.15933  ,  0.15933  ,\n",
       "           0.43353  ,  0.43353  ,  0.66881  ,  0.66881  ,  0.6345   ,\n",
       "           0.6345   ,  0.58515  ,  0.58515  ,  0.3033   ,  0.3033   ,\n",
       "           0.31446  ,  0.31446  ,  0.40582  ,  0.40582  ,  0.37597  ,\n",
       "           0.37597  ,  0.46799  ,  0.46799  ,  0.1874   ,  0.1874   ,\n",
       "           0.091397 ,  0.091397 ,  0.047718 ,  0.047718 ,  0.14286  ,\n",
       "           0.14286  ,  0.56453  ,  0.56453  ,  0.51335  ,  0.51335  ,\n",
       "           0.5968   ,  0.5968   ,  0.091152 ,  0.091152 ,  0.083658 ,\n",
       "           0.083658 ,  0.24034  ,  0.24034  ,  0.25253  ,  0.25253  ,\n",
       "           0.17331  ,  0.17331  ,  0.15214  ,  0.15214  ,  0.088057 ,\n",
       "           0.088057 ,  0.033493 ,  0.033493 ,  0.10485  ,  0.10485  ,\n",
       "           0.61631  ,  0.61631  ,  0.7151   ,  0.7151   ,  0.29513  ,\n",
       "           0.29513  ,  0.22922  ,  0.22922  ,  0.48951  ,  0.48951  ,\n",
       "           0.1781   ,  0.1781   ,  0.23601  ,  0.23601  ,  0.12201  ,\n",
       "           0.12201  ,  0.057065 ,  0.057065 ,  0.045402 ,  0.045402 ,\n",
       "           0.13086  ,  0.13086  ,  0.69467  ,  0.69467  ,  0.20101  ,\n",
       "           0.20101  ,  0.066433 ,  0.066433 ,  0.30269  ,  0.30269  ,\n",
       "           0.19766  ,  0.19766  ,  0.26341  ,  0.26341  ,  0.1146   ,\n",
       "           0.1146   ,  0.058149 ,  0.058149 ,  0.053463 ,  0.053463 ,\n",
       "           0.13575  ,  0.13575  ,  0.27974  ,  0.27974  ,  0.11393  ,\n",
       "           0.11393  ,  0.44104  ,  0.44104  ,  0.18226  ,  0.18226  ,\n",
       "           0.19928  ,  0.19928  ,  0.10922  ,  0.10922  ,  0.065321 ,\n",
       "           0.065321 ,  0.055716 ,  0.055716 ,  0.16755  ,  0.16755  ,\n",
       "           0.64809  ,  0.64809  ,  0.78581  ,  0.78581  ,  0.027869 ,\n",
       "           0.027869 ,  0.073261 ,  0.073261 ,  0.019392 ,  0.019392 ,\n",
       "          -0.028453 , -0.028453 ,  0.048853 ,  0.048853 ,  0.15552  ,\n",
       "           0.15552  ,  0.47912  ,  0.47912  ,  0.13938  ,  0.13938  ,\n",
       "           0.37744  ,  0.37744  ,  0.14814  ,  0.14814  ,  0.0052557,\n",
       "           0.0052557,  0.029382 ,  0.029382 ,  0.085198 ,  0.085198 ,\n",
       "           0.048122 ,  0.048122 ,  0.12057  ,  0.12057  ,  0.06033  ,\n",
       "           0.06033  ,  0.014264 ,  0.014264 ,  0.054312 ,  0.054312 ,\n",
       "           0.18358  ,  0.18358  ,  0.25977  ,  0.25977  ,  0.18118  ,\n",
       "           0.18118  ,  0.11717  ,  0.11717  ,  0.02445  ,  0.02445  ,\n",
       "           0.074665 ,  0.074665 ,  0.21137  ,  0.21137  ,  0.045916 ,\n",
       "           0.045916 ,  0.027396 ,  0.027396 ,  0.06124  ,  0.06124  ,\n",
       "           0.11414  ,  0.11414  ,  0.028379 ,  0.028379 ,  0.07981  ,\n",
       "           0.07981  ,  0.011463 ,  0.011463 ,  0.04058  ,  0.04058  ,\n",
       "           0.094907 ,  0.094907 ]),\n",
       "   'y_pred': array([ 3.43175501e-01,  1.95474252e-01,  1.97926000e-01,  1.79247111e-01,\n",
       "           2.09045589e-01,  1.71055704e-01,  2.19415322e-01,  1.56631276e-01,\n",
       "           2.08069444e-01,  1.93599403e-01,  2.55335361e-01,  1.67865708e-01,\n",
       "           1.97149724e-01,  2.62206495e-01,  1.61860943e-01,  1.83385253e-01,\n",
       "           1.41104549e-01,  1.46045893e-01,  2.76953846e-01,  3.48210961e-01,\n",
       "           1.89349800e-01,  2.14034349e-01,  6.75822943e-02,  2.35143185e-01,\n",
       "           1.53692439e-01,  2.99617857e-01,  1.08457848e-01,  1.73663557e-01,\n",
       "           2.65875399e-01,  3.06858003e-01,  1.51266515e-01,  1.85132712e-01,\n",
       "           3.15592080e-01,  2.38914981e-01,  1.70979694e-01,  2.45000139e-01,\n",
       "           1.64321288e-01,  2.59687752e-01,  7.59635419e-02,  1.71416730e-01,\n",
       "           2.66544193e-01,  2.62374550e-01,  5.69206476e-02,  1.41993597e-01,\n",
       "           1.93253949e-01,  1.54953569e-01,  1.97415695e-01,  2.25041702e-01,\n",
       "           1.80729717e-01,  1.40050590e-01,  8.96647424e-02,  7.19096661e-02,\n",
       "           6.47241920e-02,  6.47475421e-02,  7.43171871e-02,  5.06262928e-02,\n",
       "           4.06732619e-01,  3.92604172e-01,  3.37952584e-01,  4.17083383e-01,\n",
       "           3.50805730e-01,  3.74790877e-01,  3.08338404e-01,  2.96053439e-01,\n",
       "           4.70281482e-01,  3.49484742e-01,  3.36429894e-01,  4.07695532e-01,\n",
       "           2.46698216e-01,  2.32945278e-01,  1.63139075e-01,  1.82922244e-01,\n",
       "           2.62391388e-01,  2.25052699e-01,  3.01163495e-01,  2.01798171e-01,\n",
       "           1.59727693e-01,  2.37689868e-01,  1.68665737e-01,  1.35884374e-01,\n",
       "           4.46898699e-01,  5.29818296e-01,  3.72541547e-01,  4.52421606e-01,\n",
       "           2.93568373e-01,  2.60687530e-01,  4.12715554e-01,  4.81233656e-01,\n",
       "           4.37044859e-01,  3.84808540e-01,  3.20791930e-01,  4.17080045e-01,\n",
       "           1.80439442e-01,  2.44613886e-01,  1.35828182e-01,  1.94870472e-01,\n",
       "           3.24956089e-01,  3.04445088e-01,  3.23675513e-01,  2.47024342e-01,\n",
       "           2.85255998e-01,  2.88176477e-01,  1.18879989e-01,  2.04352885e-01,\n",
       "           8.58969241e-02,  7.95943737e-02,  8.19665343e-02,  8.98880512e-02,\n",
       "           1.82523578e-01,  1.10296831e-01,  5.75504899e-01,  6.17861509e-01,\n",
       "           4.93342042e-01,  4.84222710e-01,  4.06016350e-01,  4.39964175e-01,\n",
       "           4.17700320e-01,  4.73028898e-01,  3.51482302e-01,  2.41369516e-01,\n",
       "           1.51226625e-01,  2.24235252e-01,  1.64888099e-01,  1.68469042e-01,\n",
       "           2.05282152e-01,  2.73819417e-02,  2.49038696e-01,  1.98203713e-01,\n",
       "           1.57744259e-01,  2.00472668e-01,  2.00517491e-01,  1.18174091e-01,\n",
       "           3.79362345e-01,  3.26935887e-01,  4.52198088e-01,  4.26174581e-01,\n",
       "           4.29867685e-01,  4.33187366e-01,  3.50307047e-01,  3.55961263e-01,\n",
       "           3.91887963e-01,  4.88030434e-01,  4.51460958e-01,  4.87925291e-01,\n",
       "           1.79590911e-01,  1.91950068e-01,  8.00581723e-02, -2.91854143e-04,\n",
       "           1.90409839e-01,  3.35397273e-01,  1.96768016e-01,  1.86423808e-01,\n",
       "           1.51076466e-01,  2.13250682e-01,  1.36491492e-01,  1.02249160e-01,\n",
       "           8.26521665e-02,  6.78191781e-02,  5.69384843e-02,  6.93280995e-02,\n",
       "           1.15499958e-01,  2.11290389e-01,  6.75346017e-01,  5.27981639e-01,\n",
       "           5.36111712e-01,  4.80977654e-01,  4.69553292e-01,  5.86968064e-01,\n",
       "           3.37549627e-01,  3.73276144e-01,  1.72201902e-01,  2.07899272e-01,\n",
       "           1.71061367e-01,  2.01543316e-01,  1.98090047e-01,  1.16232723e-01,\n",
       "           1.85262725e-01,  1.99081898e-01,  7.36630708e-02,  1.53478622e-01,\n",
       "           1.92226708e-01,  1.36802644e-01,  3.02325249e-01,  3.40153039e-01,\n",
       "           4.63431478e-01,  3.94674212e-01,  5.49768269e-01,  4.94906425e-01,\n",
       "           4.25581694e-01,  3.82785559e-01,  5.45607328e-01,  6.35204673e-01,\n",
       "           4.80554163e-01,  6.08889461e-01,  2.02328697e-01,  3.13826352e-01,\n",
       "           1.39351726e-01,  5.83393723e-02,  2.41449282e-01,  4.97342825e-01,\n",
       "           2.47165754e-01,  3.37256879e-01,  1.78945035e-01,  1.68028325e-01,\n",
       "           1.54012054e-01,  1.11955866e-01,  8.72666687e-02,  9.94767547e-02,\n",
       "           8.09322000e-02,  6.77919984e-02,  1.26336098e-01,  1.81309760e-01,\n",
       "           6.61261916e-01,  5.11330962e-01,  5.10526657e-01,  5.36877275e-01,\n",
       "           2.20560610e-01,  3.20554495e-01,  2.24844113e-01,  2.22084701e-01,\n",
       "           1.54280335e-01,  1.37708277e-01,  2.78568715e-01,  2.18467638e-01,\n",
       "           1.75257236e-01,  1.60877466e-01,  3.75768840e-02,  1.40269145e-01,\n",
       "           1.69209182e-01,  2.45883733e-01,  2.88758814e-01,  3.39193165e-01,\n",
       "           4.02411789e-01,  3.71252924e-01,  7.36650705e-01,  4.77833033e-01,\n",
       "           3.67947876e-01,  4.06790406e-01,  4.27785069e-01,  5.00855088e-01,\n",
       "           4.86702442e-01,  4.74505603e-01, -3.41210961e-02,  1.85235947e-01,\n",
       "           1.73159331e-01,  1.16747916e-01,  2.56584853e-01,  3.09547365e-01,\n",
       "           2.99574316e-01,  2.83611804e-01,  2.60679007e-01,  1.34182051e-01,\n",
       "           1.61431476e-01,  1.38674915e-01,  1.11206070e-01,  1.03206292e-01,\n",
       "           5.89596778e-02,  7.64565021e-02,  9.50311869e-02,  1.94570065e-01,\n",
       "           4.86204475e-01,  5.21935940e-01,  2.68667310e-01,  2.62510687e-01,\n",
       "           1.73318088e-01,  2.22276449e-01,  1.81697369e-01,  1.44814342e-01,\n",
       "           2.03825772e-01,  3.44290942e-01,  8.29827785e-02,  1.78237036e-01,\n",
       "           2.19700783e-01,  3.05153280e-02,  1.65516242e-01,  1.86107039e-01,\n",
       "           2.75832474e-01,  2.48175979e-01,  4.52831000e-01,  3.89994740e-01,\n",
       "           5.97065628e-01,  5.20112157e-01,  4.94490266e-01,  5.42007625e-01,\n",
       "           3.96910906e-01,  3.96591812e-01,  4.17624950e-01,  5.08092165e-01,\n",
       "           2.39159510e-01,  3.33095372e-01,  2.40889773e-01,  1.08412951e-01,\n",
       "           1.52686745e-01,  2.51615822e-01,  1.87204868e-01,  2.56835878e-01,\n",
       "           1.67126447e-01,  2.00195372e-01,  9.16053057e-02,  1.21157721e-01,\n",
       "           4.93519455e-02,  1.11682743e-01,  4.63181436e-02,  4.95142043e-02,\n",
       "           1.67816132e-01,  1.01525724e-01,  3.68710697e-01,  4.29316998e-01,\n",
       "           1.64470866e-01,  1.84528589e-01,  2.10407525e-01,  2.10836336e-01,\n",
       "           3.77118975e-01,  1.52749404e-01,  2.40468845e-01,  2.81345159e-01,\n",
       "           2.44509041e-01,  1.12412423e-01,  3.57678205e-01,  3.29786718e-01,\n",
       "           3.60614300e-01,  3.96340847e-01,  5.04570603e-01,  5.42532086e-01,\n",
       "           5.21181464e-01,  5.23391128e-01,  5.23047686e-01,  5.74129701e-01,\n",
       "           4.16268229e-01,  4.58402276e-01,  5.89264870e-01,  5.18818974e-01,\n",
       "           4.02012616e-01,  3.17032218e-01,  1.72693431e-01,  2.40809768e-01,\n",
       "           3.92208576e-01,  4.06892478e-01,  1.76866829e-01,  1.16380677e-01,\n",
       "           1.47921354e-01,  1.47451848e-01,  7.98515081e-02,  1.49366498e-01,\n",
       "           5.21667600e-02,  2.08580792e-02,  8.57366920e-02,  1.92856342e-02,\n",
       "           1.69220865e-01,  1.59531608e-01,  1.32128164e-01,  4.26921248e-03,\n",
       "           1.62390843e-01,  3.03826332e-01,  3.09816509e-01,  2.19880760e-01,\n",
       "           3.78697038e-01,  3.29722822e-01,  7.03954399e-02,  1.86898232e-01,\n",
       "           3.09303284e-01,  2.58122623e-01,  3.29220831e-01,  3.39986026e-01,\n",
       "           3.23802948e-01,  4.48045671e-01,  2.25180149e-01,  2.20670521e-01,\n",
       "           3.11774582e-01,  3.68513346e-01,  3.96975756e-01,  4.69491065e-01,\n",
       "           3.86491179e-01,  3.84126246e-01,  3.66671056e-01,  5.78932881e-01,\n",
       "           2.13953555e-01,  1.37589872e-01,  5.21253943e-01,  2.93580830e-01,\n",
       "           1.77659541e-01,  1.35070026e-01,  2.45160878e-01,  1.32485017e-01,\n",
       "           1.14329636e-01,  8.79734159e-02,  4.10143286e-02,  6.48191869e-02,\n",
       "           4.32791561e-02,  6.97389245e-02,  1.48760080e-01,  1.47180364e-01,\n",
       "           1.38121724e-01,  1.10661656e-01,  3.18395913e-01,  2.26345971e-01,\n",
       "           2.48152539e-01,  2.29950368e-01,  1.49921209e-01,  1.89318970e-01,\n",
       "           1.52727246e-01,  1.39999300e-01,  2.55732983e-01,  2.40776211e-01,\n",
       "           2.45223835e-01,  2.69803673e-01,  2.54549831e-01,  2.54380554e-01,\n",
       "           2.38733947e-01,  1.96945503e-01,  1.46169186e-01,  2.22575441e-01,\n",
       "           1.29375726e-01,  2.44212493e-01,  9.69561189e-02,  1.21952549e-01,\n",
       "           2.23824024e-01,  1.58167228e-01,  4.89791781e-02,  1.06054842e-02,\n",
       "           2.45391235e-01,  2.21823514e-01,  2.10508615e-01,  1.98763967e-01,\n",
       "           1.55549690e-01,  1.47975862e-01,  5.80824614e-02,  2.23718137e-02,\n",
       "           2.95466036e-02, -1.89825892e-03,  9.44732726e-02,  5.64855188e-02,\n",
       "           2.27071643e-01,  1.93346128e-01,  2.30563954e-01,  2.96915859e-01,\n",
       "           2.14318231e-01,  2.20777750e-01,  2.96563834e-01,  2.32292771e-01,\n",
       "           1.70503497e-01,  1.58352196e-01,  4.47297096e-01,  3.11596751e-01,\n",
       "           3.36907953e-02,  7.54082352e-02,  3.14514101e-01,  2.27895185e-01,\n",
       "           2.96173155e-01,  2.13451087e-01,  2.00880066e-01,  1.84744179e-01,\n",
       "           8.97070616e-02,  8.88164788e-02,  2.73863882e-01,  2.74863064e-01,\n",
       "           2.23727837e-01,  1.53245717e-01,  1.18526682e-01,  7.33926296e-02,\n",
       "           2.80970901e-01,  2.26530597e-01,  1.78565234e-01,  1.20442197e-01,\n",
       "           1.33280754e-02, -2.31307149e-02, -2.65355706e-02, -1.90803707e-02,\n",
       "           8.85730982e-02,  3.90127897e-02,  3.04557204e-01,  3.19132775e-01,\n",
       "           3.85516673e-01,  7.36855626e-01,  2.58568168e-01,  6.72711432e-01,\n",
       "           2.12697715e-01,  1.35622323e-01,  3.82515490e-01,  3.06464612e-01,\n",
       "           1.22719854e-01,  8.88766497e-02,  3.10394228e-01,  4.70435202e-01,\n",
       "           4.32006389e-01,  1.73532188e-01,  1.68222755e-01,  6.52442873e-03,\n",
       "           6.32306337e-02,  9.59563851e-02,  3.09779912e-01,  4.62637305e-01,\n",
       "           3.20588320e-01,  4.15171891e-01,  2.74046421e-01,  2.81114042e-01,\n",
       "           4.00646687e-01,  2.01587439e-01,  6.00103885e-02,  2.15491310e-01,\n",
       "           5.41904122e-02,  9.00012553e-02,  4.31518108e-02,  6.19270951e-02,\n",
       "           2.88176388e-02,  1.91840082e-01,  4.45190132e-01,  4.37889636e-01,\n",
       "           3.28796446e-01,  3.98684919e-01,  5.47770858e-01,  2.72097707e-01,\n",
       "           2.08707646e-01,  2.52989322e-01,  2.07183942e-01,  1.91449344e-01,\n",
       "           3.61307383e-01,  2.10649014e-01,  2.11678758e-01,  2.04575613e-01,\n",
       "           2.25729883e-01,  1.27949476e-01,  3.69361877e-01,  3.01437557e-01,\n",
       "           2.46816665e-01,  3.32958996e-01,  3.09722543e-01,  3.73172522e-01,\n",
       "           2.67506629e-01,  1.62068292e-01,  2.50308454e-01,  2.73394167e-01,\n",
       "           2.00194687e-01,  1.38845921e-01,  6.18638098e-03,  7.72069395e-03,\n",
       "           2.33645290e-02,  2.82363594e-02,  1.15585148e-01,  9.08430517e-02,\n",
       "           4.96203423e-01,  5.19923806e-01,  2.28143141e-01,  1.50312901e-01,\n",
       "           1.50525689e-01,  2.78253317e-01,  1.05982840e-01,  1.13841042e-01,\n",
       "           1.93141147e-01,  2.17226833e-01,  1.02752879e-01,  4.39629406e-02,\n",
       "           5.06683290e-02,  9.42832381e-02,  2.62398511e-01,  2.30476797e-01,\n",
       "           3.78875315e-01,  4.39620674e-01,  2.85857230e-01,  2.34367162e-01,\n",
       "           1.54536754e-01,  1.66832983e-01,  3.18228781e-01,  1.75330073e-01,\n",
       "           8.38623941e-02,  1.37018368e-01,  5.31076789e-02,  7.68570602e-03,\n",
       "          -1.76455677e-02, -5.84164262e-02,  1.22820318e-01,  2.41974860e-01,\n",
       "           3.24657083e-01,  3.08139026e-01,  3.20166290e-01,  2.89212406e-01,\n",
       "           1.98391840e-01,  2.15308249e-01,  1.75309703e-01,  1.73060358e-01,\n",
       "           2.55484700e-01,  1.85502738e-01,  1.55160099e-01,  2.21227303e-01,\n",
       "           2.03211695e-01,  3.82768869e-01,  3.40623200e-01,  3.66608083e-01,\n",
       "           4.55655783e-01,  2.33637318e-01,  1.60626292e-01,  4.46646661e-02,\n",
       "           4.05160904e-01,  2.95953453e-01,  1.49914637e-01,  1.46670431e-01,\n",
       "           4.90678698e-02,  1.75201893e-02,  1.25195235e-02, -6.37206435e-03,\n",
       "           6.76339567e-02,  1.94009453e-01,  4.17218983e-01,  4.57720518e-01,\n",
       "           3.53928864e-01,  2.73443162e-01,  5.38242698e-01,  5.20985961e-01,\n",
       "           4.60874021e-01,  3.04284632e-01,  3.75811577e-01,  3.27020675e-01,\n",
       "           1.24142751e-01,  1.22732401e-01,  1.93182230e-01,  1.05126798e-01,\n",
       "           2.05809861e-01,  2.34665304e-01,  5.19745648e-01,  4.09696519e-01,\n",
       "           2.61228442e-01,  1.87173009e-01,  8.66535157e-02,  1.04676083e-01,\n",
       "           7.31366724e-02,  1.05425224e-01,  8.65889639e-02,  1.96161717e-02,\n",
       "           1.48021311e-01,  1.04630902e-01,  4.41432118e-01,  4.58827287e-01,\n",
       "           6.85183942e-01,  5.56105137e-01,  5.40259659e-01,  5.39598823e-01,\n",
       "           4.24870133e-01,  5.58976531e-01,  4.19140100e-01,  5.02099454e-01,\n",
       "           6.14590168e-01,  4.84624624e-01,  4.27037001e-01,  3.39994431e-01,\n",
       "           3.13918859e-01,  2.96178281e-01,  3.25564742e-01,  2.91078806e-01,\n",
       "           2.11346313e-01,  1.75676659e-01,  8.87370706e-02,  4.15356755e-02,\n",
       "           7.55783617e-02,  6.13923669e-02,  1.87594175e-01,  1.66684091e-01,\n",
       "           5.33788085e-01,  5.62357068e-01,  4.77991253e-01,  4.82746422e-01,\n",
       "           4.20822591e-01,  4.88377571e-01,  2.69767046e-01,  2.92137980e-01,\n",
       "           1.81305513e-01,  1.83803827e-01,  1.89264923e-01,  2.86391377e-01,\n",
       "           2.38722056e-01,  1.66244119e-01,  1.97537258e-01,  1.81498379e-01,\n",
       "           9.86960977e-02,  1.44808769e-01,  9.82921869e-02,  9.62825716e-02,\n",
       "           6.67338818e-02,  3.14288586e-02,  1.31918639e-01,  1.21729746e-01,\n",
       "           5.65615058e-01,  6.01597071e-01,  4.76586342e-01,  4.09899890e-01,\n",
       "           3.00197423e-01,  2.29677185e-01,  2.35193565e-01,  3.74282271e-01,\n",
       "           3.90785098e-01,  2.15166479e-01,  3.51939499e-01,  2.75377482e-01,\n",
       "           2.79351443e-01,  2.86763400e-01,  2.20419049e-01,  1.22784182e-01,\n",
       "           6.92621320e-02,  6.26587123e-02,  5.36803901e-02,  2.57580727e-02,\n",
       "           1.13910317e-01,  1.34288222e-01,  5.81108689e-01,  4.77964163e-01,\n",
       "           3.44059467e-01,  2.93955535e-01,  1.78372249e-01,  2.14822248e-01,\n",
       "           3.80831480e-01,  2.47194305e-01,  2.13449195e-01,  2.77512342e-01,\n",
       "           2.07113728e-01,  2.30007589e-01,  1.71737820e-01,  1.42536342e-01,\n",
       "           7.50379413e-02,  2.70579606e-02,  7.58374631e-02,  7.49944150e-03,\n",
       "           1.38668120e-01,  1.57147199e-01,  1.34692028e-01,  3.97798955e-01,\n",
       "           8.87935162e-02,  2.33687013e-01,  3.78210127e-01,  2.88501590e-01,\n",
       "           2.79303789e-01,  3.24899167e-01,  8.83264691e-02,  2.43837833e-01,\n",
       "           1.92718208e-01,  1.45221621e-01,  4.70895022e-02,  6.04136735e-02,\n",
       "           5.26944548e-02,  5.17719984e-02,  1.49349183e-01,  1.35445759e-01,\n",
       "           2.43541881e-01,  2.44349599e-01,  6.72652602e-01,  5.60540318e-01,\n",
       "           9.28669870e-02,  1.51455998e-01,  2.12147236e-01,  3.18004310e-01,\n",
       "           1.15728110e-01,  6.24694228e-02, -2.79259086e-02,  4.53871787e-02,\n",
       "           4.86844480e-02, -1.44838095e-02,  6.49867803e-02,  1.69556439e-02,\n",
       "           5.29711843e-01,  2.25609213e-01,  2.75417089e-01,  1.33612901e-01,\n",
       "           4.38133061e-01,  2.50691056e-01,  2.69539982e-01,  2.74239898e-01,\n",
       "           1.13500491e-01,  7.04343170e-02,  8.77398700e-02,  2.79357880e-02,\n",
       "           1.84718698e-01,  2.85807252e-03,  1.55962795e-01,  2.93540001e-01,\n",
       "           1.80641949e-01,  1.83346257e-01,  1.01597518e-01,  1.34017169e-02,\n",
       "           5.91365993e-02,  2.79990286e-02,  8.52522552e-02,  8.31020027e-02,\n",
       "           1.39065057e-01,  1.06832862e-01,  1.87953234e-01,  2.53589392e-01,\n",
       "           1.58689529e-01,  1.76265866e-01,  7.33883381e-02,  7.67436624e-02,\n",
       "           4.83393669e-04,  5.15229702e-02,  1.15560591e-01,  6.01168424e-02,\n",
       "           1.96624607e-01,  2.44146913e-01,  5.62140495e-02,  1.02144659e-01,\n",
       "           1.57823116e-02,  9.45154130e-02,  5.97468168e-02,  7.79940337e-02,\n",
       "           5.88620007e-02,  8.09832662e-02,  1.45483464e-02,  7.27202296e-02,\n",
       "           6.35894090e-02,  8.38258713e-02,  7.12830722e-02,  1.16734356e-02,\n",
       "           2.98552364e-02,  4.20675725e-02,  2.95137912e-02,  8.15089196e-02],\n",
       "         dtype=float32),\n",
       "   'feature_importances': array([0.00697603, 0.01114407, 0.03372398, 0.03589126, 0.06205141,\n",
       "          0.01209585, 0.01869309, 0.04936665, 0.0146551 , 0.02460275,\n",
       "          0.01791997, 0.01286456, 0.02013133, 0.01231314, 0.01698987,\n",
       "          0.03860422, 0.01777418, 0.02925199, 0.02353217, 0.01943179,\n",
       "          0.01428171, 0.01110111, 0.03226722, 0.04268188, 0.02992044,\n",
       "          0.01264487, 0.01380425, 0.06275222, 0.02036194, 0.04350464,\n",
       "          0.01423635, 0.01388983, 0.02082376, 0.01603049, 0.02202212,\n",
       "          0.0363067 , 0.03156768, 0.03285113, 0.02212922, 0.02880904],\n",
       "         dtype=float32),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 0.0006355905673706079,\n",
       "    'mae': 0.019500724921117082,\n",
       "    'r2': 0.9806787414155302,\n",
       "    'pearson_corr': 0.9905485842217123},\n",
       "   'best_val_score': 0.7758276289206564,\n",
       "   'test_metrics': {'mse': 0.007440853559069646,\n",
       "    'mae': 0.057219163406732894,\n",
       "    'r2': 0.8035136694948986,\n",
       "    'pearson_corr': 0.8972587856334614,\n",
       "    'connectome_corr': 0.7958709725995459,\n",
       "    'connectome_r2': 0.2795932707747635,\n",
       "    'geodesic_distance': 5.071492299022182},\n",
       "   'y_true': array([ 0.71922  ,  0.71922  ,  0.48128  ,  0.48128  ,  0.43659  ,\n",
       "           0.43659  ,  0.32399  ,  0.32399  ,  0.44948  ,  0.44948  ,\n",
       "           0.33167  ,  0.33167  ,  0.43798  ,  0.43798  ,  0.50016  ,\n",
       "           0.50016  ,  0.1345   ,  0.1345   ,  0.26447  ,  0.26447  ,\n",
       "           0.29921  ,  0.29921  ,  0.30794  ,  0.30794  ,  0.3946   ,\n",
       "           0.3946   ,  0.19136  ,  0.19136  ,  0.23954  ,  0.23954  ,\n",
       "           0.92977  ,  0.92977  ,  0.59727  ,  0.59727  ,  0.49227  ,\n",
       "           0.49227  ,  0.42251  ,  0.42251  ,  0.39702  ,  0.39702  ,\n",
       "           0.32595  ,  0.32595  ,  0.12236  ,  0.12236  ,  0.21177  ,\n",
       "           0.21177  ,  0.16332  ,  0.16332  ,  0.2177   ,  0.2177   ,\n",
       "           0.051711 ,  0.051711 ,  0.045002 ,  0.045002 ,  0.027955 ,\n",
       "           0.027955 ,  0.4555   ,  0.4555   ,  0.70545  ,  0.70545  ,\n",
       "           0.35772  ,  0.35772  ,  0.40483  ,  0.40483  ,  0.31522  ,\n",
       "           0.31522  ,  0.3479   ,  0.3479   ,  0.47006  ,  0.47006  ,\n",
       "           0.11232  ,  0.11232  ,  0.21738  ,  0.21738  ,  0.2367   ,\n",
       "           0.2367   ,  0.19925  ,  0.19925  ,  0.42237  ,  0.42237  ,\n",
       "           0.15409  ,  0.15409  ,  0.12644  ,  0.12644  ,  0.76273  ,\n",
       "           0.76273  ,  1.4159   ,  1.4159   ,  0.53017  ,  0.53017  ,\n",
       "           0.48218  ,  0.48218  ,  0.39509  ,  0.39509  ,  0.46872  ,\n",
       "           0.46872  ,  0.11871  ,  0.11871  ,  0.1184   ,  0.1184   ,\n",
       "           0.088568 ,  0.088568 ,  0.10206  ,  0.10206  ,  0.068446 ,\n",
       "           0.068446 ,  0.033191 ,  0.033191 ,  0.02063  ,  0.02063  ,\n",
       "           0.4428   ,  0.4428   ,  0.41565  ,  0.41565  ,  0.34578  ,\n",
       "           0.34578  ,  0.39808  ,  0.39808  ,  0.44063  ,  0.44063  ,\n",
       "           0.46516  ,  0.46516  ,  0.15989  ,  0.15989  ,  0.26702  ,\n",
       "           0.26702  ,  0.33929  ,  0.33929  ,  0.21536  ,  0.21536  ,\n",
       "           0.56331  ,  0.56331  ,  0.25194  ,  0.25194  ,  0.25443  ,\n",
       "           0.25443  ,  0.49248  ,  0.49248  ,  0.39745  ,  0.39745  ,\n",
       "           0.54343  ,  0.54343  ,  0.4951   ,  0.4951   ,  0.54511  ,\n",
       "           0.54511  ,  0.32239  ,  0.32239  ,  0.14092  ,  0.14092  ,\n",
       "           0.23301  ,  0.23301  ,  0.21211  ,  0.21211  ,  0.23812  ,\n",
       "           0.23812  ,  0.096152 ,  0.096152 ,  0.046146 ,  0.046146 ,\n",
       "           0.034861 ,  0.034861 ,  0.61068  ,  0.61068  ,  0.48006  ,\n",
       "           0.48006  ,  0.34539  ,  0.34539  ,  0.37022  ,  0.37022  ,\n",
       "           0.43956  ,  0.43956  ,  0.17585  ,  0.17585  ,  0.49562  ,\n",
       "           0.49562  ,  0.50252  ,  0.50252  ,  0.18569  ,  0.18569  ,\n",
       "           0.54201  ,  0.54201  ,  0.2935   ,  0.2935   ,  0.13164  ,\n",
       "           0.13164  ,  0.45332  ,  0.45332  ,  0.7205   ,  0.7205   ,\n",
       "           0.38752  ,  0.38752  ,  0.43245  ,  0.43245  ,  0.35889  ,\n",
       "           0.35889  ,  0.67894  ,  0.67894  ,  0.1627   ,  0.1627   ,\n",
       "           0.20474  ,  0.20474  ,  0.23652  ,  0.23652  ,  0.10442  ,\n",
       "           0.10442  ,  0.09912  ,  0.09912  ,  0.036825 ,  0.036825 ,\n",
       "           0.020006 ,  0.020006 ,  0.45635  ,  0.45635  ,  0.45603  ,\n",
       "           0.45603  ,  0.38549  ,  0.38549  ,  0.44073  ,  0.44073  ,\n",
       "           0.1256   ,  0.1256   ,  0.40294  ,  0.40294  ,  0.42535  ,\n",
       "           0.42535  ,  0.1231   ,  0.1231   ,  0.41124  ,  0.41124  ,\n",
       "           0.17447  ,  0.17447  ,  0.044144 ,  0.044144 ,  0.34047  ,\n",
       "           0.34047  ,  0.32739  ,  0.32739  ,  0.37778  ,  0.37778  ,\n",
       "           0.49065  ,  0.49065  ,  0.36448  ,  0.36448  ,  0.58298  ,\n",
       "           0.58298  ,  0.11132  ,  0.11132  ,  0.10034  ,  0.10034  ,\n",
       "           0.073646 ,  0.073646 ,  0.013839 ,  0.013839 ,  0.076694 ,\n",
       "           0.076694 ,  0.037086 ,  0.037086 ,  0.0091754,  0.0091754,\n",
       "           0.34648  ,  0.34648  ,  0.43885  ,  0.43885  ,  0.63528  ,\n",
       "           0.63528  ,  0.1177   ,  0.1177   ,  0.44497  ,  0.44497  ,\n",
       "           0.42537  ,  0.42537  ,  0.4355   ,  0.4355   ,  0.39446  ,\n",
       "           0.39446  ,  0.095204 ,  0.095204 ,  0.14335  ,  0.14335  ,\n",
       "           0.43289  ,  0.43289  ,  0.36495  ,  0.36495  ,  0.2908   ,\n",
       "           0.2908   ,  0.36472  ,  0.36472  ,  0.28198  ,  0.28198  ,\n",
       "           0.47213  ,  0.47213  ,  0.10033  ,  0.10033  ,  0.24096  ,\n",
       "           0.24096  ,  0.15704  ,  0.15704  ,  0.14838  ,  0.14838  ,\n",
       "           0.039112 ,  0.039112 ,  0.039563 ,  0.039563 ,  0.023165 ,\n",
       "           0.023165 ,  0.55964  ,  0.55964  ,  0.4745   ,  0.4745   ,\n",
       "           0.12526  ,  0.12526  ,  0.30224  ,  0.30224  ,  0.42563  ,\n",
       "           0.42563  ,  0.22515  ,  0.22515  ,  0.38099  ,  0.38099  ,\n",
       "           0.20657  ,  0.20657  ,  0.032886 ,  0.032886 ,  0.3238   ,\n",
       "           0.3238   ,  0.27486  ,  0.27486  ,  0.3946   ,  0.3946   ,\n",
       "           0.29752  ,  0.29752  ,  0.26246  ,  0.26246  ,  0.36549  ,\n",
       "           0.36549  ,  0.08782  ,  0.08782  ,  0.1119   ,  0.1119   ,\n",
       "          -0.019675 , -0.019675 , -0.0045123, -0.0045123,  0.04498  ,\n",
       "           0.04498  ,  0.055842 ,  0.055842 ,  0.022106 ,  0.022106 ,\n",
       "           0.53023  ,  0.53023  ,  0.16353  ,  0.16353  ,  0.37731  ,\n",
       "           0.37731  ,  0.46037  ,  0.46037  ,  0.35026  ,  0.35026  ,\n",
       "           0.38297  ,  0.38297  ,  0.25752  ,  0.25752  ,  0.23182  ,\n",
       "           0.23182  ,  0.40406  ,  0.40406  ,  0.31176  ,  0.31176  ,\n",
       "           0.40604  ,  0.40604  ,  0.34375  ,  0.34375  ,  0.34606  ,\n",
       "           0.34606  ,  0.35101  ,  0.35101  ,  0.109    ,  0.109    ,\n",
       "           0.20861  ,  0.20861  ,  0.10234  ,  0.10234  ,  0.16967  ,\n",
       "           0.16967  ,  0.044379 ,  0.044379 ,  0.06157  ,  0.06157  ,\n",
       "           0.040003 ,  0.040003 ,  0.12983  ,  0.12983  ,  0.29599  ,\n",
       "           0.29599  ,  0.34645  ,  0.34645  ,  0.36782  ,  0.36782  ,\n",
       "           0.44099  ,  0.44099  ,  0.13153  ,  0.13153  ,  0.1545   ,\n",
       "           0.1545   ,  0.50111  ,  0.50111  ,  0.41965  ,  0.41965  ,\n",
       "           0.45112  ,  0.45112  ,  0.44269  ,  0.44269  ,  0.39682  ,\n",
       "           0.39682  ,  0.38445  ,  0.38445  ,  0.11951  ,  0.11951  ,\n",
       "           0.16574  ,  0.16574  ,  0.079648 ,  0.079648 ,  0.14578  ,\n",
       "           0.14578  ,  0.054895 ,  0.054895 ,  0.049641 ,  0.049641 ,\n",
       "           0.028757 ,  0.028757 ,  0.21889  ,  0.21889  ,  0.25943  ,\n",
       "           0.25943  ,  0.14335  ,  0.14335  ,  0.14767  ,  0.14767  ,\n",
       "           0.25599  ,  0.25599  ,  0.21192  ,  0.21192  ,  0.11635  ,\n",
       "           0.11635  ,  0.1081   ,  0.1081   ,  0.10367  ,  0.10367  ,\n",
       "           0.10679  ,  0.10679  ,  0.11532  ,  0.11532  ,  0.12951  ,\n",
       "           0.12951  ,  0.11851  ,  0.11851  ,  0.14697  ,  0.14697  ,\n",
       "           0.18269  ,  0.18269  ,  0.15903  ,  0.15903  ,  0.043178 ,\n",
       "           0.043178 ,  0.024286 ,  0.024286 ,  0.020426 ,  0.020426 ,\n",
       "           0.96592  ,  0.96592  ,  0.49741  ,  0.49741  ,  0.33026  ,\n",
       "           0.33026  ,  0.38577  ,  0.38577  ,  0.027429 ,  0.027429 ,\n",
       "           0.23037  ,  0.23037  ,  0.19739  ,  0.19739  ,  0.16974  ,\n",
       "           0.16974  ,  0.19702  ,  0.19702  ,  0.17137  ,  0.17137  ,\n",
       "           0.71622  ,  0.71622  ,  0.13794  ,  0.13794  ,  0.50895  ,\n",
       "           0.50895  ,  0.434    ,  0.434    ,  0.0098301,  0.0098301,\n",
       "           0.017404 ,  0.017404 ,  0.039726 ,  0.039726 ,  0.01181  ,\n",
       "           0.01181  ,  0.35224  ,  0.35224  ,  0.40969  ,  0.40969  ,\n",
       "           0.56554  ,  0.56554  ,  0.086979 ,  0.086979 ,  0.26136  ,\n",
       "           0.26136  ,  0.21452  ,  0.21452  ,  0.23154  ,  0.23154  ,\n",
       "           0.22368  ,  0.22368  ,  0.21647  ,  0.21647  ,  0.53709  ,\n",
       "           0.53709  ,  0.14928  ,  0.14928  ,  0.42003  ,  0.42003  ,\n",
       "           0.31317  ,  0.31317  ,  0.02881  ,  0.02881  ,  0.046027 ,\n",
       "           0.046027 ,  0.048635 ,  0.048635 ,  0.016227 ,  0.016227 ,\n",
       "           0.21579  ,  0.21579  ,  0.086568 ,  0.086568 ,  0.22599  ,\n",
       "           0.22599  ,  0.2586   ,  0.2586   ,  0.17552  ,  0.17552  ,\n",
       "           0.13738  ,  0.13738  ,  0.13292  ,  0.13292  ,  0.091804 ,\n",
       "           0.091804 ,  0.2714   ,  0.2714   ,  0.084917 ,  0.084917 ,\n",
       "           0.37345  ,  0.37345  ,  0.22315  ,  0.22315  ,  0.24222  ,\n",
       "           0.24222  , -0.021994 , -0.021994 ,  0.04059  ,  0.04059  ,\n",
       "           0.036565 ,  0.036565 ,  0.4397   ,  0.4397   ,  0.16518  ,\n",
       "           0.16518  ,  0.4114   ,  0.4114   ,  0.3826   ,  0.3826   ,\n",
       "           0.41368  ,  0.41368  ,  0.39789  ,  0.39789  ,  0.38326  ,\n",
       "           0.38326  ,  0.33313  ,  0.33313  ,  0.14608  ,  0.14608  ,\n",
       "           0.2563   ,  0.2563   ,  0.19318  ,  0.19318  ,  0.14108  ,\n",
       "           0.14108  ,  0.10198  ,  0.10198  ,  0.040245 ,  0.040245 ,\n",
       "           0.015939 ,  0.015939 ,  0.24942  ,  0.24942  ,  0.16531  ,\n",
       "           0.16531  ,  0.1696   ,  0.1696   ,  0.1745   ,  0.1745   ,\n",
       "           0.16606  ,  0.16606  ,  0.21919  ,  0.21919  ,  0.15928  ,\n",
       "           0.15928  ,  0.15385  ,  0.15385  ,  0.36641  ,  0.36641  ,\n",
       "           0.35634  ,  0.35634  ,  0.15194  ,  0.15194  ,  0.07519  ,\n",
       "           0.07519  ,  0.039233 ,  0.039233 ,  0.0028819,  0.0028819,\n",
       "           0.19999  ,  0.19999  ,  0.13245  ,  0.13245  ,  0.13954  ,\n",
       "           0.13954  ,  0.17444  ,  0.17444  ,  0.2211   ,  0.2211   ,\n",
       "          -0.040998 , -0.040998 ,  0.14267  ,  0.14267  ,  0.15903  ,\n",
       "           0.15903  ,  0.29571  ,  0.29571  ,  0.71477  ,  0.71477  ,\n",
       "           0.067065 ,  0.067065 ,  0.028937 ,  0.028937 ,  0.058216 ,\n",
       "           0.058216 ,  0.65175  ,  0.65175  ,  0.53779  ,  0.53779  ,\n",
       "           0.47837  ,  0.47837  ,  0.44458  ,  0.44458  ,  0.33217  ,\n",
       "           0.33217  ,  0.11667  ,  0.11667  ,  0.16692  ,  0.16692  ,\n",
       "           0.13881  ,  0.13881  ,  0.18278  ,  0.18278  ,  0.064966 ,\n",
       "           0.064966 ,  0.03759  ,  0.03759  ,  0.021881 ,  0.021881 ,\n",
       "           0.4799   ,  0.4799   ,  0.44247  ,  0.44247  ,  0.35893  ,\n",
       "           0.35893  ,  0.49644  ,  0.49644  ,  0.11726  ,  0.11726  ,\n",
       "           0.10522  ,  0.10522  ,  0.095924 ,  0.095924 ,  0.1036   ,\n",
       "           0.1036   ,  0.066691 ,  0.066691 ,  0.029888 ,  0.029888 ,\n",
       "           0.018513 ,  0.018513 ,  0.55442  ,  0.55442  ,  0.59529  ,\n",
       "           0.59529  ,  0.30399  ,  0.30399  ,  0.10286  ,  0.10286  ,\n",
       "           0.11277  ,  0.11277  ,  0.080131 ,  0.080131 ,  0.10934  ,\n",
       "           0.10934  ,  0.085419 ,  0.085419 ,  0.041231 ,  0.041231 ,\n",
       "           0.021797 ,  0.021797 ,  0.67374  ,  0.67374  ,  0.35077  ,\n",
       "           0.35077  ,  0.10422  ,  0.10422  ,  0.10999  ,  0.10999  ,\n",
       "           0.12321  ,  0.12321  ,  0.14327  ,  0.14327  ,  0.090271 ,\n",
       "           0.090271 ,  0.028932 ,  0.028932 ,  0.019377 ,  0.019377 ,\n",
       "           0.23682  ,  0.23682  ,  0.10727  ,  0.10727  ,  0.1266   ,\n",
       "           0.1266   ,  0.15259  ,  0.15259  ,  0.18575  ,  0.18575  ,\n",
       "           0.10293  ,  0.10293  ,  0.030834 ,  0.030834 ,  0.023963 ,\n",
       "           0.023963 ,  0.13622  ,  0.13622  ,  0.34866  ,  0.34866  ,\n",
       "           0.28463  ,  0.28463  , -0.024828 , -0.024828 ,  0.037177 ,\n",
       "           0.037177 ,  0.035488 ,  0.035488 ,  0.011733 ,  0.011733 ,\n",
       "           0.16488  ,  0.16488  ,  0.19003  ,  0.19003  ,  0.18605  ,\n",
       "           0.18605  ,  0.036091 ,  0.036091 ,  0.017743 ,  0.017743 ,\n",
       "           0.034771 ,  0.034771 ,  0.67479  ,  0.67479  ,  0.27241  ,\n",
       "           0.27241  ,  0.0089888,  0.0089888,  0.032533 ,  0.032533 ,\n",
       "           0.015969 ,  0.015969 ,  0.40099  ,  0.40099  ,  0.047716 ,\n",
       "           0.047716 ,  0.020583 ,  0.020583 ,  0.022499 ,  0.022499 ,\n",
       "           0.057452 ,  0.057452 ,  0.022605 ,  0.022605 ,  0.060679 ,\n",
       "           0.060679 ,  0.010376 ,  0.010376 ,  0.012433 ,  0.012433 ,\n",
       "           0.0095714,  0.0095714]),\n",
       "   'y_pred': array([ 6.02864623e-01,  5.70070446e-01,  4.71655548e-01,  4.23266917e-01,\n",
       "           3.97323012e-01,  4.86947775e-01,  3.66101444e-01,  3.52854162e-01,\n",
       "           5.27656794e-01,  3.78499717e-01,  4.85656083e-01,  4.05382693e-01,\n",
       "           3.55451703e-01,  3.40795487e-01,  4.10441160e-01,  4.73574400e-01,\n",
       "           1.35674685e-01,  1.19446635e-01,  3.26260030e-01,  3.24118644e-01,\n",
       "           2.71727115e-01,  2.28903487e-01,  2.87722230e-01,  4.54289258e-01,\n",
       "           4.12338614e-01,  4.16029483e-01,  1.72543824e-01,  1.48200244e-01,\n",
       "           1.80292323e-01,  2.94468164e-01,  7.57365465e-01,  7.72858858e-01,\n",
       "           5.75971484e-01,  5.23875535e-01,  4.14610893e-01,  3.85429025e-01,\n",
       "           4.26826924e-01,  4.25692558e-01,  4.03537214e-01,  4.00393695e-01,\n",
       "           3.92933547e-01,  2.71487236e-01,  1.37468278e-01,  1.09837741e-01,\n",
       "           2.37915412e-01,  2.31845438e-01,  1.28893510e-01,  2.01389790e-01,\n",
       "           1.83226272e-01,  2.43439466e-01,  1.16852418e-01,  9.14595276e-02,\n",
       "           4.45127487e-02,  8.98720920e-02,  5.19896299e-02,  3.33502442e-02,\n",
       "           5.16953528e-01,  4.83077645e-01,  5.91470838e-01,  6.34002924e-01,\n",
       "           4.36659634e-01,  4.60685402e-01,  3.86141181e-01,  4.07615870e-01,\n",
       "           3.76530945e-01,  3.53739828e-01,  2.91544795e-01,  3.81384075e-01,\n",
       "           4.54330862e-01,  4.33264792e-01,  8.36025029e-02,  1.40516222e-01,\n",
       "           3.85729253e-01,  3.46859068e-01,  2.05969542e-01,  2.22257748e-01,\n",
       "           1.77440047e-01,  3.11451405e-01,  4.07990277e-01,  4.81042832e-01,\n",
       "           2.34047562e-01,  1.68419510e-01,  1.08991250e-01,  1.10611513e-01,\n",
       "           8.19459438e-01,  6.31641507e-01,  7.88347602e-01,  9.49284077e-01,\n",
       "           3.73324454e-01,  4.27679420e-01,  4.27266598e-01,  4.89298731e-01,\n",
       "           4.14999425e-01,  4.07084346e-01,  3.84183228e-01,  3.50500643e-01,\n",
       "           1.12301856e-01,  1.30692899e-01,  2.43321955e-01,  2.51792669e-01,\n",
       "           1.78047150e-01,  1.00055024e-01,  1.23054385e-01,  1.39008164e-01,\n",
       "           1.20074838e-01,  1.61245197e-01,  1.05034262e-02,  2.27792412e-02,\n",
       "          -7.91704655e-03,  1.54955983e-02,  4.90671784e-01,  4.92139220e-01,\n",
       "           3.88764560e-01,  4.08255339e-01,  3.08125287e-01,  3.92280102e-01,\n",
       "           4.24273372e-01,  4.73274529e-01,  4.83859807e-01,  5.02818346e-01,\n",
       "           4.23967421e-01,  4.82178330e-01,  1.44192338e-01,  1.50690287e-01,\n",
       "           2.09253013e-01,  2.42351979e-01,  3.15400451e-01,  1.96761131e-01,\n",
       "           2.46140108e-01,  2.30865046e-01,  6.14026248e-01,  5.60818374e-01,\n",
       "           3.39270175e-01,  3.46057802e-01,  2.32474789e-01,  2.64958501e-01,\n",
       "           4.27326381e-01,  4.22823966e-01,  3.92678022e-01,  4.08065557e-01,\n",
       "           5.28856277e-01,  5.10658026e-01,  4.91005659e-01,  5.21861792e-01,\n",
       "           4.50399041e-01,  4.82216895e-01,  3.60361427e-01,  4.09154445e-01,\n",
       "           1.15809619e-01,  9.76030678e-02,  1.48748398e-01,  2.36379489e-01,\n",
       "           2.11227849e-01,  1.80501878e-01,  1.72827631e-01,  2.48316616e-01,\n",
       "           8.09185952e-02,  1.12371027e-01,  9.95811969e-02,  6.40518963e-02,\n",
       "           5.89270741e-02, -3.38262618e-02,  4.68638927e-01,  5.22560656e-01,\n",
       "           3.52732122e-01,  4.09237862e-01,  2.93943942e-01,  2.00417489e-01,\n",
       "           2.75031447e-01,  2.86281407e-01,  4.88392651e-01,  4.04826611e-01,\n",
       "           1.99063599e-01,  2.29677796e-01,  4.53061163e-01,  5.97504258e-01,\n",
       "           2.26651758e-01,  3.03226620e-01,  2.88425952e-01,  3.26526523e-01,\n",
       "           4.30424988e-01,  3.99302214e-01,  2.53971070e-01,  3.46905231e-01,\n",
       "           2.64682502e-01,  2.13274181e-01,  5.30169010e-01,  4.07607079e-01,\n",
       "           5.75776339e-01,  4.88110840e-01,  4.58416611e-01,  4.02681768e-01,\n",
       "           4.48387951e-01,  4.30701554e-01,  4.06234682e-01,  3.67457539e-01,\n",
       "           4.45875287e-01,  4.56536114e-01,  1.58477217e-01,  1.38176531e-01,\n",
       "           2.19329193e-01,  4.68536377e-01,  3.63528907e-01,  4.34917718e-01,\n",
       "           2.35366195e-01,  1.97049990e-01,  1.19239941e-01,  8.17988962e-02,\n",
       "           1.13412917e-01,  5.99792153e-02,  5.64843416e-03, -1.95414722e-02,\n",
       "           3.59699905e-01,  3.67930293e-01,  3.75721991e-01,  3.86180073e-01,\n",
       "           3.62873137e-01,  2.38748789e-01,  4.73615974e-01,  4.37264919e-01,\n",
       "           1.05854690e-01,  1.63069844e-01,  4.11670148e-01,  5.45909047e-01,\n",
       "           2.71588743e-01,  3.01007181e-01,  7.18935877e-02,  1.30892247e-01,\n",
       "           4.59883928e-01,  4.75962043e-01,  1.45299435e-01,  2.27019638e-01,\n",
       "          -1.16177499e-02,  1.95997506e-02,  3.64826173e-01,  4.33238804e-01,\n",
       "           3.73452723e-01,  4.78996396e-01,  4.58281517e-01,  4.81800795e-01,\n",
       "           5.53416193e-01,  5.12234151e-01,  4.47041988e-01,  4.23491657e-01,\n",
       "           6.67188048e-01,  5.21473110e-01,  7.55344182e-02,  9.91480052e-02,\n",
       "           1.64682209e-01,  1.52123660e-01,  9.93949026e-02,  1.24914780e-01,\n",
       "           5.08313328e-02,  8.32846016e-02,  6.21455908e-02,  9.32225585e-02,\n",
       "           4.62075472e-02,  3.94866616e-02, -1.11898184e-02,  8.26501846e-03,\n",
       "           2.50305802e-01,  1.65270999e-01,  2.79875308e-01,  3.40574861e-01,\n",
       "           4.96331453e-01,  5.39333940e-01,  1.15919560e-01,  7.86268264e-02,\n",
       "           6.93144560e-01,  2.57820696e-01,  3.92982543e-01,  2.64930546e-01,\n",
       "           5.17197609e-01,  4.78781462e-01,  4.61415470e-01,  2.84908056e-01,\n",
       "           1.57541618e-01,  8.47363323e-02,  3.28424007e-01,  2.13107884e-01,\n",
       "           4.63020504e-01,  5.87158442e-01,  3.22153389e-01,  3.99805844e-01,\n",
       "           4.01996613e-01,  3.60688239e-01,  4.51797485e-01,  4.28024292e-01,\n",
       "           3.82559061e-01,  1.84430629e-01,  4.76747870e-01,  1.98575363e-01,\n",
       "           1.38578549e-01,  6.13027364e-02,  4.32012886e-01,  3.50335419e-01,\n",
       "           1.93332404e-01,  2.32095122e-01,  2.56617069e-01,  2.63164014e-01,\n",
       "           7.32329190e-02,  1.52373821e-01,  1.42521411e-02,  3.18309516e-02,\n",
       "           3.98878604e-02,  3.12293172e-02,  5.07477880e-01,  4.07389641e-01,\n",
       "           4.42283630e-01,  4.45648551e-01,  1.17613226e-01,  1.77150100e-01,\n",
       "           3.48985851e-01,  1.27193049e-01,  2.77724415e-01,  1.76510140e-01,\n",
       "           2.20838428e-01,  1.40409797e-01,  3.26668113e-01,  3.04197580e-01,\n",
       "           5.20082951e-01,  2.42767110e-01,  4.40270215e-01,  2.13376179e-01,\n",
       "           2.81934321e-01,  5.29078245e-01,  2.31638849e-01,  2.41696775e-01,\n",
       "           4.65164721e-01,  3.03949326e-01,  3.57299685e-01,  3.88963491e-01,\n",
       "           3.79572272e-01,  2.95875371e-01,  3.40629101e-01,  2.16809154e-01,\n",
       "           8.23387355e-02,  7.54554719e-02,  2.07186475e-01,  1.98421568e-01,\n",
       "           6.68274760e-02,  2.49946803e-01,  1.90661967e-01,  1.19010895e-01,\n",
       "          -1.20933950e-02,  1.14076212e-01,  5.13647348e-02,  6.17988110e-02,\n",
       "          -4.06520069e-02, -2.68357098e-02,  3.58803362e-01,  4.21993285e-01,\n",
       "           1.07425824e-01,  1.53175056e-01,  1.91271946e-01,  2.28548616e-01,\n",
       "           2.77497292e-01,  2.97954053e-01,  2.34338760e-01,  3.95059943e-01,\n",
       "           4.04230982e-01,  2.65314162e-01,  3.06889236e-01,  4.00491178e-01,\n",
       "           4.01618063e-01,  3.59237373e-01,  3.26168865e-01,  3.45227480e-01,\n",
       "           3.39669228e-01,  2.72469103e-01,  4.28062648e-01,  4.15018886e-01,\n",
       "           3.40912461e-01,  3.43818724e-01,  4.50186610e-01,  5.74506879e-01,\n",
       "           2.37436801e-01,  2.98395693e-01,  1.03467345e-01,  1.22729689e-01,\n",
       "           2.52636760e-01,  2.69830078e-01,  2.38957226e-01,  2.44482398e-01,\n",
       "           4.08620447e-01,  3.20286989e-01,  1.13652572e-01,  7.99708962e-02,\n",
       "           4.83956188e-02,  7.15689808e-02,  5.30670285e-02,  3.67224813e-02,\n",
       "           1.06559843e-01,  4.47950512e-02,  5.23109794e-01,  2.72796422e-01,\n",
       "           2.87451893e-01,  2.30781615e-01,  5.88157058e-01,  3.87720823e-01,\n",
       "           3.83515924e-01,  4.57233936e-01,  1.77855015e-01,  1.47823632e-01,\n",
       "           3.95387471e-01,  2.32327446e-01,  5.25780916e-01,  3.61994922e-01,\n",
       "           4.39129174e-01,  4.39005375e-01,  4.77751434e-01,  3.39317083e-01,\n",
       "           5.56641161e-01,  5.91149330e-01,  4.18862551e-01,  5.35418749e-01,\n",
       "           4.20867711e-01,  3.75466824e-01,  7.45504051e-02,  9.99247730e-02,\n",
       "           2.16999948e-01,  2.79075712e-01,  1.22686595e-01,  2.09820658e-01,\n",
       "           3.45525086e-01,  6.97112978e-02,  3.91933918e-02,  8.61139745e-02,\n",
       "           1.20281577e-02,  4.97664064e-02,  1.45825595e-02,  8.89223367e-02,\n",
       "           9.87600982e-02,  1.68787509e-01,  9.70418900e-02,  1.61621556e-01,\n",
       "           1.70283109e-01,  1.95752412e-01,  1.74408183e-01,  1.96146354e-01,\n",
       "           1.18454337e-01,  1.67111665e-01,  1.68186679e-01,  1.57615066e-01,\n",
       "           1.01267070e-01,  1.25058219e-01,  1.62495360e-01,  1.10284194e-01,\n",
       "           1.16853133e-01,  1.44537002e-01,  1.11866936e-01,  1.00575238e-01,\n",
       "           1.46809220e-01,  8.85750353e-02,  8.68325233e-02,  1.13859743e-01,\n",
       "           5.31248301e-02,  7.33225793e-02,  2.76655287e-01,  1.30905122e-01,\n",
       "           2.85651863e-01,  1.53457612e-01,  1.56054959e-01,  2.05330163e-01,\n",
       "           9.22220945e-02,  6.99035972e-02,  4.14915830e-02,  3.99902910e-02,\n",
       "           8.62050056e-03,  1.27709508e-02,  5.30640066e-01,  5.81105709e-01,\n",
       "           1.62740007e-01,  3.04891884e-01,  2.28046581e-01,  3.82856697e-01,\n",
       "           2.15199500e-01,  2.89331585e-01,  1.18736431e-01,  2.54030973e-01,\n",
       "           2.33909696e-01,  2.44410276e-01,  2.33548835e-01,  3.52776736e-01,\n",
       "           2.50503272e-01,  2.34136954e-01,  2.07030863e-01,  2.45153263e-01,\n",
       "           3.58461320e-01,  2.87553817e-01,  4.52939242e-01,  5.13133168e-01,\n",
       "           1.05502009e-01,  1.09889463e-01,  3.33284825e-01,  3.55673581e-01,\n",
       "           2.46653453e-01,  3.14022303e-01,  1.28341079e-01,  1.58650249e-01,\n",
       "           6.47242963e-02,  8.40288401e-03,  3.37067395e-02,  8.21181536e-02,\n",
       "          -1.61230564e-02,  3.65231335e-02,  2.84450650e-01,  3.77687156e-01,\n",
       "           2.58790553e-01,  4.08302605e-01,  3.91670555e-01,  3.58369768e-01,\n",
       "           2.23186910e-01,  3.73268068e-01,  2.86792457e-01,  2.13313609e-01,\n",
       "           2.18533099e-01,  1.42081410e-01,  2.53147304e-01,  2.30551988e-01,\n",
       "           2.44729400e-01,  2.76581287e-01,  2.74958700e-01,  2.11384177e-01,\n",
       "           4.14265037e-01,  2.02849865e-01,  1.65660262e-01,  5.21100014e-02,\n",
       "           5.41484952e-01,  6.50173068e-01,  2.88776010e-01,  2.66720086e-01,\n",
       "           2.85273045e-01,  1.65703505e-01,  7.86257982e-02,  7.31831193e-02,\n",
       "           2.37523913e-02,  5.09267896e-02,  1.36183202e-02, -1.30558312e-02,\n",
       "           2.20542058e-01,  2.67316014e-01,  1.39391109e-01,  2.32438073e-01,\n",
       "           2.58872002e-01,  3.41563821e-01,  2.86051989e-01,  2.52771974e-01,\n",
       "           1.17392004e-01,  2.60934025e-01,  1.40797600e-01,  7.48920441e-02,\n",
       "           2.09310532e-01,  2.27951735e-01,  1.71371728e-01,  1.73369646e-01,\n",
       "           2.14047909e-01,  3.31615746e-01,  1.77548856e-01,  9.35361385e-02,\n",
       "           3.44333470e-01,  1.65574312e-01,  1.48183733e-01,  1.97225690e-01,\n",
       "           2.71977633e-01,  3.58991385e-01, -1.60151720e-03,  4.85193729e-03,\n",
       "           3.16572785e-02, -3.99244726e-02,  2.74138451e-02,  6.14372790e-02,\n",
       "           3.65255535e-01,  4.60047424e-01,  1.85141921e-01,  1.69494361e-01,\n",
       "           3.32060784e-01,  3.99439096e-01,  3.75329018e-01,  3.84624451e-01,\n",
       "           4.68906581e-01,  4.42365170e-01,  4.45269048e-01,  4.34420258e-01,\n",
       "           3.68979305e-01,  3.88906002e-01,  3.95988315e-01,  2.02259302e-01,\n",
       "           1.58531740e-01,  1.49540022e-01,  3.43100458e-01,  2.28946641e-01,\n",
       "           3.16482782e-01,  2.17990533e-01,  2.13450581e-01,  2.62576669e-01,\n",
       "           7.20681250e-02,  1.27009660e-01,  2.59138942e-02,  5.19130081e-02,\n",
       "           3.47878933e-02,  1.49482489e-02,  3.63695145e-01,  2.98575848e-01,\n",
       "           1.11901119e-01,  1.95151076e-01,  2.84561396e-01,  1.21620223e-01,\n",
       "           1.88507780e-01,  2.42957741e-01,  2.21675783e-01,  1.51510283e-01,\n",
       "           2.01751232e-01,  2.34568819e-01,  2.68100888e-01,  1.21758163e-01,\n",
       "           8.59266073e-02,  1.08175367e-01,  4.15842503e-01,  3.66324842e-01,\n",
       "           2.09303170e-01,  4.07875240e-01,  3.04771066e-01,  2.75799811e-01,\n",
       "           8.80254060e-02,  7.17478245e-02, -1.22694671e-02, -1.03723109e-02,\n",
       "          -4.41512764e-02,  3.11577320e-02,  1.84495598e-01,  1.20564654e-01,\n",
       "           1.72165155e-01,  1.45101428e-01,  1.57857805e-01,  2.77472049e-01,\n",
       "           1.71078026e-01,  1.63824454e-01,  1.29482061e-01,  2.03448489e-01,\n",
       "           1.46178395e-01,  1.43125638e-01,  1.32989332e-01,  1.40495881e-01,\n",
       "           2.39072099e-01,  1.39188036e-01,  1.52582482e-01,  1.79582059e-01,\n",
       "           2.84865737e-01,  3.07429641e-01,  3.74285579e-02,  1.65531188e-01,\n",
       "           8.31782818e-02,  1.30577445e-01,  7.42069036e-02,  9.28363502e-02,\n",
       "           6.80967331e-01,  7.87526488e-01,  5.23946524e-01,  5.09630084e-01,\n",
       "           4.84470785e-01,  4.24369097e-01,  3.43987554e-01,  4.42634612e-01,\n",
       "           4.10838723e-01,  3.05303693e-01,  9.19936597e-02,  1.28789932e-01,\n",
       "           9.25962776e-02,  1.60289139e-01,  6.62138909e-02,  2.78958350e-01,\n",
       "           1.75061554e-01,  2.58321166e-01,  4.06874418e-02,  2.35005662e-01,\n",
       "          -9.63377953e-03,  8.29007328e-02,  9.48137045e-03,  2.06307769e-02,\n",
       "           3.56089175e-01,  4.00952578e-01,  4.52804685e-01,  4.11941022e-01,\n",
       "           3.73750091e-01,  3.93707395e-01,  3.70408207e-01,  3.94365966e-01,\n",
       "           1.15218326e-01,  7.10759461e-02,  1.32510230e-01,  1.35761425e-01,\n",
       "           2.50736117e-01,  8.93131047e-02,  1.22266263e-01,  1.54390305e-01,\n",
       "           3.89100611e-02,  2.01493651e-02,  4.37766314e-02,  3.18500400e-03,\n",
       "          -2.75115371e-02, -1.34351850e-03,  5.20018935e-01,  5.16632318e-01,\n",
       "           4.76933599e-01,  5.26977897e-01,  4.02915537e-01,  2.85875440e-01,\n",
       "           1.26798004e-01,  1.13054633e-01,  1.51233256e-01,  1.13765195e-01,\n",
       "           2.08489150e-01,  1.13429680e-01,  1.52105168e-01,  7.14583695e-02,\n",
       "           9.82237756e-02,  1.17728442e-01,  2.32159495e-02,  4.74736094e-02,\n",
       "           4.76022512e-02,  5.76869994e-02,  5.00083566e-01,  6.63113713e-01,\n",
       "           5.05503595e-01,  3.87651920e-01,  5.58996350e-02,  8.83912742e-02,\n",
       "           1.32189929e-01,  1.74876928e-01,  9.54413563e-02,  1.41056031e-01,\n",
       "           9.99930352e-02,  1.08321279e-01,  1.06406420e-01,  8.04791301e-02,\n",
       "           5.09706438e-02,  6.49207830e-02,  2.78778523e-02,  1.36186033e-02,\n",
       "           4.38186407e-01,  3.37925434e-01,  9.11857337e-02,  1.08496562e-01,\n",
       "           2.27052033e-01,  1.18126854e-01,  1.88320652e-01,  2.30465382e-01,\n",
       "           1.06737062e-01,  9.76999849e-02,  1.16170764e-01,  1.28912687e-01,\n",
       "          -6.13063574e-04,  7.35417008e-02,  1.50984824e-02,  1.38494521e-02,\n",
       "           8.41529071e-02,  1.12384230e-01,  4.05041099e-01,  4.11877245e-01,\n",
       "           4.04123127e-01,  3.61595452e-01,  2.13783458e-01,  1.39373749e-01,\n",
       "           6.56146705e-02,  1.76688284e-02,  6.95968419e-02, -1.63186789e-02,\n",
       "           3.06483507e-02,  4.94691730e-03,  2.29130700e-01,  1.89463437e-01,\n",
       "           2.67593741e-01,  2.56800681e-01,  1.41511589e-01,  1.71265692e-01,\n",
       "           4.64455187e-02,  7.47889429e-02,  4.10395116e-02,  8.93311203e-03,\n",
       "           2.91153938e-02, -2.13158131e-03,  3.54813099e-01,  4.04192328e-01,\n",
       "           1.78642631e-01,  3.34657371e-01,  5.32926172e-02, -2.79710293e-02,\n",
       "          -1.55652463e-02,  1.91674680e-02, -7.26205111e-03,  2.24783719e-02,\n",
       "           3.24430346e-01,  3.11328828e-01,  2.11033225e-01,  2.68009126e-01,\n",
       "          -4.41940427e-02,  1.85965151e-02,  1.14676684e-01,  5.39123118e-02,\n",
       "           4.72788811e-02,  1.01834834e-01,  6.35818839e-02,  1.01340041e-01,\n",
       "           1.14855975e-01,  1.13143966e-01, -3.88960540e-02,  7.24963844e-03,\n",
       "           1.09860152e-02, -3.10513973e-02, -8.67864490e-03, -6.84899092e-03],\n",
       "         dtype=float32),\n",
       "   'feature_importances': array([0.0080945 , 0.0124976 , 0.03109321, 0.03400255, 0.06231288,\n",
       "          0.01412028, 0.01357787, 0.05825002, 0.02115348, 0.02237158,\n",
       "          0.01885904, 0.01379797, 0.01407453, 0.01167567, 0.01555672,\n",
       "          0.03751874, 0.0156835 , 0.03257258, 0.01854269, 0.02358656,\n",
       "          0.01391093, 0.00984279, 0.03507164, 0.04516212, 0.03191764,\n",
       "          0.01414396, 0.01400892, 0.0637697 , 0.01294174, 0.0403651 ,\n",
       "          0.0220586 , 0.01295149, 0.02656778, 0.01354628, 0.02231911,\n",
       "          0.03379391, 0.0298082 , 0.03432113, 0.02372541, 0.02043153],\n",
       "         dtype=float32),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 0.000662425106155722,\n",
       "    'mae': 0.0198908579608497,\n",
       "    'r2': 0.9801188677120769,\n",
       "    'pearson_corr': 0.9902897102431489},\n",
       "   'best_val_score': 0.7986328158873616,\n",
       "   'test_metrics': {'mse': 0.009469663746600977,\n",
       "    'mae': 0.06739890985781175,\n",
       "    'r2': 0.697816079108099,\n",
       "    'pearson_corr': 0.836533644830454,\n",
       "    'connectome_corr': 0.7607976107729945,\n",
       "    'connectome_r2': 0.46163836953733445,\n",
       "    'geodesic_distance': 6.059807663602764},\n",
       "   'y_true': array([ 0.33148  ,  0.33148  ,  0.25656  ,  0.25656  ,  0.20587  ,\n",
       "           0.20587  ,  0.31174  ,  0.31174  ,  0.21878  ,  0.21878  ,\n",
       "           0.21532  ,  0.21532  ,  0.17031  ,  0.17031  ,  0.2146   ,\n",
       "           0.2146   ,  0.17066  ,  0.17066  ,  0.22792  ,  0.22792  ,\n",
       "           0.16084  ,  0.16084  ,  0.35445  ,  0.35445  ,  0.29327  ,\n",
       "           0.29327  ,  0.27628  ,  0.27628  ,  0.16877  ,  0.16877  ,\n",
       "           0.25129  ,  0.25129  ,  0.23922  ,  0.23922  ,  0.24166  ,\n",
       "           0.24166  ,  0.23041  ,  0.23041  ,  0.1454   ,  0.1454   ,\n",
       "           0.2518   ,  0.2518   ,  0.2819   ,  0.2819   ,  0.1944   ,\n",
       "           0.1944   ,  0.20706  ,  0.20706  ,  0.18931  ,  0.18931  ,\n",
       "           0.1788   ,  0.1788   ,  0.09972  ,  0.09972  ,  0.73677  ,\n",
       "           0.73677  ,  0.52629  ,  0.52629  ,  0.64905  ,  0.64905  ,\n",
       "           0.37377  ,  0.37377  ,  0.28208  ,  0.28208  ,  0.17852  ,\n",
       "           0.17852  ,  0.13445  ,  0.13445  ,  0.23099  ,  0.23099  ,\n",
       "           0.18371  ,  0.18371  ,  0.32211  ,  0.32211  ,  0.5291   ,\n",
       "           0.5291   ,  0.74704  ,  0.74704  ,  0.49516  ,  0.49516  ,\n",
       "           0.3926   ,  0.3926   ,  0.57732  ,  0.57732  ,  0.39736  ,\n",
       "           0.39736  ,  0.46903  ,  0.46903  ,  0.45497  ,  0.45497  ,\n",
       "           0.25314  ,  0.25314  ,  0.28381  ,  0.28381  ,  0.29232  ,\n",
       "           0.29232  ,  0.12649  ,  0.12649  ,  0.17147  ,  0.17147  ,\n",
       "           0.11213  ,  0.11213  ,  0.14714  ,  0.14714  ,  0.16611  ,\n",
       "           0.16611  ,  0.5164   ,  0.5164   ,  0.55261  ,  0.55261  ,\n",
       "           0.36462  ,  0.36462  ,  0.23562  ,  0.23562  ,  0.18814  ,\n",
       "           0.18814  ,  0.10769  ,  0.10769  ,  0.27835  ,  0.27835  ,\n",
       "           0.16852  ,  0.16852  ,  0.2832   ,  0.2832   ,  0.99059  ,\n",
       "           0.99059  ,  1.3686   ,  1.3686   ,  0.51897  ,  0.51897  ,\n",
       "           0.39011  ,  0.39011  ,  0.46333  ,  0.46333  ,  0.42757  ,\n",
       "           0.42757  ,  0.5324   ,  0.5324   ,  0.49968  ,  0.49968  ,\n",
       "           0.18134  ,  0.18134  ,  0.34009  ,  0.34009  ,  0.35677  ,\n",
       "           0.35677  ,  0.088168 ,  0.088168 ,  0.15894  ,  0.15894  ,\n",
       "           0.056894 ,  0.056894 ,  0.12265  ,  0.12265  ,  0.14579  ,\n",
       "           0.14579  ,  0.39599  ,  0.39599  ,  0.32793  ,  0.32793  ,\n",
       "           0.3222   ,  0.3222   ,  0.17706  ,  0.17706  ,  0.17992  ,\n",
       "           0.17992  ,  0.25264  ,  0.25264  ,  0.22991  ,  0.22991  ,\n",
       "           0.25632  ,  0.25632  ,  0.39837  ,  0.39837  ,  0.49432  ,\n",
       "           0.49432  ,  0.57933  ,  0.57933  ,  0.6152   ,  0.6152   ,\n",
       "           0.38098  ,  0.38098  ,  0.30403  ,  0.30403  ,  0.3698   ,\n",
       "           0.3698   ,  0.43567  ,  0.43567  ,  0.2435   ,  0.2435   ,\n",
       "           0.15002  ,  0.15002  ,  0.18289  ,  0.18289  ,  0.14929  ,\n",
       "           0.14929  ,  0.17282  ,  0.17282  ,  0.14255  ,  0.14255  ,\n",
       "           0.16803  ,  0.16803  ,  0.20912  ,  0.20912  ,  0.31918  ,\n",
       "           0.31918  ,  0.15832  ,  0.15832  ,  0.14124  ,  0.14124  ,\n",
       "           0.12168  ,  0.12168  ,  0.21204  ,  0.21204  ,  0.090976 ,\n",
       "           0.090976 ,  0.34417  ,  0.34417  ,  0.48272  ,  0.48272  ,\n",
       "           0.57398  ,  0.57398  ,  0.37793  ,  0.37793  ,  0.27984  ,\n",
       "           0.27984  ,  0.39158  ,  0.39158  ,  0.39081  ,  0.39081  ,\n",
       "           0.46012  ,  0.46012  ,  0.42524  ,  0.42524  ,  0.1816   ,\n",
       "           0.1816   ,  0.34134  ,  0.34134  ,  0.38275  ,  0.38275  ,\n",
       "           0.05783  ,  0.05783  ,  0.17506  ,  0.17506  ,  0.020916 ,\n",
       "           0.020916 ,  0.21375  ,  0.21375  ,  0.13493  ,  0.13493  ,\n",
       "           0.18505  ,  0.18505  ,  0.23936  ,  0.23936  ,  0.10311  ,\n",
       "           0.10311  ,  0.15239  ,  0.15239  ,  0.097861 ,  0.097861 ,\n",
       "           0.18874  ,  0.18874  ,  0.33161  ,  0.33161  ,  0.35783  ,\n",
       "           0.35783  ,  0.47651  ,  0.47651  ,  0.28464  ,  0.28464  ,\n",
       "           0.38285  ,  0.38285  ,  0.49751  ,  0.49751  ,  0.43599  ,\n",
       "           0.43599  ,  0.44142  ,  0.44142  ,  0.18     ,  0.18     ,\n",
       "           0.31861  ,  0.31861  ,  0.25083  ,  0.25083  ,  0.072256 ,\n",
       "           0.072256 ,  0.20171  ,  0.20171  ,  0.08234  ,  0.08234  ,\n",
       "           0.10307  ,  0.10307  ,  0.11407  ,  0.11407  ,  0.37818  ,\n",
       "           0.37818  ,  0.70079  ,  0.70079  ,  0.22818  ,  0.22818  ,\n",
       "           0.4826   ,  0.4826   ,  0.18201  ,  0.18201  ,  0.22923  ,\n",
       "           0.22923  ,  0.22029  ,  0.22029  ,  0.32249  ,  0.32249  ,\n",
       "           0.29678  ,  0.29678  ,  0.37948  ,  0.37948  ,  0.1018   ,\n",
       "           0.1018   ,  0.11987  ,  0.11987  ,  0.15403  ,  0.15403  ,\n",
       "           0.38225  ,  0.38225  ,  0.047428 ,  0.047428 ,  0.17087  ,\n",
       "           0.17087  ,  0.48353  ,  0.48353  ,  0.28801  ,  0.28801  ,\n",
       "           0.49612  ,  0.49612  ,  0.19856  ,  0.19856  ,  0.15518  ,\n",
       "           0.15518  ,  0.35209  ,  0.35209  ,  0.15824  ,  0.15824  ,\n",
       "           0.24511  ,  0.24511  ,  0.11831  ,  0.11831  ,  0.21464  ,\n",
       "           0.21464  ,  0.1765   ,  0.1765   ,  0.24353  ,  0.24353  ,\n",
       "           0.16734  ,  0.16734  ,  0.26438  ,  0.26438  ,  0.23999  ,\n",
       "           0.23999  ,  0.19193  ,  0.19193  ,  0.22554  ,  0.22554  ,\n",
       "           0.20534  ,  0.20534  ,  0.18197  ,  0.18197  ,  0.17313  ,\n",
       "           0.17313  ,  0.2402   ,  0.2402   ,  0.34958  ,  0.34958  ,\n",
       "           0.33265  ,  0.33265  ,  0.12881  ,  0.12881  ,  0.093283 ,\n",
       "           0.093283 ,  0.24612  ,  0.24612  ,  0.54612  ,  0.54612  ,\n",
       "           0.1315   ,  0.1315   ,  0.17622  ,  0.17622  ,  0.094095 ,\n",
       "           0.094095 ,  0.20863  ,  0.20863  ,  0.18872  ,  0.18872  ,\n",
       "           0.22455  ,  0.22455  ,  0.037041 ,  0.037041 ,  0.067595 ,\n",
       "           0.067595 ,  0.071538 ,  0.071538 ,  0.26982  ,  0.26982  ,\n",
       "           0.10338  ,  0.10338  ,  0.23899  ,  0.23899  ,  0.59564  ,\n",
       "           0.59564  ,  0.30835  ,  0.30835  ,  0.58107  ,  0.58107  ,\n",
       "           0.36845  ,  0.36845  ,  0.11855  ,  0.11855  ,  0.61204  ,\n",
       "           0.61204  ,  0.37042  ,  0.37042  ,  0.31008  ,  0.31008  ,\n",
       "           0.22684  ,  0.22684  ,  0.28242  ,  0.28242  ,  0.23092  ,\n",
       "           0.23092  ,  0.24189  ,  0.24189  ,  0.068368 ,  0.068368 ,\n",
       "           0.2144   ,  0.2144   ,  0.11238  ,  0.11238  ,  0.22456  ,\n",
       "           0.22456  ,  0.074981 ,  0.074981 ,  0.45418  ,  0.45418  ,\n",
       "           0.38823  ,  0.38823  ,  0.22955  ,  0.22955  ,  0.24779  ,\n",
       "           0.24779  ,  0.46636  ,  0.46636  ,  0.231    ,  0.231    ,\n",
       "           0.26248  ,  0.26248  ,  0.21753  ,  0.21753  ,  0.13705  ,\n",
       "           0.13705  ,  0.25632  ,  0.25632  ,  0.21373  ,  0.21373  ,\n",
       "           0.23771  ,  0.23771  , -0.029437 , -0.029437 ,  0.11596  ,\n",
       "           0.11596  ,  0.028802 ,  0.028802 ,  0.31914  ,  0.31914  ,\n",
       "           0.011954 ,  0.011954 ,  0.46983  ,  0.46983  ,  0.70364  ,\n",
       "           0.70364  ,  0.28605  ,  0.28605  ,  0.52316  ,  0.52316  ,\n",
       "           0.51308  ,  0.51308  ,  0.23119  ,  0.23119  ,  0.26831  ,\n",
       "           0.26831  ,  0.26953  ,  0.26953  ,  0.28503  ,  0.28503  ,\n",
       "           0.22905  ,  0.22905  ,  0.29339  ,  0.29339  ,  0.16203  ,\n",
       "           0.16203  ,  0.23115  ,  0.23115  ,  0.18003  ,  0.18003  ,\n",
       "           0.28613  ,  0.28613  ,  0.09553  ,  0.09553  ,  0.27519  ,\n",
       "           0.27519  ,  0.21045  ,  0.21045  ,  0.19372  ,  0.19372  ,\n",
       "           0.15535  ,  0.15535  ,  0.28494  ,  0.28494  ,  0.22675  ,\n",
       "           0.22675  ,  1.0155   ,  1.0155   ,  0.46736  ,  0.46736  ,\n",
       "           0.3253   ,  0.3253   ,  0.38513  ,  0.38513  ,  0.38889  ,\n",
       "           0.38889  ,  0.47048  ,  0.47048  ,  0.43624  ,  0.43624  ,\n",
       "           0.16823  ,  0.16823  ,  0.32943  ,  0.32943  ,  0.38119  ,\n",
       "           0.38119  ,  0.15107  ,  0.15107  ,  0.21734  ,  0.21734  ,\n",
       "           0.12138  ,  0.12138  ,  0.19254  ,  0.19254  ,  0.13583  ,\n",
       "           0.13583  ,  0.50327  ,  0.50327  ,  0.36506  ,  0.36506  ,\n",
       "           0.45718  ,  0.45718  ,  0.42915  ,  0.42915  ,  0.52831  ,\n",
       "           0.52831  ,  0.49068  ,  0.49068  ,  0.17272  ,  0.17272  ,\n",
       "           0.34581  ,  0.34581  ,  0.34235  ,  0.34235  ,  0.065032 ,\n",
       "           0.065032 ,  0.1468   ,  0.1468   ,  0.042703 ,  0.042703 ,\n",
       "           0.091022 ,  0.091022 ,  0.13416  ,  0.13416  ,  0.54402  ,\n",
       "           0.54402  ,  0.52982  ,  0.52982  ,  0.51557  ,  0.51557  ,\n",
       "           0.48936  ,  0.48936  ,  0.50885  ,  0.50885  ,  0.25358  ,\n",
       "           0.25358  ,  0.29956  ,  0.29956  ,  0.31516  ,  0.31516  ,\n",
       "           0.19706  ,  0.19706  ,  0.27752  ,  0.27752  ,  0.17393  ,\n",
       "           0.17393  ,  0.22682  ,  0.22682  ,  0.20172  ,  0.20172  ,\n",
       "           0.35103  ,  0.35103  ,  0.29129  ,  0.29129  ,  0.29662  ,\n",
       "           0.29662  ,  0.37846  ,  0.37846  ,  0.2243   ,  0.2243   ,\n",
       "           0.12522  ,  0.12522  ,  0.15747  ,  0.15747  ,  0.16605  ,\n",
       "           0.16605  ,  0.17666  ,  0.17666  ,  0.15819  ,  0.15819  ,\n",
       "           0.17849  ,  0.17849  ,  0.18453  ,  0.18453  ,  0.45635  ,\n",
       "           0.45635  ,  0.46468  ,  0.46468  ,  0.45726  ,  0.45726  ,\n",
       "           0.34839  ,  0.34839  ,  0.3866   ,  0.3866   ,  0.39813  ,\n",
       "           0.39813  ,  0.35559  ,  0.35559  ,  0.28283  ,  0.28283  ,\n",
       "           0.29687  ,  0.29687  ,  0.24929  ,  0.24929  ,  0.1526   ,\n",
       "           0.1526   ,  0.57962  ,  0.57962  ,  0.67896  ,  0.67896  ,\n",
       "           0.1415   ,  0.1415   ,  0.57955  ,  0.57955  ,  0.30799  ,\n",
       "           0.30799  , -0.0033665, -0.0033665,  0.27324  ,  0.27324  ,\n",
       "           0.027971 ,  0.027971 ,  0.077822 ,  0.077822 ,  0.087999 ,\n",
       "           0.087999 ,  0.60946  ,  0.60946  ,  0.15583  ,  0.15583  ,\n",
       "           0.53966  ,  0.53966  ,  0.44818  ,  0.44818  ,  0.10311  ,\n",
       "           0.10311  ,  0.19167  ,  0.19167  ,  0.071054 ,  0.071054 ,\n",
       "           0.18039  ,  0.18039  ,  0.11114  ,  0.11114  ,  0.1465   ,\n",
       "           0.1465   ,  0.50191  ,  0.50191  ,  0.27017  ,  0.27017  ,\n",
       "           0.0045791,  0.0045791,  0.18719  ,  0.18719  ,  0.047998 ,\n",
       "           0.047998 ,  0.084946 ,  0.084946 ,  0.096601 ,  0.096601 ,\n",
       "           0.067615 ,  0.067615 ,  0.15926  ,  0.15926  ,  0.37743  ,\n",
       "           0.37743  ,  0.27607  ,  0.27607  ,  0.41946  ,  0.41946  ,\n",
       "           0.21872  ,  0.21872  ,  0.2139   ,  0.2139   ,  0.5309   ,\n",
       "           0.5309   ,  0.17173  ,  0.17173  ,  0.1977   ,  0.1977   ,\n",
       "           0.15914  ,  0.15914  ,  0.31658  ,  0.31658  ,  0.027    ,\n",
       "           0.027    ,  0.4958   ,  0.4958   ,  0.25637  ,  0.25637  ,\n",
       "           0.25208  ,  0.25208  ,  0.60549  ,  0.60549  ,  0.13979  ,\n",
       "           0.13979  ,  0.36196  ,  0.36196  ,  0.85576  ,  0.85576  ,\n",
       "           0.71396  ,  0.71396  ,  0.17641  ,  0.17641  ,  0.43945  ,\n",
       "           0.43945  ,  0.28797  ,  0.28797  ,  0.15046  ,  0.15046  ,\n",
       "           0.48791  ,  0.48791  ,  0.14908  ,  0.14908  ,  0.18887  ,\n",
       "           0.18887  ]),\n",
       "   'y_pred': array([ 0.31471795,  0.3550055 ,  0.4034199 ,  0.52620685,  0.28520665,\n",
       "           0.3533917 ,  0.24758944,  0.5460807 ,  0.2976919 ,  0.20003483,\n",
       "           0.19487147,  0.18015453,  0.16630466,  0.14402872,  0.1296036 ,\n",
       "           0.06670858,  0.34084216,  0.17802295,  0.22094956,  0.16571417,\n",
       "           0.21636344,  0.25802624,  0.22596925,  0.39171135,  0.2598107 ,\n",
       "           0.3910306 ,  0.35293832,  0.36706975,  0.18877265,  0.2069383 ,\n",
       "           0.46540046,  0.20044327,  0.23310888,  0.31309587,  0.23824939,\n",
       "           0.33537817,  0.42143065,  0.29282114,  0.22753854,  0.1420845 ,\n",
       "           0.29586864,  0.13239554,  0.25037718,  0.31753194,  0.0299744 ,\n",
       "           0.10038687,  0.43259948,  0.40719444,  0.22487466,  0.2388065 ,\n",
       "           0.23033708,  0.21546935,  0.00857824,  0.0279956 ,  0.58410025,\n",
       "           0.54413736,  0.472615  ,  0.50316143,  0.6935171 ,  0.65190667,\n",
       "           0.35632148,  0.3866979 ,  0.327687  ,  0.3342591 ,  0.18277475,\n",
       "           0.19046743,  0.2893641 ,  0.30848384,  0.23454173,  0.15972048,\n",
       "           0.21706098,  0.20638116,  0.29758194,  0.35701197,  0.49507433,\n",
       "           0.48164713,  0.67642456,  0.6400459 ,  0.45084256,  0.64369273,\n",
       "           0.39720333,  0.38094622,  0.43137497,  0.53227854,  0.40442967,\n",
       "           0.37881136,  0.42316285,  0.5230156 ,  0.40892023,  0.46212545,\n",
       "           0.24065956,  0.2643627 ,  0.27316603,  0.28364447,  0.35813254,\n",
       "           0.39114776,  0.29567963,  0.26067388,  0.1192532 ,  0.22737382,\n",
       "           0.12193669,  0.13370782,  0.22631022,  0.13172117,  0.16242275,\n",
       "           0.19727963,  0.40545976,  0.40812188,  0.5182719 ,  0.55681777,\n",
       "           0.47583166,  0.34641612,  0.21431518,  0.13757771,  0.22020885,\n",
       "           0.16813429,  0.25794005,  0.12354089,  0.2609314 ,  0.35499114,\n",
       "           0.28013864,  0.24437064,  0.3378443 ,  0.35648763,  0.9709406 ,\n",
       "           0.7896013 ,  0.973513  ,  0.9337661 ,  0.43445432,  0.4502488 ,\n",
       "           0.37602407,  0.38508546,  0.333147  ,  0.3669091 ,  0.4034188 ,\n",
       "           0.36997873,  0.5188626 ,  0.5224619 ,  0.35847935,  0.38047427,\n",
       "           0.13179344,  0.2351695 ,  0.3770166 ,  0.26673618,  0.3886942 ,\n",
       "           0.5086658 ,  0.25707558,  0.22409597,  0.14417988,  0.11823244,\n",
       "           0.09917165,  0.06774497,  0.20961848,  0.19526489,  0.17112991,\n",
       "           0.14881265,  0.5343761 ,  0.47364235,  0.34518194,  0.392792  ,\n",
       "           0.26762325,  0.22163177,  0.19102365,  0.19979712,  0.15519613,\n",
       "           0.23284173,  0.23030503,  0.20181935,  0.18446045,  0.10402004,\n",
       "           0.22236471,  0.25632557,  0.3608402 ,  0.3960034 ,  0.42463306,\n",
       "           0.39456046,  0.52537465,  0.45114404,  0.6000486 ,  0.61693466,\n",
       "           0.34980506,  0.35556003,  0.37162712,  0.31683627,  0.33240837,\n",
       "           0.45786265,  0.4522947 ,  0.43571138,  0.23095532,  0.24475531,\n",
       "           0.17462914,  0.21464415,  0.1486138 ,  0.2098663 ,  0.22383782,\n",
       "           0.13846087,  0.16113737,  0.22081049,  0.15537095,  0.12636286,\n",
       "           0.16778493,  0.20648552,  0.17219824,  0.15959248,  0.3201767 ,\n",
       "           0.47257644,  0.24776545,  0.25652346,  0.12790799,  0.18314368,\n",
       "           0.2748416 ,  0.3688479 ,  0.21944334,  0.22961824,  0.1827558 ,\n",
       "           0.39372638,  0.2657843 ,  0.32276177,  0.5069401 ,  0.5068179 ,\n",
       "           0.7570096 ,  0.59543014,  0.38670942,  0.47349346,  0.44198322,\n",
       "           0.33306167,  0.37887412,  0.28856182,  0.31515586,  0.40973192,\n",
       "           0.45094815,  0.5031802 ,  0.5258972 ,  0.37992626,  0.13975702,\n",
       "           0.08240946,  0.3792289 ,  0.45176536,  0.4436652 ,  0.38161844,\n",
       "           0.13149172,  0.22715485,  0.02906209,  0.15046903,  0.2064721 ,\n",
       "           0.23156098,  0.29737914,  0.21213394,  0.06223172,  0.2311335 ,\n",
       "           0.40443408,  0.16617154,  0.1981639 ,  0.17507643,  0.1602161 ,\n",
       "           0.20729089,  0.244352  ,  0.08406122,  0.26064435,  0.08847617,\n",
       "           0.19576551,  0.20267342,  0.39507544,  0.22315817,  0.41539663,\n",
       "           0.43227714,  0.46566117,  0.45815802,  0.31527233,  0.3163287 ,\n",
       "           0.40262282,  0.34114116,  0.27393764,  0.3648918 ,  0.35550407,\n",
       "           0.33243063,  0.49697202,  0.3991269 ,  0.14037386,  0.20462506,\n",
       "           0.3352784 ,  0.44390947,  0.25972766,  0.184839  ,  0.09635775,\n",
       "           0.22434911,  0.42729396,  0.24013476,  0.20427419,  0.09211519,\n",
       "           0.2526443 ,  0.10982056,  0.1274995 ,  0.10716708,  0.31427887,\n",
       "           0.3030473 ,  0.37238133,  0.4181003 ,  0.17907377,  0.22872002,\n",
       "           0.28999546,  0.14943567,  0.27791935,  0.20979446,  0.26546496,\n",
       "           0.23271307,  0.26934177,  0.17212793,  0.32474768,  0.40073115,\n",
       "           0.23565811,  0.22149256,  0.6011143 ,  0.6218275 ,  0.24513184,\n",
       "           0.13777548,  0.2842021 ,  0.12880066,  0.14936063,  0.16070774,\n",
       "           0.29820323,  0.29488972,  0.27430135,  0.14519534,  0.20490195,\n",
       "           0.21681145,  0.31147546,  0.3438689 ,  0.25354934,  0.26812246,\n",
       "           0.24011843,  0.3889866 ,  0.12800783,  0.27353364,  0.11600715,\n",
       "           0.1340915 ,  0.3144931 ,  0.40940428,  0.2278992 ,  0.19962981,\n",
       "           0.23949982,  0.3717593 ,  0.1317353 ,  0.163624  ,  0.24294804,\n",
       "           0.22669978,  0.24992238,  0.1735603 ,  0.32268086,  0.23255482,\n",
       "           0.16789117,  0.17420289,  0.28230384,  0.25057325,  0.21171673,\n",
       "           0.20279795,  0.21784166,  0.15378192,  0.288457  ,  0.34275305,\n",
       "           0.14079724,  0.19851352,  0.17158246,  0.16576996,  0.18198578,\n",
       "           0.14927143,  0.27743593,  0.19410121,  0.2882682 ,  0.25317574,\n",
       "           0.31498343,  0.22938956,  0.22634242,  0.2057629 ,  0.07969216,\n",
       "           0.1159225 ,  0.28070307,  0.30262473,  0.4875803 ,  0.3456391 ,\n",
       "           0.19763494,  0.18756299,  0.3096291 ,  0.42100978,  0.09954587,\n",
       "           0.12910725,  0.17366734,  0.2556952 , -0.01134405,  0.01095131,\n",
       "           0.43587363,  0.2863691 ,  0.18282929,  0.17490324,  0.17588429,\n",
       "           0.14831558,  0.3383727 ,  0.11672805,  0.20246014,  0.22432598,\n",
       "           0.24631162,  0.23868553,  0.32672465,  0.27683175,  0.16795123,\n",
       "           0.30503926,  0.34460366,  0.2605621 ,  0.5127697 ,  0.30217698,\n",
       "           0.4287073 ,  0.4766429 ,  0.07126538,  0.02799374,  0.4314864 ,\n",
       "           0.23385538,  0.2740164 ,  0.28678998,  0.35993475,  0.46863797,\n",
       "           0.26562563,  0.45692912,  0.286262  ,  0.2508722 ,  0.43804657,\n",
       "           0.42531204,  0.23797329,  0.25308636,  0.07101302,  0.2298222 ,\n",
       "           0.3934126 ,  0.24742423,  0.23916915,  0.2430828 ,  0.23594475,\n",
       "           0.19290078,  0.19903794,  0.0923811 ,  0.38183516,  0.46621597,\n",
       "           0.28809842,  0.1862944 ,  0.13131583,  0.19003305,  0.19641678,\n",
       "           0.16131094,  0.2842257 ,  0.33209166,  0.12279639,  0.14343396,\n",
       "           0.28805506,  0.31333435,  0.23859137,  0.19448562,  0.25632548,\n",
       "           0.19601658,  0.19516867,  0.18019135,  0.22084744,  0.16671315,\n",
       "           0.24203181,  0.17985746,  0.19735663,  0.12067467,  0.2789936 ,\n",
       "           0.43924505,  0.12183824,  0.03573874,  0.33474618,  0.28637648,\n",
       "           0.26071367,  0.10221636,  0.57465446,  0.78444767,  0.3315761 ,\n",
       "           0.18614498,  0.4104604 ,  0.22161622,  0.13333079,  0.13005489,\n",
       "           0.15036324,  0.45083427,  0.03694944,  0.15204932,  0.3642437 ,\n",
       "           0.37394774,  0.36946633,  0.42447618,  0.29365003,  0.2592906 ,\n",
       "           0.16878116,  0.22825865,  0.2851135 ,  0.32070395,  0.20542067,\n",
       "           0.1975664 ,  0.20420216,  0.27161843,  0.14771898,  0.20680633,\n",
       "           0.23091751,  0.2738191 ,  0.1393654 ,  0.07650848,  0.27742404,\n",
       "           0.29023397,  0.23399884,  0.21295983,  0.23596391,  0.2335494 ,\n",
       "           0.21805358,  0.10822646,  0.22842574,  0.18710396,  0.2048128 ,\n",
       "           0.21214291,  0.73034924,  0.86253   ,  0.2886214 ,  0.51118237,\n",
       "           0.33207917,  0.2899674 ,  0.35581368,  0.35366863,  0.32922217,\n",
       "           0.3420562 ,  0.43025404,  0.4612432 ,  0.38524145,  0.3732573 ,\n",
       "           0.31308687,  0.1885564 ,  0.29360527,  0.254387  ,  0.3043313 ,\n",
       "           0.29324943,  0.2931789 ,  0.17208287,  0.17700693,  0.17510244,\n",
       "           0.1781156 ,  0.13758391,  0.2380145 ,  0.17847031,  0.27440366,\n",
       "           0.27328587,  0.44483292,  0.41327202,  0.4224416 ,  0.36177376,\n",
       "           0.423953  ,  0.42641395,  0.46238467,  0.31339157,  0.5659243 ,\n",
       "           0.45830363,  0.34594384,  0.43117595,  0.20048463,  0.1125662 ,\n",
       "           0.32126296,  0.35476997,  0.39235762,  0.38002372,  0.11145698,\n",
       "           0.09152496,  0.16399464,  0.11256412,  0.04850034,  0.00914943,\n",
       "           0.16248804,  0.12423804,  0.17104697,  0.10681517,  0.4464129 ,\n",
       "           0.433406  ,  0.45238054,  0.4134902 ,  0.46545509,  0.43719047,\n",
       "           0.5176772 ,  0.5475533 ,  0.43999535,  0.44341895,  0.32837516,\n",
       "           0.2633377 ,  0.21681085,  0.26342186,  0.30980805,  0.40477365,\n",
       "           0.11532554,  0.26486355,  0.24527514,  0.2915961 ,  0.23307616,\n",
       "           0.15142256,  0.27974415,  0.2303727 ,  0.13623542,  0.24241816,\n",
       "           0.30660304,  0.34638816,  0.36102775,  0.34776816,  0.27433833,\n",
       "           0.3580341 ,  0.31090334,  0.35126534,  0.20538245,  0.18221185,\n",
       "           0.1578757 ,  0.19228992,  0.1393491 ,  0.18492985,  0.06407197,\n",
       "           0.10046734,  0.13521267,  0.17490946,  0.12409304,  0.1384319 ,\n",
       "           0.07408382,  0.12149319,  0.14707237,  0.12793365,  0.43100685,\n",
       "           0.34702665,  0.46322185,  0.3404756 ,  0.30120125,  0.36873525,\n",
       "           0.24741533,  0.31867182,  0.32823896,  0.32521817,  0.50403   ,\n",
       "           0.37840652,  0.22947298,  0.5619137 ,  0.41028324,  0.3436221 ,\n",
       "           0.465371  ,  0.35700706,  0.28940108,  0.26065642,  0.09110093,\n",
       "           0.13063431,  0.25451705,  0.4592359 ,  0.48577785,  0.6214906 ,\n",
       "           0.13881657,  0.14287308,  0.26229593,  0.31055072,  0.16340998,\n",
       "           0.300692  ,  0.17434254,  0.37046763,  0.21529704,  0.2713549 ,\n",
       "           0.23607719,  0.13418035,  0.33791056,  0.08215024,  0.13868272,\n",
       "           0.11899085,  0.41937476,  0.55081093,  0.15646107,  0.18070118,\n",
       "           0.34852338,  0.4127438 ,  0.37545735,  0.36164075,  0.13190246,\n",
       "           0.27349523,  0.2631879 ,  0.13681951,  0.22592956,  0.24594021,\n",
       "           0.2915634 ,  0.29892793,  0.20896944,  0.19360219, -0.01038232,\n",
       "           0.09915888,  0.49711102,  0.4455505 ,  0.402069  ,  0.15740284,\n",
       "          -0.01711613,  0.21831076,  0.23102535,  0.19640669,  0.15844676,\n",
       "           0.1818867 ,  0.19710188,  0.32555625,  0.11205667,  0.08398534,\n",
       "           0.10783841,  0.05515791,  0.18701002,  0.12932803,  0.31622314,\n",
       "           0.34667242,  0.12449032,  0.22427484,  0.308484  ,  0.35566992,\n",
       "           0.19735992,  0.2492465 ,  0.13640219,  0.1949988 ,  0.48428163,\n",
       "           0.3967243 ,  0.29831222,  0.31864268,  0.3623734 ,  0.22206946,\n",
       "           0.26351577,  0.23062415,  0.31348294,  0.22158617,  0.21482475,\n",
       "           0.13218482,  0.49364972,  0.35223624,  0.30173212,  0.21790105,\n",
       "           0.31280822,  0.3651189 ,  0.36733663,  0.309364  ,  0.23084787,\n",
       "           0.1922518 ,  0.43954378,  0.28465164,  0.5307692 ,  0.5608468 ,\n",
       "           0.45262253,  0.4662041 ,  0.06084158,  0.13211352,  0.46602553,\n",
       "           0.4122905 ,  0.24982041,  0.2949683 ,  0.07891703,  0.17302155,\n",
       "           0.4187163 ,  0.4111122 ,  0.07120918,  0.04456539,  0.26773617,\n",
       "           0.21273904], dtype=float32),\n",
       "   'feature_importances': array([0.00826249, 0.01235813, 0.04102311, 0.03126013, 0.06519645,\n",
       "          0.01544047, 0.01346341, 0.06048879, 0.01956667, 0.01899849,\n",
       "          0.02355965, 0.01523121, 0.01557537, 0.01142646, 0.01533097,\n",
       "          0.03290537, 0.0155919 , 0.02390893, 0.02122297, 0.02880135,\n",
       "          0.01399832, 0.00900202, 0.02417308, 0.04856491, 0.0287888 ,\n",
       "          0.01540925, 0.01198195, 0.06976161, 0.01740263, 0.04497966,\n",
       "          0.01485376, 0.01314911, 0.01788379, 0.01216206, 0.01753047,\n",
       "          0.04818081, 0.02731207, 0.03040062, 0.02743134, 0.01742144],\n",
       "         dtype=float32),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 0.0006791631959356874,\n",
       "    'mae': 0.0203094279969541,\n",
       "    'r2': 0.9796326935357412,\n",
       "    'pearson_corr': 0.9900604947612802},\n",
       "   'best_val_score': 0.8031158911648472,\n",
       "   'test_metrics': {'mse': 0.0074071602736818995,\n",
       "    'mae': 0.057016760852192136,\n",
       "    'r2': 0.7592661839145378,\n",
       "    'pearson_corr': 0.8715399935088349,\n",
       "    'connectome_corr': 0.7213574264931617,\n",
       "    'connectome_r2': 0.34033994270403944,\n",
       "    'geodesic_distance': 5.613401736792768},\n",
       "   'y_true': array([ 0.81838 ,  0.81838 ,  0.44134 ,  0.44134 ,  0.3922  ,  0.3922  ,\n",
       "           0.33937 ,  0.33937 ,  0.31612 ,  0.31612 ,  0.34407 ,  0.34407 ,\n",
       "           0.1723  ,  0.1723  ,  0.26053 ,  0.26053 ,  0.9718  ,  0.9718  ,\n",
       "           0.733   ,  0.733   ,  0.40486 ,  0.40486 ,  0.3934  ,  0.3934  ,\n",
       "           0.39635 ,  0.39635 ,  0.39357 ,  0.39357 ,  0.3708  ,  0.3708  ,\n",
       "           0.17249 ,  0.17249 ,  0.13774 ,  0.13774 ,  0.24104 ,  0.24104 ,\n",
       "           0.34322 ,  0.34322 ,  0.22774 ,  0.22774 ,  0.14728 ,  0.14728 ,\n",
       "           0.018301,  0.018301,  0.097647,  0.097647,  0.11349 ,  0.11349 ,\n",
       "           0.14274 ,  0.14274 ,  0.087701,  0.087701,  0.14287 ,  0.14287 ,\n",
       "           0.4315  ,  0.4315  ,  0.4144  ,  0.4144  ,  0.30494 ,  0.30494 ,\n",
       "           0.33556 ,  0.33556 ,  0.35054 ,  0.35054 ,  0.13225 ,  0.13225 ,\n",
       "           0.20896 ,  0.20896 ,  0.88472 ,  0.88472 ,  0.56087 ,  0.56087 ,\n",
       "           0.40877 ,  0.40877 ,  0.40643 ,  0.40643 ,  0.37281 ,  0.37281 ,\n",
       "           0.35164 ,  0.35164 ,  0.39117 ,  0.39117 ,  0.12708 ,  0.12708 ,\n",
       "           0.12771 ,  0.12771 ,  0.23802 ,  0.23802 ,  0.30795 ,  0.30795 ,\n",
       "           0.18453 ,  0.18453 ,  0.11813 ,  0.11813 ,  0.014342,  0.014342,\n",
       "           0.081532,  0.081532,  0.10419 ,  0.10419 ,  0.14018 ,  0.14018 ,\n",
       "           0.070059,  0.070059,  0.13455 ,  0.13455 ,  0.652   ,  0.652   ,\n",
       "           0.31912 ,  0.31912 ,  0.3282  ,  0.3282  ,  0.52059 ,  0.52059 ,\n",
       "           0.16183 ,  0.16183 ,  0.29156 ,  0.29156 ,  0.44222 ,  0.44222 ,\n",
       "           0.4727  ,  0.4727  ,  0.61856 ,  0.61856 ,  0.57925 ,  0.57925 ,\n",
       "           0.70341 ,  0.70341 ,  0.31806 ,  0.31806 ,  0.39637 ,  0.39637 ,\n",
       "           0.12368 ,  0.12368 ,  0.10866 ,  0.10866 ,  0.21268 ,  0.21268 ,\n",
       "           0.37674 ,  0.37674 ,  0.19332 ,  0.19332 ,  0.17454 ,  0.17454 ,\n",
       "           0.025401,  0.025401,  0.10078 ,  0.10078 ,  0.1409  ,  0.1409  ,\n",
       "           0.18789 ,  0.18789 ,  0.083905,  0.083905,  0.15993 ,  0.15993 ,\n",
       "           0.31714 ,  0.31714 ,  0.47498 ,  0.47498 ,  0.46442 ,  0.46442 ,\n",
       "           0.108   ,  0.108   ,  0.17035 ,  0.17035 ,  0.40173 ,  0.40173 ,\n",
       "           0.45769 ,  0.45769 ,  0.49706 ,  0.49706 ,  0.49691 ,  0.49691 ,\n",
       "           0.45997 ,  0.45997 ,  0.32508 ,  0.32508 ,  0.57744 ,  0.57744 ,\n",
       "           0.08625 ,  0.08625 ,  0.11305 ,  0.11305 ,  0.20143 ,  0.20143 ,\n",
       "           0.26651 ,  0.26651 ,  0.12242 ,  0.12242 ,  0.10453 ,  0.10453 ,\n",
       "           0.016032,  0.016032,  0.07877 ,  0.07877 ,  0.1242  ,  0.1242  ,\n",
       "           0.1604  ,  0.1604  ,  0.065636,  0.065636,  0.14085 ,  0.14085 ,\n",
       "           0.61055 ,  0.61055 ,  0.48032 ,  0.48032 ,  0.34818 ,  0.34818 ,\n",
       "           0.13401 ,  0.13401 ,  0.31784 ,  0.31784 ,  0.41468 ,  0.41468 ,\n",
       "           0.31134 ,  0.31134 ,  0.3635  ,  0.3635  ,  0.26366 ,  0.26366 ,\n",
       "           0.72974 ,  0.72974 ,  0.50262 ,  0.50262 ,  0.1968  ,  0.1968  ,\n",
       "           0.063801,  0.063801,  0.31384 ,  0.31384 ,  0.27739 ,  0.27739 ,\n",
       "           0.32239 ,  0.32239 , -0.05647 , -0.05647 ,  0.010482,  0.010482,\n",
       "           0.16089 ,  0.16089 ,  0.18213 ,  0.18213 ,  0.15926 ,  0.15926 ,\n",
       "           0.13811 ,  0.13811 ,  0.13602 ,  0.13602 ,  0.51535 ,  0.51535 ,\n",
       "           0.19514 ,  0.19514 ,  0.026434,  0.026434,  0.31163 ,  0.31163 ,\n",
       "           0.33739 ,  0.33739 ,  0.36784 ,  0.36784 ,  0.48796 ,  0.48796 ,\n",
       "           0.17221 ,  0.17221 ,  0.59915 ,  0.59915 ,  0.9465  ,  0.9465  ,\n",
       "           0.1477  ,  0.1477  ,  0.16485 ,  0.16485 ,  0.36548 ,  0.36548 ,\n",
       "           0.28938 ,  0.28938 ,  0.34738 ,  0.34738 , -0.069929, -0.069929,\n",
       "           0.012004,  0.012004,  0.13788 ,  0.13788 ,  0.19761 ,  0.19761 ,\n",
       "           0.19086 ,  0.19086 ,  0.126   ,  0.126   ,  0.17964 ,  0.17964 ,\n",
       "           0.11124 ,  0.11124 ,  0.17191 ,  0.17191 ,  0.34755 ,  0.34755 ,\n",
       "           0.35721 ,  0.35721 ,  0.43137 ,  0.43137 ,  0.53786 ,  0.53786 ,\n",
       "           0.3672  ,  0.3672  ,  0.43597 ,  0.43597 ,  0.51059 ,  0.51059 ,\n",
       "           0.086054,  0.086054,  0.086132,  0.086132,  0.28085 ,  0.28085 ,\n",
       "           0.31346 ,  0.31346 ,  0.24337 ,  0.24337 ,  0.022592,  0.022592,\n",
       "           0.018079,  0.018079,  0.13688 ,  0.13688 ,  0.18395 ,  0.18395 ,\n",
       "           0.16099 ,  0.16099 ,  0.11672 ,  0.11672 ,  0.14472 ,  0.14472 ,\n",
       "           0.2988  ,  0.2988  ,  0.14685 ,  0.14685 ,  0.20041 ,  0.20041 ,\n",
       "           0.14503 ,  0.14503 ,  0.1079  ,  0.1079  ,  0.1484  ,  0.1484  ,\n",
       "           0.21373 ,  0.21373 ,  0.092073,  0.092073,  0.3208  ,  0.3208  ,\n",
       "           0.15637 ,  0.15637 ,  0.19179 ,  0.19179 ,  0.2362  ,  0.2362  ,\n",
       "           0.23041 ,  0.23041 ,  0.22145 ,  0.22145 ,  0.018007,  0.018007,\n",
       "           0.13568 ,  0.13568 ,  0.097995,  0.097995,  0.13314 ,  0.13314 ,\n",
       "           0.11861 ,  0.11861 ,  0.11165 ,  0.11165 ,  0.25668 ,  0.25668 ,\n",
       "           0.27928 ,  0.27928 ,  0.25164 ,  0.25164 ,  0.17766 ,  0.17766 ,\n",
       "           0.30654 ,  0.30654 ,  0.04025 ,  0.04025 , -0.019243, -0.019243,\n",
       "           0.10886 ,  0.10886 ,  0.079629,  0.079629,  0.037198,  0.037198,\n",
       "           0.55236 ,  0.55236 ,  0.31815 ,  0.31815 ,  0.47026 ,  0.47026 ,\n",
       "           0.023071,  0.023071,  0.096659,  0.096659,  0.07033 ,  0.07033 ,\n",
       "           0.088388,  0.088388,  0.06929 ,  0.06929 ,  0.063706,  0.063706,\n",
       "           0.75899 ,  0.75899 ,  0.4182  ,  0.4182  ,  0.41105 ,  0.41105 ,\n",
       "           0.40217 ,  0.40217 ,  0.38907 ,  0.38907 ,  0.38795 ,  0.38795 ,\n",
       "           0.13881 ,  0.13881 ,  0.1268  ,  0.1268  ,  0.22659 ,  0.22659 ,\n",
       "           0.34833 ,  0.34833 ,  0.20942 ,  0.20942 ,  0.13749 ,  0.13749 ,\n",
       "           0.015259,  0.015259,  0.08026 ,  0.08026 ,  0.1011  ,  0.1011  ,\n",
       "           0.13007 ,  0.13007 ,  0.068697,  0.068697,  0.13054 ,  0.13054 ,\n",
       "           0.41689 ,  0.41689 ,  0.41408 ,  0.41408 ,  0.4566  ,  0.4566  ,\n",
       "           0.52749 ,  0.52749 ,  0.47462 ,  0.47462 ,  0.15207 ,  0.15207 ,\n",
       "           0.10558 ,  0.10558 ,  0.20897 ,  0.20897 ,  0.38513 ,  0.38513 ,\n",
       "           0.22266 ,  0.22266 ,  0.13338 ,  0.13338 ,  0.023153,  0.023153,\n",
       "           0.10571 ,  0.10571 ,  0.12122 ,  0.12122 ,  0.14254 ,  0.14254 ,\n",
       "           0.095759,  0.095759,  0.14333 ,  0.14333 ,  0.68306 ,  0.68306 ,\n",
       "           0.5435  ,  0.5435  ,  0.33372 ,  0.33372 ,  0.48485 ,  0.48485 ,\n",
       "           0.13303 ,  0.13303 ,  0.13193 ,  0.13193 ,  0.24759 ,  0.24759 ,\n",
       "           0.35667 ,  0.35667 ,  0.20544 ,  0.20544 ,  0.16099 ,  0.16099 ,\n",
       "           0.023318,  0.023318,  0.097558,  0.097558,  0.14163 ,  0.14163 ,\n",
       "           0.1726  ,  0.1726  ,  0.084644,  0.084644,  0.1691  ,  0.1691  ,\n",
       "           0.48128 ,  0.48128 ,  0.43514 ,  0.43514 ,  0.67583 ,  0.67583 ,\n",
       "           0.10531 ,  0.10531 ,  0.1171  ,  0.1171  ,  0.25078 ,  0.25078 ,\n",
       "           0.34208 ,  0.34208 ,  0.20078 ,  0.20078 ,  0.068998,  0.068998,\n",
       "           0.018846,  0.018846,  0.09443 ,  0.09443 ,  0.16398 ,  0.16398 ,\n",
       "           0.15269 ,  0.15269 ,  0.082883,  0.082883,  0.14856 ,  0.14856 ,\n",
       "           0.25675 ,  0.25675 ,  0.25601 ,  0.25601 ,  0.10158 ,  0.10158 ,\n",
       "           0.055389,  0.055389,  0.12263 ,  0.12263 ,  0.35918 ,  0.35918 ,\n",
       "           0.16189 ,  0.16189 ,  0.1744  ,  0.1744  ,  0.019853,  0.019853,\n",
       "           0.069583,  0.069583,  0.076881,  0.076881,  0.11079 ,  0.11079 ,\n",
       "           0.053991,  0.053991,  0.10556 ,  0.10556 ,  0.70754 ,  0.70754 ,\n",
       "           0.28536 ,  0.28536 ,  0.12229 ,  0.12229 ,  0.40747 ,  0.40747 ,\n",
       "           0.34977 ,  0.34977 ,  0.45087 ,  0.45087 , -0.029604, -0.029604,\n",
       "           0.012842,  0.012842,  0.14106 ,  0.14106 ,  0.17495 ,  0.17495 ,\n",
       "           0.15435 ,  0.15435 ,  0.14986 ,  0.14986 ,  0.16964 ,  0.16964 ,\n",
       "           0.11954 ,  0.11954 ,  0.15531 ,  0.15531 ,  0.34743 ,  0.34743 ,\n",
       "           0.25542 ,  0.25542 ,  0.2543  ,  0.2543  , -0.076543, -0.076543,\n",
       "           0.01228 ,  0.01228 ,  0.10778 ,  0.10778 ,  0.1859  ,  0.1859  ,\n",
       "           0.1875  ,  0.1875  ,  0.10722 ,  0.10722 ,  0.18352 ,  0.18352 ,\n",
       "           0.32773 ,  0.32773 ,  0.43278 ,  0.43278 ,  0.28433 ,  0.28433 ,\n",
       "           0.34246 ,  0.34246 ,  0.31868 ,  0.31868 ,  0.021865,  0.021865,\n",
       "           0.15572 ,  0.15572 ,  0.10647 ,  0.10647 ,  0.14874 ,  0.14874 ,\n",
       "           0.19075 ,  0.19075 ,  0.17451 ,  0.17451 ,  0.33589 ,  0.33589 ,\n",
       "           0.15461 ,  0.15461 ,  0.12699 ,  0.12699 ,  0.38227 ,  0.38227 ,\n",
       "           0.031151,  0.031151,  0.12646 ,  0.12646 ,  0.10322 ,  0.10322 ,\n",
       "           0.16502 ,  0.16502 ,  0.13441 ,  0.13441 ,  0.17819 ,  0.17819 ,\n",
       "           0.23785 ,  0.23785 ,  0.30948 ,  0.30948 ,  0.17583 ,  0.17583 ,\n",
       "           0.038642,  0.038642,  0.19682 ,  0.19682 ,  0.17838 ,  0.17838 ,\n",
       "           0.21146 ,  0.21146 ,  0.21378 ,  0.21378 ,  0.22637 ,  0.22637 ,\n",
       "           0.60657 ,  0.60657 ,  0.33179 ,  0.33179 ,  0.019819,  0.019819,\n",
       "           0.1191  ,  0.1191  ,  0.10777 ,  0.10777 ,  0.11949 ,  0.11949 ,\n",
       "           0.11785 ,  0.11785 ,  0.12269 ,  0.12269 ,  0.19324 ,  0.19324 ,\n",
       "           0.010799,  0.010799,  0.14439 ,  0.14439 ,  0.12377 ,  0.12377 ,\n",
       "           0.12771 ,  0.12771 ,  0.15287 ,  0.15287 ,  0.13668 ,  0.13668 ,\n",
       "           0.040826,  0.040826,  0.11812 ,  0.11812 ,  0.062458,  0.062458,\n",
       "           0.14295 ,  0.14295 ,  0.1203  ,  0.1203  ,  0.14825 ,  0.14825 ,\n",
       "           0.07886 ,  0.07886 ,  0.060065,  0.060065,  0.04385 ,  0.04385 ,\n",
       "           0.06811 ,  0.06811 ,  0.045071,  0.045071,  0.28453 ,  0.28453 ,\n",
       "           0.25202 ,  0.25202 ,  0.40311 ,  0.40311 ,  0.23479 ,  0.23479 ,\n",
       "           0.22615 ,  0.22615 ,  0.27356 ,  0.27356 ,  0.21588 ,  0.21588 ,\n",
       "           0.23607 ,  0.23607 ,  0.35804 ,  0.35804 ,  0.24593 ,  0.24593 ]),\n",
       "   'y_pred': array([ 5.89375615e-01,  6.52408123e-01,  3.65277618e-01,  4.13745761e-01,\n",
       "           3.36664617e-01,  3.84585500e-01,  4.65253055e-01,  3.41438353e-01,\n",
       "           3.09157670e-01,  4.27115679e-01,  3.95371795e-01,  4.34922636e-01,\n",
       "           3.43645483e-01,  2.31867567e-01,  3.37796271e-01,  2.20581070e-01,\n",
       "           6.60306871e-01,  6.92601562e-01,  5.33894539e-01,  5.22269785e-01,\n",
       "           4.34735596e-01,  4.45584595e-01,  4.62461501e-01,  4.10648704e-01,\n",
       "           2.56452233e-01,  3.22558463e-01,  3.36205959e-01,  4.34358180e-01,\n",
       "           4.69280630e-01,  3.97589684e-01,  1.75516427e-01,  2.33875588e-01,\n",
       "           1.62186652e-01,  1.77907050e-01,  1.76774234e-01,  3.70000899e-01,\n",
       "           3.32525402e-01,  3.26125979e-01,  2.61992931e-01,  1.52846158e-01,\n",
       "           1.14687562e-01,  2.18404382e-01, -4.06762958e-03, -3.02615762e-02,\n",
       "           1.60252482e-01,  5.77707887e-02,  2.97067136e-01,  1.81096628e-01,\n",
       "           1.69563740e-01,  1.41249821e-01,  1.82394534e-01,  1.27696127e-01,\n",
       "           1.48207396e-01,  1.48396403e-01,  3.81878734e-01,  4.59987283e-01,\n",
       "           3.83507192e-01,  4.32250559e-01,  3.66453499e-01,  2.26758227e-01,\n",
       "           3.33990097e-01,  4.00430024e-01,  3.63300204e-01,  4.43334877e-01,\n",
       "           2.03530520e-01,  1.79079607e-01,  2.12803826e-01,  1.77329585e-01,\n",
       "           7.28958249e-01,  6.10923409e-01,  6.80631757e-01,  4.34104860e-01,\n",
       "           3.90657365e-01,  3.84295762e-01,  4.01982307e-01,  3.75474334e-01,\n",
       "           3.34945977e-01,  3.38295817e-01,  2.79286802e-01,  3.72542083e-01,\n",
       "           3.72376740e-01,  4.32506561e-01,  1.39364988e-01,  1.66128367e-01,\n",
       "           1.59808964e-01,  2.44230926e-01,  2.23563328e-01,  4.06023443e-01,\n",
       "           3.53055477e-01,  3.13283324e-01,  2.08481506e-01,  2.09490597e-01,\n",
       "           9.51041132e-02,  1.61448538e-01,  4.70313877e-02, -9.39568877e-03,\n",
       "           1.07419774e-01,  7.81851560e-02,  1.94393039e-01,  1.29623875e-01,\n",
       "           1.59378171e-01,  2.09469065e-01,  9.45102721e-02,  1.50369376e-01,\n",
       "           1.12542391e-01,  1.51805609e-01,  7.11894155e-01,  6.54398918e-01,\n",
       "           2.63683051e-01,  3.61827195e-01,  4.30018246e-01,  5.04217267e-01,\n",
       "           5.03507376e-01,  5.41229486e-01,  1.50723010e-01,  1.91784620e-01,\n",
       "           2.67180681e-01,  2.34292790e-01,  4.29041982e-01,  5.79031706e-01,\n",
       "           5.06221652e-01,  3.89499605e-01,  5.46312928e-01,  5.44082403e-01,\n",
       "           5.68530798e-01,  5.34419179e-01,  6.45675659e-01,  5.42984128e-01,\n",
       "           2.63548702e-01,  3.71532500e-01,  4.57392573e-01,  5.31875551e-01,\n",
       "           1.38487056e-01,  2.25940779e-01,  1.26765773e-01,  1.27071604e-01,\n",
       "           3.62724185e-01,  2.72681147e-01,  3.04321766e-01,  3.20885569e-01,\n",
       "           2.02040315e-01,  2.28314012e-01,  1.20040640e-01,  6.03677183e-02,\n",
       "           5.02536446e-02,  5.32888323e-02,  1.22303680e-01,  2.26890802e-01,\n",
       "           1.56771660e-01,  1.77093267e-01,  1.94840431e-01,  1.54781818e-01,\n",
       "           8.69019926e-02,  1.02689132e-01,  1.89194620e-01,  9.33477730e-02,\n",
       "           3.23096573e-01,  4.28430408e-01,  3.52875978e-01,  4.76523280e-01,\n",
       "           4.24839854e-01,  3.98533344e-01,  1.60264164e-01,  1.46129966e-01,\n",
       "           1.76232114e-01,  1.60109818e-01,  4.69621509e-01,  4.57407176e-01,\n",
       "           4.03179586e-01,  4.27662611e-01,  4.01560456e-01,  5.03842354e-01,\n",
       "           4.22586799e-01,  5.35349905e-01,  5.29879034e-01,  4.27971125e-01,\n",
       "           3.52089852e-01,  3.68097186e-01,  3.61611366e-01,  6.47527218e-01,\n",
       "           4.98158932e-02,  5.96468002e-02,  1.49534553e-01,  2.47332990e-01,\n",
       "           2.88315803e-01,  2.54179567e-01,  2.52139091e-01,  3.42785120e-01,\n",
       "           2.42083356e-01,  1.30347326e-01,  5.97400367e-02,  1.21010706e-01,\n",
       "           8.77141953e-04,  9.24755633e-03,  9.22439843e-02,  6.26469105e-02,\n",
       "           7.56393969e-02,  1.14450648e-01,  1.46243513e-01,  1.66236401e-01,\n",
       "           6.85368031e-02,  8.84201825e-02,  1.75596476e-01,  2.20302939e-01,\n",
       "           1.94477320e-01,  2.60090411e-01,  3.78338933e-01,  2.97861159e-01,\n",
       "           1.79552019e-01,  2.42347345e-01,  2.94923335e-01,  3.53272736e-01,\n",
       "           4.09905136e-01,  3.44707400e-01,  3.03492844e-01,  4.54216182e-01,\n",
       "           4.78510261e-01,  4.65345412e-01,  3.41885835e-01,  4.15399909e-01,\n",
       "           2.75567591e-01,  3.98891926e-01,  3.82405132e-01,  3.63174677e-01,\n",
       "           3.58361244e-01,  2.94876248e-01,  2.99613625e-01,  2.36812800e-01,\n",
       "           1.04528800e-01,  7.19093978e-02,  1.75755516e-01,  2.26763070e-01,\n",
       "           3.07301968e-01,  4.01558518e-01,  3.22037607e-01,  2.49597639e-01,\n",
       "           1.33960605e-01,  2.04141170e-01, -5.86925447e-02, -3.92216444e-02,\n",
       "           7.39905238e-02,  1.18009448e-01,  1.27674565e-01,  1.31806165e-01,\n",
       "           9.46751684e-02,  2.72056878e-01,  1.03360072e-01,  8.20937008e-02,\n",
       "           1.35637105e-01,  8.99830610e-02,  2.09314972e-01,  3.97014797e-01,\n",
       "           1.06689140e-01,  3.05339813e-01,  1.44521058e-01,  2.79467672e-01,\n",
       "           2.83317745e-01,  3.08806807e-01,  1.89027265e-01,  5.10600567e-01,\n",
       "           4.88064647e-01,  3.61260265e-01,  3.60597223e-01,  3.90458465e-01,\n",
       "           2.72106916e-01,  3.07420284e-01,  3.70738626e-01,  4.37118769e-01,\n",
       "           4.66910779e-01,  5.39028883e-01,  2.32704818e-01,  1.64481968e-01,\n",
       "           1.06865406e-01,  8.18257630e-02,  4.70419943e-01,  4.14206803e-01,\n",
       "           5.13010383e-01,  3.71362627e-01,  2.01547191e-01,  2.46310800e-01,\n",
       "           1.40293449e-01,  7.82054216e-02, -7.77946711e-02, -3.48953605e-02,\n",
       "           1.16558164e-01,  7.43308663e-02,  1.27757788e-01,  1.54875368e-01,\n",
       "           1.39976740e-01,  2.01556057e-01,  1.19736955e-01,  8.29184502e-02,\n",
       "           1.61579400e-01,  2.68385381e-01,  1.46303475e-01,  1.49836659e-01,\n",
       "           2.02045828e-01,  2.43269622e-01,  4.05329615e-01,  3.67114395e-01,\n",
       "           4.08607632e-01,  4.19786155e-01,  3.26084793e-01,  4.01275635e-01,\n",
       "           4.34844524e-01,  3.60875726e-01,  3.86404395e-01,  3.40811849e-01,\n",
       "           3.50776941e-01,  3.86434138e-01,  4.21985567e-01,  2.93139935e-01,\n",
       "           1.20706722e-01,  1.35723174e-01,  9.55109596e-02,  7.53394812e-02,\n",
       "           3.35184038e-01,  3.62087607e-01,  3.10354412e-01,  3.12141746e-01,\n",
       "           1.60624146e-01,  3.40425968e-01,  1.80103958e-01,  8.54467303e-02,\n",
       "          -2.55780220e-02, -3.31036747e-02,  1.25031620e-01,  2.20338896e-01,\n",
       "           1.52788237e-01,  1.45144045e-01,  1.61041215e-01,  1.46728620e-01,\n",
       "           5.41920811e-02,  1.21942073e-01,  1.81494474e-01,  1.50010228e-01,\n",
       "           2.63324052e-01,  3.10990304e-01,  2.45758072e-01,  2.02136278e-01,\n",
       "           6.22772753e-01,  3.63002956e-01,  2.28488982e-01,  1.91823065e-01,\n",
       "           1.47669718e-01,  9.14974213e-02,  1.59513518e-01,  1.19186148e-01,\n",
       "           1.95232749e-01,  2.16690928e-01,  1.39874279e-01,  8.21543187e-02,\n",
       "           1.19467303e-01,  2.58211464e-01,  1.52832389e-01,  1.74541041e-01,\n",
       "           1.51564747e-01,  2.00196505e-01,  3.73192817e-01,  3.52943003e-01,\n",
       "           2.87398219e-01,  3.38247001e-01,  1.96005583e-01,  2.94466466e-01,\n",
       "           8.24596286e-02,  4.02091295e-02,  5.70163280e-02,  1.45392329e-01,\n",
       "           1.10617667e-01,  1.07896745e-01,  1.12132043e-01,  1.47756070e-01,\n",
       "           1.05992660e-01,  6.55290335e-02,  1.50100514e-01,  9.32505578e-02,\n",
       "           3.21908832e-01,  2.83776313e-01,  2.49396399e-01,  2.94027060e-01,\n",
       "           2.85639942e-01,  2.82946885e-01,  3.11711282e-01,  2.40670472e-01,\n",
       "           1.58828020e-01,  2.49994978e-01,  1.91053852e-01,  1.49625063e-01,\n",
       "           2.42735937e-01,  2.01534376e-01,  1.77799821e-01,  7.32167661e-02,\n",
       "           1.46509647e-01,  1.62939727e-01,  2.80118972e-01,  2.09493071e-01,\n",
       "           3.60651731e-01,  3.98893535e-01,  2.26377428e-01,  3.96305889e-01,\n",
       "           2.11450055e-01,  3.96108061e-01,  1.27576038e-01,  9.04809535e-02,\n",
       "           1.72797188e-01,  2.03559592e-01,  9.63026881e-02,  6.81656748e-02,\n",
       "           1.23530135e-01,  1.37732029e-01,  8.23813379e-02,  1.63496703e-01,\n",
       "           1.56529844e-01,  1.57488555e-01,  7.18379080e-01,  6.39185131e-01,\n",
       "           4.02934074e-01,  5.07683754e-01,  4.51363772e-01,  5.06218195e-01,\n",
       "           4.20983434e-01,  3.56649399e-01,  3.40025991e-01,  3.25461686e-01,\n",
       "           4.17379916e-01,  4.29693222e-01,  1.28529847e-01,  1.22846588e-01,\n",
       "           8.68201554e-02,  1.55698061e-01,  2.08040833e-01,  3.40370953e-01,\n",
       "           3.27626884e-01,  3.63753855e-01,  2.14090049e-01,  1.81770146e-01,\n",
       "           1.38152242e-01,  1.87249079e-01, -1.83724165e-02,  1.13928914e-02,\n",
       "           5.81842959e-02,  1.68703645e-02,  1.36684716e-01,  1.38378561e-01,\n",
       "           1.25936046e-01,  1.68071225e-01,  8.75019878e-02,  1.44722968e-01,\n",
       "           8.87774974e-02,  1.15059137e-01,  4.38973367e-01,  4.07500356e-01,\n",
       "           4.45568174e-01,  3.65930378e-01,  3.41977984e-01,  4.24669266e-01,\n",
       "           4.92872834e-01,  4.33008611e-01,  4.86472487e-01,  3.48367989e-01,\n",
       "           2.04005152e-01,  3.46046627e-01,  1.66199565e-01,  1.10698462e-01,\n",
       "           3.06491315e-01,  2.32386187e-01,  4.65714365e-01,  3.19109201e-01,\n",
       "           2.35278159e-01,  4.76803780e-01,  2.69676059e-01,  3.05113405e-01,\n",
       "           6.32529706e-02,  4.21804935e-02,  9.07663405e-02,  1.76294476e-01,\n",
       "           8.73467028e-02,  1.20538056e-01,  1.33725137e-01,  1.55971915e-01,\n",
       "           1.24806672e-01,  9.70745236e-02,  1.24765337e-01,  1.15699545e-01,\n",
       "           5.72411239e-01,  5.53945422e-01,  5.39338231e-01,  4.72406447e-01,\n",
       "           4.36205059e-01,  4.14983332e-01,  5.18694401e-01,  4.67362314e-01,\n",
       "           1.42205626e-01,  1.21473625e-01,  1.35937423e-01,  1.65551141e-01,\n",
       "           2.27199733e-01,  3.14239353e-01,  3.47935468e-01,  4.09359455e-01,\n",
       "           2.19055057e-01,  1.84328675e-01,  1.51324227e-01,  2.13232711e-01,\n",
       "           2.29973048e-02,  4.55402732e-02,  9.79976505e-02,  1.25625134e-01,\n",
       "           7.00996220e-02,  1.50960058e-01,  1.48096606e-01,  1.66518748e-01,\n",
       "           8.54486674e-02,  1.08765274e-01,  1.22044921e-01,  2.14485362e-01,\n",
       "           5.84108949e-01,  5.67174256e-01,  3.79390121e-01,  4.21132386e-01,\n",
       "           4.17130768e-01,  4.96643543e-01,  1.61127985e-01,  9.37801749e-02,\n",
       "           8.13255310e-02,  9.75829959e-02,  2.83379197e-01,  3.25001657e-01,\n",
       "           3.70397061e-01,  4.56325650e-01,  1.60625935e-01,  1.61093593e-01,\n",
       "           9.15384442e-02,  1.94757432e-02,  2.72017568e-02,  1.39794201e-02,\n",
       "           1.31787151e-01,  5.01093119e-02,  9.99866873e-02,  1.27876207e-01,\n",
       "           1.20986015e-01,  9.57649350e-02,  1.35724574e-01,  1.01314455e-01,\n",
       "           1.09999344e-01,  1.75088510e-01,  3.77024293e-01,  3.47957253e-01,\n",
       "           2.75552750e-01,  4.58154559e-01,  1.76561102e-01,  9.41734761e-02,\n",
       "           1.07214183e-01,  8.76545459e-02,  1.65343747e-01,  1.64615840e-01,\n",
       "           3.06046396e-01,  2.12482929e-01,  1.38692945e-01,  9.68414247e-02,\n",
       "           1.58212841e-01,  1.28783435e-01,  5.32589853e-02,  1.74354613e-02,\n",
       "           1.02306738e-01,  9.26515162e-02,  1.29204378e-01,  1.21707588e-01,\n",
       "           1.16378725e-01,  1.45124570e-01,  1.93464935e-01,  1.04421273e-01,\n",
       "           8.97475630e-02,  1.42726928e-01,  4.41210687e-01,  3.69591296e-01,\n",
       "           3.64537954e-01,  2.53527075e-01,  5.92765808e-02,  1.18421197e-01,\n",
       "           3.55151534e-01,  3.89896631e-01,  3.50895643e-01,  4.08594251e-01,\n",
       "           4.01904523e-01,  2.98500240e-01,  1.04934067e-01,  3.83233130e-02,\n",
       "          -1.01168066e-01, -1.87761486e-02, -1.48183703e-02,  2.32845843e-02,\n",
       "           1.06609374e-01,  8.45709443e-02,  1.25264540e-01,  1.50748819e-01,\n",
       "           1.57747135e-01,  1.32707983e-01,  3.09917003e-01,  2.75524974e-01,\n",
       "           2.90407449e-01,  1.57582015e-01,  1.34781197e-01,  8.41812491e-02,\n",
       "           3.94989312e-01,  4.16008800e-01,  4.40582901e-01,  5.54598689e-01,\n",
       "           2.99218059e-01,  2.34802455e-01,  1.00791320e-01,  4.22856361e-02,\n",
       "          -5.08001745e-02, -7.00997114e-02,  8.93333554e-03,  7.32638240e-02,\n",
       "           1.91327572e-01,  1.20781630e-01,  1.92048967e-01,  1.23525664e-01,\n",
       "           9.36991423e-02,  1.04842559e-01,  2.10557759e-01,  3.07374269e-01,\n",
       "           2.22277313e-01,  2.75546730e-01,  2.27615401e-01,  3.07234138e-01,\n",
       "           3.67202342e-01,  3.41597199e-01,  5.30673862e-01,  5.08429408e-01,\n",
       "           2.77843565e-01,  3.31821799e-01,  6.84338808e-03,  1.42553598e-02,\n",
       "           6.23963177e-02,  1.66220695e-01,  9.00947303e-02,  8.94749016e-02,\n",
       "           1.09433234e-01,  1.72511369e-01,  1.79367155e-01,  1.48689866e-01,\n",
       "           1.20674267e-01,  1.30974174e-01,  1.69074178e-01,  9.26794857e-02,\n",
       "           2.17809021e-01,  1.97110683e-01,  2.43820027e-01,  2.61148870e-01,\n",
       "           3.86364847e-01,  3.01924378e-01,  4.31457162e-03,  6.68455660e-03,\n",
       "           1.17843628e-01,  9.64766145e-02,  8.33022743e-02,  6.62343949e-02,\n",
       "           1.74547389e-01,  1.39225125e-01,  9.96066481e-02,  6.34001940e-02,\n",
       "           1.41539320e-01,  1.65595487e-01,  3.12892437e-01,  4.40248609e-01,\n",
       "           2.37613767e-01,  3.76600087e-01,  2.55115092e-01,  2.13032484e-01,\n",
       "           3.64803821e-02,  1.71222985e-02,  2.07465619e-01,  1.23216853e-01,\n",
       "           8.98665935e-02,  1.51350290e-01,  1.43932164e-01,  1.69322044e-01,\n",
       "           2.06886515e-01,  1.69994250e-01,  1.71154469e-01,  1.62415788e-01,\n",
       "           2.41133675e-01,  3.21134329e-01,  3.84112030e-01,  3.82205188e-01,\n",
       "           5.26355058e-02, -1.60753727e-04,  2.12614298e-01,  1.29345998e-01,\n",
       "           8.18505138e-02,  1.35336637e-01,  1.10752687e-01,  1.33941501e-01,\n",
       "           1.30121171e-01,  1.40880525e-01,  1.32533282e-01,  1.20260358e-01,\n",
       "           1.02663130e-01,  1.75972283e-01,  5.08397818e-04, -1.88741088e-02,\n",
       "           1.50283992e-01,  1.24670729e-01,  1.75499946e-01,  1.45356834e-01,\n",
       "           7.92536587e-02,  2.19627172e-01,  1.75590128e-01,  1.71494558e-01,\n",
       "           1.33130029e-01,  9.53510404e-02,  8.19921046e-02,  4.93376255e-02,\n",
       "           2.25688338e-01,  1.60463512e-01,  7.80827105e-02,  1.05503887e-01,\n",
       "           1.64530128e-01,  9.84955430e-02,  8.51267427e-02,  1.48461342e-01,\n",
       "           1.00353986e-01,  1.14067882e-01,  7.21953660e-02,  4.38705832e-02,\n",
       "           5.12965918e-02,  4.43307012e-02, -2.20103562e-02,  1.23114139e-02,\n",
       "           6.98855817e-02,  6.69497102e-02,  5.83304465e-02,  1.32279843e-02,\n",
       "           1.31880313e-01,  1.47756875e-01,  1.29875571e-01,  1.45698190e-01,\n",
       "           1.22807994e-01,  1.18462905e-01,  1.40465021e-01,  8.50299150e-02,\n",
       "           1.69063598e-01,  2.51999587e-01,  1.11498013e-01,  2.21742257e-01,\n",
       "           1.53361082e-01,  2.10082188e-01,  1.41275138e-01,  7.72251636e-02,\n",
       "           2.53618777e-01,  1.68197855e-01,  1.13033310e-01,  1.41529381e-01],\n",
       "         dtype=float32),\n",
       "   'feature_importances': array([0.00939197, 0.01302673, 0.0287022 , 0.02685777, 0.06070608,\n",
       "          0.01235096, 0.02151507, 0.0589208 , 0.0189619 , 0.02146692,\n",
       "          0.01981896, 0.01449548, 0.01496711, 0.02032387, 0.01402278,\n",
       "          0.02945313, 0.01722727, 0.02470666, 0.02040267, 0.02771664,\n",
       "          0.01314521, 0.01242199, 0.0344445 , 0.04238289, 0.02739608,\n",
       "          0.0144966 , 0.01277626, 0.07188758, 0.01539471, 0.04900575,\n",
       "          0.01956255, 0.01583831, 0.02239916, 0.01106487, 0.02004867,\n",
       "          0.03445929, 0.02793473, 0.02394727, 0.03514637, 0.02121225],\n",
       "         dtype=float32),\n",
       "   'model_json': None}]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='xgboost',\n",
    "              feature_type=[{'structural': 'spectral_A_20'}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('bayes', 'pearson'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecae9564-5679-4e42-84eb-4842edc0941d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name transcriptome\n",
      "processing_type None\n",
      "X shape (114, 11053)\n",
      "\n",
      " Test fold num: 1\n",
      "(12070, 22106) (12070,) (812, 22106) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "2\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.970, test=0.882) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.970, test=0.721) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.969, test=0.876) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.848, test=0.850) total time=   5.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.845, test=0.730) total time=   5.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.837, test=0.822) total time=   5.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.862, test=0.855) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.858, test=0.725) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.851, test=0.822) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.811, test=0.827) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.807, test=0.642) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.785, test=0.781) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.700, test=0.753) total time=   0.6s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.714, test=0.468) total time=   0.6s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.689, test=0.729) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.896) total time=   5.7s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.803) total time=   5.7s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.872) total time=   5.7s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.942, test=0.886) total time=   0.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.952, test=0.804) total time=   0.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.941, test=0.870) total time=   0.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.893, test=0.873) total time=   4.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.888, test=0.803) total time=   4.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.884, test=0.844) total time=   4.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.741, test=0.786) total time=   2.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.748, test=0.526) total time=   2.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.715, test=0.723) total time=   2.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.754, test=0.782) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.774, test=0.565) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.760, test=0.767) total time=   2.7s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8567301836651571\n",
      "\n",
      "Train Metrics: {'mse': 5.6968078087558433e-05, 'mae': 0.005878787953682761, 'r2': 0.9982958234184567, 'pearson_corr': 0.9991585287750417}\n",
      "Test Metrics: {'mse': 0.0062891323182717625, 'mae': 0.05440465662571298, 'r2': 0.7884484079272344, 'pearson_corr': 0.889278829511946, 'connectome_corr': 0.8348527450479643, 'connectome_r2': 0.6459153754875666, 'geodesic_distance': 5.330889243803316}\n",
      "BEST VAL SCORE 0.8567301836651571\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_112749-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_transcriptome_predFC_random_fold0_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.85673</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_transcriptome_predFC_random_fold0_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_112749-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 43.3%\n",
      "RAM Usage: 7.1%\n",
      "Available RAM: 935.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 34% |\n",
      "\n",
      " Test fold num: 2\n",
      "(12070, 22106) (12070,) (812, 22106) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.962, test=0.784) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.959, test=0.728) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.957, test=0.875) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.802, test=0.786) total time=   5.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.805, test=0.687) total time=   5.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.797, test=0.822) total time=   5.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.809, test=0.775) total time=   2.6s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.809, test=0.677) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.800, test=0.826) total time=   2.6s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.762, test=0.754) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.758, test=0.593) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.749, test=0.806) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.639, test=0.651) total time=   0.6s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.641, test=0.433) total time=   0.6s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.624, test=0.713) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.803) total time=   5.7s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.776) total time=   5.7s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.887) total time=   5.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.932, test=0.806) total time=   0.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.932, test=0.761) total time=   0.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.936, test=0.885) total time=   0.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.846, test=0.813) total time=   4.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.845, test=0.754) total time=   4.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.843, test=0.839) total time=   4.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.689, test=0.690) total time=   2.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.695, test=0.517) total time=   2.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.675, test=0.760) total time=   2.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.712, test=0.686) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.717, test=0.552) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.697, test=0.759) total time=   2.8s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8217112371956888\n",
      "\n",
      "Train Metrics: {'mse': 5.079246184224279e-05, 'mae': 0.005543035661689372, 'r2': 0.9984559646732083, 'pearson_corr': 0.9992393878167726}\n",
      "Test Metrics: {'mse': 0.004686182744299174, 'mae': 0.042075318389049356, 'r2': 0.87625467371531, 'pearson_corr': 0.9367637072325312, 'connectome_corr': 0.8757148918305248, 'connectome_r2': 0.7217305907702226, 'geodesic_distance': 4.270082379912743}\n",
      "BEST VAL SCORE 0.8217112371956888\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_112935-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_transcriptome_predFC_random_fold1_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.82171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_transcriptome_predFC_random_fold1_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_112935-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 42.9%\n",
      "RAM Usage: 7.1%\n",
      "Available RAM: 935.7G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 34% |\n",
      "\n",
      " Test fold num: 3\n",
      "(12126, 22106) (12126,) (756, 22106) (756,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "2\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.971, test=0.805) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.966, test=0.895) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.968, test=0.870) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.846, test=0.794) total time=   5.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.844, test=0.832) total time=   5.3s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.840, test=0.822) total time=   5.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.856, test=0.790) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.851, test=0.835) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.847, test=0.825) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.821, test=0.759) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.822, test=0.817) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.816, test=0.804) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.731, test=0.654) total time=   0.6s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.717, test=0.744) total time=   0.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.713, test=0.726) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.816) total time=   5.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.907) total time=   5.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.874) total time=   5.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.948, test=0.829) total time=   0.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.941, test=0.883) total time=   0.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.942, test=0.885) total time=   0.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.876, test=0.819) total time=   4.0s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.875, test=0.848) total time=   4.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.873, test=0.841) total time=   4.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.772, test=0.692) total time=   2.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.768, test=0.782) total time=   2.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.757, test=0.763) total time=   2.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.782, test=0.701) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.776, test=0.790) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.777, test=0.775) total time=   2.9s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.8), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 7), ('n_estimators', 150), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0), ('reg_lambda', 0.1), ('subsample', 0.6), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8656291771593819\n",
      "\n",
      "Train Metrics: {'mse': 5.8203454388554864e-05, 'mae': 0.005877033071954979, 'r2': 0.9982531601451093, 'pearson_corr': 0.9991390753977303}\n",
      "Test Metrics: {'mse': 0.007098845717824137, 'mae': 0.057735771286982415, 'r2': 0.7734706225879713, 'pearson_corr': 0.8815650687872534, 'connectome_corr': 0.8459289815607882, 'connectome_r2': 0.5175568404711981, 'geodesic_distance': 5.244207421623032}\n",
      "BEST VAL SCORE 0.8656291771593819\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 150, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 0.1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.6, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_113123-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_transcriptome_predFC_random_fold2_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.86563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_transcriptome_predFC_random_fold2_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_113123-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 42.9%\n",
      "RAM Usage: 7.1%\n",
      "Available RAM: 935.5G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 41% |\n",
      "\n",
      " Test fold num: 4\n",
      "(12126, 22106) (12126,) (756, 22106) (756,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "2\n",
      "3\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.960, test=0.793) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.959, test=0.869) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.959, test=0.711) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.781, test=0.753) total time=   4.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.777, test=0.815) total time=   4.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.788, test=0.638) total time=   5.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.802, test=0.759) total time=   2.6s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.795, test=0.826) total time=   2.6s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.802, test=0.648) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.725, test=0.703) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.710, test=0.767) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.735, test=0.520) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.634, test=0.629) total time=   0.6s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.621, test=0.713) total time=   0.6s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.644, test=0.413) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.796) total time=   5.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.893) total time=   5.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.796) total time=   5.9s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.927, test=0.818) total time=   0.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.928, test=0.893) total time=   0.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.933, test=0.790) total time=   0.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.836, test=0.794) total time=   3.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.827, test=0.837) total time=   3.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.832, test=0.728) total time=   3.6s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.660, test=0.653) total time=   2.0s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.649, test=0.728) total time=   2.0s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.679, test=0.434) total time=   2.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.713, test=0.678) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.697, test=0.766) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.722, test=0.534) total time=   2.8s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.6), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 4), ('n_estimators', 50), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0.0001), ('reg_lambda', 0.01), ('subsample', 1), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8335815664109655\n",
      "\n",
      "Train Metrics: {'mse': 0.005917605283539093, 'mae': 0.053143852723005036, 'r2': 0.8225379686861438, 'pearson_corr': 0.9115167685546717}\n",
      "Test Metrics: {'mse': 0.005859427558856006, 'mae': 0.05088605683237315, 'r2': 0.8095677284948799, 'pearson_corr': 0.9031347687099174, 'connectome_corr': 0.7849199824613721, 'connectome_r2': 0.5661209019113081, 'geodesic_distance': 4.265327472286463}\n",
      "BEST VAL SCORE 0.8335815664109655\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 4, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 50, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0.0001, 'reg_lambda': 0.01, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 1, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_113300-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_transcriptome_predFC_random_fold3_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.83358</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_transcriptome_predFC_random_fold3_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_113300-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 44.3%\n",
      "RAM Usage: 7.2%\n",
      "Available RAM: 934.9G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 43% |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 5.6968078087558433e-05,\n",
       "    'mae': 0.005878787953682761,\n",
       "    'r2': 0.9982958234184567,\n",
       "    'pearson_corr': 0.9991585287750417},\n",
       "   'best_val_score': 0.8567301836651571,\n",
       "   'test_metrics': {'mse': 0.0062891323182717625,\n",
       "    'mae': 0.05440465662571298,\n",
       "    'r2': 0.7884484079272344,\n",
       "    'pearson_corr': 0.889278829511946,\n",
       "    'connectome_corr': 0.8348527450479643,\n",
       "    'connectome_r2': 0.6459153754875666,\n",
       "    'geodesic_distance': 5.330889243803316},\n",
       "   'y_true': array([ 0.25718  ,  0.25718  ,  0.21614  ,  0.21614  ,  0.17879  ,\n",
       "           0.17879  ,  0.20003  ,  0.20003  ,  0.18572  ,  0.18572  ,\n",
       "           0.25026  ,  0.25026  ,  0.11669  ,  0.11669  ,  0.24455  ,\n",
       "           0.24455  ,  0.11424  ,  0.11424  ,  0.29925  ,  0.29925  ,\n",
       "           0.18083  ,  0.18083  ,  0.18606  ,  0.18606  ,  0.32224  ,\n",
       "           0.32224  ,  0.18932  ,  0.18932  ,  0.40234  ,  0.40234  ,\n",
       "           0.18223  ,  0.18223  ,  0.26495  ,  0.26495  ,  0.23553  ,\n",
       "           0.23553  ,  0.21608  ,  0.21608  ,  0.059681 ,  0.059681 ,\n",
       "           0.21966  ,  0.21966  ,  0.14268  ,  0.14268  ,  0.22835  ,\n",
       "           0.22835  ,  0.42801  ,  0.42801  ,  0.21521  ,  0.21521  ,\n",
       "           0.057658 ,  0.057658 ,  0.024612 ,  0.024612 ,  0.063759 ,\n",
       "           0.063759 ,  0.37308  ,  0.37308  ,  0.34132  ,  0.34132  ,\n",
       "           0.32791  ,  0.32791  ,  0.27542  ,  0.27542  ,  0.36759  ,\n",
       "           0.36759  ,  0.32945  ,  0.32945  ,  0.19856  ,  0.19856  ,\n",
       "           0.24324  ,  0.24324  ,  0.28955  ,  0.28955  ,  0.30675  ,\n",
       "           0.30675  ,  0.23096  ,  0.23096  ,  0.26071  ,  0.26071  ,\n",
       "           0.73698  ,  0.73698  ,  0.4264   ,  0.4264   ,  0.27143  ,\n",
       "           0.27143  ,  0.45758  ,  0.45758  ,  0.36727  ,  0.36727  ,\n",
       "           0.35085  ,  0.35085  ,  0.25663  ,  0.25663  ,  0.27071  ,\n",
       "           0.27071  ,  0.33455  ,  0.33455  ,  0.20266  ,  0.20266  ,\n",
       "           0.31722  ,  0.31722  ,  0.14571  ,  0.14571  ,  0.027941 ,\n",
       "           0.027941 ,  0.060493 ,  0.060493 ,  0.15441  ,  0.15441  ,\n",
       "           0.6665   ,  0.6665   ,  0.57117  ,  0.57117  ,  0.38161  ,\n",
       "           0.38161  ,  0.37302  ,  0.37302  ,  0.28516  ,  0.28516  ,\n",
       "           0.21945  ,  0.21945  ,  0.17296  ,  0.17296  ,  0.20472  ,\n",
       "           0.20472  ,  0.21497  ,  0.21497  ,  0.19057  ,  0.19057  ,\n",
       "           0.18716  ,  0.18716  ,  0.31581  ,  0.31581  ,  0.36647  ,\n",
       "           0.36647  ,  0.39506  ,  0.39506  ,  0.30871  ,  0.30871  ,\n",
       "           0.41039  ,  0.41039  ,  0.3606   ,  0.3606   ,  0.1418   ,\n",
       "           0.1418   ,  0.13914  ,  0.13914  ,  0.22276  ,  0.22276  ,\n",
       "           0.27225  ,  0.27225  ,  0.24408  ,  0.24408  ,  0.17506  ,\n",
       "           0.17506  ,  0.072852 ,  0.072852 ,  0.051264 ,  0.051264 ,\n",
       "           0.14285  ,  0.14285  ,  0.71373  ,  0.71373  ,  0.51352  ,\n",
       "           0.51352  ,  0.50611  ,  0.50611  ,  0.40024  ,  0.40024  ,\n",
       "           0.21241  ,  0.21241  ,  0.14927  ,  0.14927  ,  0.11245  ,\n",
       "           0.11245  ,  0.17597  ,  0.17597  ,  0.10735  ,  0.10735  ,\n",
       "           0.1207   ,  0.1207   ,  0.29097  ,  0.29097  ,  0.40723  ,\n",
       "           0.40723  ,  0.53582  ,  0.53582  ,  0.33953  ,  0.33953  ,\n",
       "           0.51564  ,  0.51564  ,  0.49405  ,  0.49405  ,  0.15535  ,\n",
       "           0.15535  ,  0.054372 ,  0.054372 ,  0.27296  ,  0.27296  ,\n",
       "           0.26516  ,  0.26516  ,  0.17728  ,  0.17728  ,  0.14978  ,\n",
       "           0.14978  ,  0.081124 ,  0.081124 ,  0.059407 ,  0.059407 ,\n",
       "           0.17772  ,  0.17772  ,  0.41042  ,  0.41042  ,  0.37986  ,\n",
       "           0.37986  ,  0.2326   ,  0.2326   ,  0.23302  ,  0.23302  ,\n",
       "           0.1023   ,  0.1023   ,  0.22117  ,  0.22117  ,  0.18856  ,\n",
       "           0.18856  ,  0.22485  ,  0.22485  ,  0.20126  ,  0.20126  ,\n",
       "           0.27932  ,  0.27932  ,  0.36743  ,  0.36743  ,  0.46038  ,\n",
       "           0.46038  ,  0.30677  ,  0.30677  ,  0.36399  ,  0.36399  ,\n",
       "           0.35942  ,  0.35942  ,  0.081346 ,  0.081346 ,  0.12832  ,\n",
       "           0.12832  ,  0.16343  ,  0.16343  ,  0.30423  ,  0.30423  ,\n",
       "           0.22562  ,  0.22562  ,  0.18438  ,  0.18438  ,  0.10238  ,\n",
       "           0.10238  ,  0.046463 ,  0.046463 ,  0.12617  ,  0.12617  ,\n",
       "           0.57758  ,  0.57758  ,  0.25516  ,  0.25516  ,  0.20396  ,\n",
       "           0.20396  ,  0.082235 ,  0.082235 ,  0.15964  ,  0.15964  ,\n",
       "           0.16099  ,  0.16099  ,  0.13542  ,  0.13542  ,  0.16695  ,\n",
       "           0.16695  ,  0.23593  ,  0.23593  ,  0.40179  ,  0.40179  ,\n",
       "           0.83487  ,  0.83487  ,  0.50918  ,  0.50918  ,  0.49254  ,\n",
       "           0.49254  ,  0.53114  ,  0.53114  ,  0.10013  ,  0.10013  ,\n",
       "           0.10591  ,  0.10591  ,  0.22573  ,  0.22573  ,  0.22525  ,\n",
       "           0.22525  ,  0.1804   ,  0.1804   ,  0.15122  ,  0.15122  ,\n",
       "           0.077285 ,  0.077285 ,  0.032009 ,  0.032009 ,  0.088393 ,\n",
       "           0.088393 ,  0.52482  ,  0.52482  ,  0.17916  ,  0.17916  ,\n",
       "           0.12961  ,  0.12961  ,  0.15284  ,  0.15284  ,  0.23742  ,\n",
       "           0.23742  ,  0.075633 ,  0.075633 ,  0.29751  ,  0.29751  ,\n",
       "           0.26412  ,  0.26412  ,  0.5749   ,  0.5749   ,  0.531    ,\n",
       "           0.531    ,  0.64141  ,  0.64141  ,  0.65289  ,  0.65289  ,\n",
       "           0.82631  ,  0.82631  ,  0.26751  ,  0.26751  ,  0.14327  ,\n",
       "           0.14327  ,  0.41185  ,  0.41185  ,  0.18529  ,  0.18529  ,\n",
       "           0.23687  ,  0.23687  ,  0.12537  ,  0.12537  ,  0.061412 ,\n",
       "           0.061412 ,  0.05069  ,  0.05069  ,  0.16083  ,  0.16083  ,\n",
       "           0.099214 ,  0.099214 ,  0.28009  ,  0.28009  ,  0.14229  ,\n",
       "           0.14229  ,  0.43305  ,  0.43305  ,  0.14596  ,  0.14596  ,\n",
       "           0.24134  ,  0.24134  ,  0.30743  ,  0.30743  ,  0.3538   ,\n",
       "           0.3538   ,  0.22765  ,  0.22765  ,  0.33891  ,  0.33891  ,\n",
       "           0.46497  ,  0.46497  ,  0.45604  ,  0.45604  ,  0.41889  ,\n",
       "           0.41889  ,  0.078087 ,  0.078087 ,  0.38251  ,  0.38251  ,\n",
       "           0.11138  ,  0.11138  ,  0.11192  ,  0.11192  ,  0.049193 ,\n",
       "           0.049193 ,  0.0029833,  0.0029833,  0.064237 ,  0.064237 ,\n",
       "           0.19875  ,  0.19875  ,  0.088349 ,  0.088349 ,  0.31613  ,\n",
       "           0.31613  ,  0.17825  ,  0.17825  ,  0.31984  ,  0.31984  ,\n",
       "           0.20939  ,  0.20939  ,  0.18911  ,  0.18911  ,  0.27698  ,\n",
       "           0.27698  ,  0.20825  ,  0.20825  ,  0.19395  ,  0.19395  ,\n",
       "           0.16338  ,  0.16338  ,  0.15413  ,  0.15413  ,  0.031288 ,\n",
       "           0.031288 ,  0.14008  ,  0.14008  ,  0.090333 ,  0.090333 ,\n",
       "           0.3232   ,  0.3232   ,  0.19489  ,  0.19489  ,  0.21312  ,\n",
       "           0.21312  ,  0.089073 ,  0.089073 ,  0.019102 ,  0.019102 ,\n",
       "           0.057632 ,  0.057632 ,  0.3293   ,  0.3293   ,  0.3667   ,\n",
       "           0.3667   ,  0.21927  ,  0.21927  ,  0.25507  ,  0.25507  ,\n",
       "           0.24062  ,  0.24062  ,  0.15685  ,  0.15685  ,  0.063974 ,\n",
       "           0.063974 ,  0.12795  ,  0.12795  ,  0.14111  ,  0.14111  ,\n",
       "           0.10713  ,  0.10713  ,  0.29691  ,  0.29691  ,  0.31763  ,\n",
       "           0.31763  ,  0.21632  ,  0.21632  ,  0.088533 ,  0.088533 ,\n",
       "           0.19251  ,  0.19251  ,  0.09435  ,  0.09435  , -0.0097822,\n",
       "          -0.0097822,  0.042749 ,  0.042749 ,  0.1148   ,  0.1148   ,\n",
       "           0.63295  ,  0.63295  ,  0.89729  ,  0.89729  ,  0.78426  ,\n",
       "           0.78426  ,  0.25748  ,  0.25748  ,  0.35261  ,  0.35261  ,\n",
       "           0.13637  ,  0.13637  ,  0.12896  ,  0.12896  ,  0.10413  ,\n",
       "           0.10413  ,  0.069807 ,  0.069807 ,  0.29382  ,  0.29382  ,\n",
       "           0.66414  ,  0.66414  ,  0.11436  ,  0.11436  ,  0.33009  ,\n",
       "           0.33009  ,  0.40708  ,  0.40708  ,  0.21692  ,  0.21692  ,\n",
       "           0.046517 ,  0.046517 ,  0.024411 ,  0.024411 ,  0.074275 ,\n",
       "           0.074275 ,  0.49719  ,  0.49719  ,  0.55071  ,  0.55071  ,\n",
       "           0.29181  ,  0.29181  ,  0.22733  ,  0.22733  ,  0.14105  ,\n",
       "           0.14105  ,  0.18604  ,  0.18604  ,  0.15466  ,  0.15466  ,\n",
       "           0.16411  ,  0.16411  ,  0.35593  ,  0.35593  ,  0.45398  ,\n",
       "           0.45398  ,  0.2662   ,  0.2662   ,  0.15843  ,  0.15843  ,\n",
       "           0.23967  ,  0.23967  ,  0.13648  ,  0.13648  ,  0.01001  ,\n",
       "           0.01001  ,  0.041901 ,  0.041901 ,  0.12886  ,  0.12886  ,\n",
       "           0.54538  ,  0.54538  ,  0.23193  ,  0.23193  ,  0.16253  ,\n",
       "           0.16253  ,  0.14732  ,  0.14732  ,  0.037314 ,  0.037314 ,\n",
       "           0.029983 ,  0.029983 ,  0.019508 ,  0.019508 ,  0.039895 ,\n",
       "           0.039895 ,  0.26181  ,  0.26181  , -0.036992 , -0.036992 ,\n",
       "           0.32256  ,  0.32256  ,  0.2233   ,  0.2233   ,  0.18918  ,\n",
       "           0.18918  ,  0.062411 ,  0.062411 ,  0.021565 ,  0.021565 ,\n",
       "           0.072701 ,  0.072701 ,  0.20146  ,  0.20146  ,  0.34138  ,\n",
       "           0.34138  ,  0.14711  ,  0.14711  ,  0.16088  ,  0.16088  ,\n",
       "           0.1457   ,  0.1457   ,  0.16897  ,  0.16897  ,  0.22849  ,\n",
       "           0.22849  ,  0.52415  ,  0.52415  ,  0.19247  ,  0.19247  ,\n",
       "           0.1996   ,  0.1996   ,  0.42029  ,  0.42029  ,  0.19304  ,\n",
       "           0.19304  ,  0.035691 ,  0.035691 ,  0.02462  ,  0.02462  ,\n",
       "           0.0827   ,  0.0827   ,  0.35579  ,  0.35579  ,  0.24985  ,\n",
       "           0.24985  ,  0.44664  ,  0.44664  ,  0.28575  ,  0.28575  ,\n",
       "           0.27349  ,  0.27349  ,  0.22864  ,  0.22864  ,  0.21489  ,\n",
       "           0.21489  ,  0.29219  ,  0.29219  ,  0.20292  ,  0.20292  ,\n",
       "           0.21713  ,  0.21713  ,  0.13414  ,  0.13414  ,  0.033316 ,\n",
       "           0.033316 ,  0.055339 ,  0.055339 ,  0.15933  ,  0.15933  ,\n",
       "           0.43353  ,  0.43353  ,  0.66881  ,  0.66881  ,  0.6345   ,\n",
       "           0.6345   ,  0.58515  ,  0.58515  ,  0.3033   ,  0.3033   ,\n",
       "           0.31446  ,  0.31446  ,  0.40582  ,  0.40582  ,  0.37597  ,\n",
       "           0.37597  ,  0.46799  ,  0.46799  ,  0.1874   ,  0.1874   ,\n",
       "           0.091397 ,  0.091397 ,  0.047718 ,  0.047718 ,  0.14286  ,\n",
       "           0.14286  ,  0.56453  ,  0.56453  ,  0.51335  ,  0.51335  ,\n",
       "           0.5968   ,  0.5968   ,  0.091152 ,  0.091152 ,  0.083658 ,\n",
       "           0.083658 ,  0.24034  ,  0.24034  ,  0.25253  ,  0.25253  ,\n",
       "           0.17331  ,  0.17331  ,  0.15214  ,  0.15214  ,  0.088057 ,\n",
       "           0.088057 ,  0.033493 ,  0.033493 ,  0.10485  ,  0.10485  ,\n",
       "           0.61631  ,  0.61631  ,  0.7151   ,  0.7151   ,  0.29513  ,\n",
       "           0.29513  ,  0.22922  ,  0.22922  ,  0.48951  ,  0.48951  ,\n",
       "           0.1781   ,  0.1781   ,  0.23601  ,  0.23601  ,  0.12201  ,\n",
       "           0.12201  ,  0.057065 ,  0.057065 ,  0.045402 ,  0.045402 ,\n",
       "           0.13086  ,  0.13086  ,  0.69467  ,  0.69467  ,  0.20101  ,\n",
       "           0.20101  ,  0.066433 ,  0.066433 ,  0.30269  ,  0.30269  ,\n",
       "           0.19766  ,  0.19766  ,  0.26341  ,  0.26341  ,  0.1146   ,\n",
       "           0.1146   ,  0.058149 ,  0.058149 ,  0.053463 ,  0.053463 ,\n",
       "           0.13575  ,  0.13575  ,  0.27974  ,  0.27974  ,  0.11393  ,\n",
       "           0.11393  ,  0.44104  ,  0.44104  ,  0.18226  ,  0.18226  ,\n",
       "           0.19928  ,  0.19928  ,  0.10922  ,  0.10922  ,  0.065321 ,\n",
       "           0.065321 ,  0.055716 ,  0.055716 ,  0.16755  ,  0.16755  ,\n",
       "           0.64809  ,  0.64809  ,  0.78581  ,  0.78581  ,  0.027869 ,\n",
       "           0.027869 ,  0.073261 ,  0.073261 ,  0.019392 ,  0.019392 ,\n",
       "          -0.028453 , -0.028453 ,  0.048853 ,  0.048853 ,  0.15552  ,\n",
       "           0.15552  ,  0.47912  ,  0.47912  ,  0.13938  ,  0.13938  ,\n",
       "           0.37744  ,  0.37744  ,  0.14814  ,  0.14814  ,  0.0052557,\n",
       "           0.0052557,  0.029382 ,  0.029382 ,  0.085198 ,  0.085198 ,\n",
       "           0.048122 ,  0.048122 ,  0.12057  ,  0.12057  ,  0.06033  ,\n",
       "           0.06033  ,  0.014264 ,  0.014264 ,  0.054312 ,  0.054312 ,\n",
       "           0.18358  ,  0.18358  ,  0.25977  ,  0.25977  ,  0.18118  ,\n",
       "           0.18118  ,  0.11717  ,  0.11717  ,  0.02445  ,  0.02445  ,\n",
       "           0.074665 ,  0.074665 ,  0.21137  ,  0.21137  ,  0.045916 ,\n",
       "           0.045916 ,  0.027396 ,  0.027396 ,  0.06124  ,  0.06124  ,\n",
       "           0.11414  ,  0.11414  ,  0.028379 ,  0.028379 ,  0.07981  ,\n",
       "           0.07981  ,  0.011463 ,  0.011463 ,  0.04058  ,  0.04058  ,\n",
       "           0.094907 ,  0.094907 ]),\n",
       "   'y_pred': array([ 0.19923303,  0.23738496,  0.21240605,  0.24545945,  0.18165123,\n",
       "           0.16333276,  0.15218002,  0.18421102,  0.18331334,  0.15033628,\n",
       "           0.22031827,  0.18176612,  0.15322664,  0.11321618,  0.15868786,\n",
       "           0.18856204,  0.15172383,  0.1598145 ,  0.30822948,  0.23770589,\n",
       "           0.3088837 ,  0.23945567,  0.212081  ,  0.14320941,  0.29733872,\n",
       "           0.24653322,  0.15582177,  0.22358875,  0.25755855,  0.29867196,\n",
       "           0.17119971,  0.21867138,  0.16830447,  0.16106433,  0.18391205,\n",
       "           0.24912325,  0.15288776,  0.11973192,  0.1237084 ,  0.06169294,\n",
       "           0.33492905,  0.13812113,  0.11128508,  0.06271848,  0.24098517,\n",
       "           0.22317974,  0.37770262,  0.29030135,  0.14489831,  0.22932133,\n",
       "           0.05355825,  0.13337848,  0.03169246,  0.02813329,  0.10325144,\n",
       "           0.09087583,  0.44074237,  0.40533692,  0.391578  ,  0.33196342,\n",
       "           0.37309974,  0.29195526,  0.3660515 ,  0.29497045,  0.3753811 ,\n",
       "           0.40674716,  0.311549  ,  0.2683412 ,  0.215004  ,  0.16630141,\n",
       "           0.20855218,  0.2826996 ,  0.25893846,  0.26977584,  0.23139094,\n",
       "           0.31082806,  0.24840197,  0.24105865,  0.24230915,  0.25678012,\n",
       "           0.97700906,  0.56087816,  0.54161847,  0.43289095,  0.3073508 ,\n",
       "           0.36702144,  0.44539624,  0.56590176,  0.47958523,  0.379835  ,\n",
       "           0.38439134,  0.33420813,  0.25285608,  0.2690076 ,  0.25008968,\n",
       "           0.33271635,  0.31147686,  0.34555417,  0.22972655,  0.20744383,\n",
       "           0.31124932,  0.41298446,  0.19052134,  0.13619862,  0.02394649,\n",
       "           0.03509386,  0.05514224,  0.02660148,  0.16479278,  0.15694518,\n",
       "           0.56578183,  0.51883554,  0.54199636,  0.5388443 ,  0.31798536,\n",
       "           0.35007623,  0.47550052,  0.45855302,  0.26793715,  0.3559655 ,\n",
       "           0.23944618,  0.21764657,  0.15849538,  0.15412459,  0.1331779 ,\n",
       "           0.1400376 ,  0.24304964,  0.16073655,  0.05853836,  0.15135188,\n",
       "           0.19162406,  0.14387694,  0.30949914,  0.4312743 ,  0.3857698 ,\n",
       "           0.40845227,  0.429473  ,  0.47226447,  0.32827067,  0.3034609 ,\n",
       "           0.40763855,  0.3699508 ,  0.4542268 ,  0.4494215 ,  0.22295943,\n",
       "           0.19661705,  0.10877681,  0.15791054,  0.18823898,  0.2351504 ,\n",
       "           0.31831637,  0.36086112,  0.22616437,  0.24320409,  0.15391853,\n",
       "           0.16610917,  0.11818051,  0.05670512,  0.06344607,  0.05201264,\n",
       "           0.17950793,  0.13841653,  0.56552505,  0.51396734,  0.48198575,\n",
       "           0.43918413,  0.49933675,  0.5089027 ,  0.31081998,  0.43279982,\n",
       "           0.15676121,  0.13910306,  0.13946134,  0.1486424 ,  0.24529634,\n",
       "           0.21220577,  0.20107749,  0.25511056,  0.10739839,  0.14437087,\n",
       "           0.14221668,  0.12210718,  0.30441046,  0.2949658 ,  0.4636002 ,\n",
       "           0.58689016,  0.4231848 ,  0.42881265,  0.41153145,  0.3893178 ,\n",
       "           0.55175126,  0.5724337 ,  0.44423595,  0.5786743 ,  0.20518996,\n",
       "           0.3203091 ,  0.1015183 ,  0.03166363,  0.32829863,  0.29223967,\n",
       "           0.3950634 ,  0.31413004,  0.12287876,  0.14034925,  0.14910284,\n",
       "           0.08548144,  0.11011776,  0.0830622 ,  0.05234182,  0.06388217,\n",
       "           0.15409778,  0.12943706,  0.32397372,  0.4082429 ,  0.44324943,\n",
       "           0.42709732,  0.16813372,  0.31089032,  0.256182  ,  0.17446336,\n",
       "           0.08782358,  0.16239509,  0.19163603,  0.13456868,  0.1882261 ,\n",
       "           0.22966614,  0.11484089,  0.1831401 ,  0.16523783,  0.23973264,\n",
       "           0.22930434,  0.36944482,  0.42726618,  0.39341334,  0.41752273,\n",
       "           0.3117132 ,  0.36434683,  0.33745855,  0.546144  ,  0.45798996,\n",
       "           0.44322258,  0.55017376,  0.16749418,  0.2692903 ,  0.14853594,\n",
       "           0.07662331,  0.4158783 ,  0.1741248 ,  0.2866423 ,  0.2630874 ,\n",
       "           0.2552363 ,  0.18766847,  0.16588038,  0.16256261,  0.11686967,\n",
       "           0.11095861,  0.03679259,  0.03107542,  0.10236205,  0.14269584,\n",
       "           0.43699762,  0.4322756 ,  0.293493  ,  0.4035415 ,  0.16033792,\n",
       "           0.16781327,  0.16746198,  0.1365448 ,  0.06659669,  0.20771044,\n",
       "           0.2742743 ,  0.22437477,  0.22490782,  0.16845296,  0.15017024,\n",
       "           0.1768071 ,  0.17270039,  0.40227854,  0.46291274,  0.39741284,\n",
       "           0.39856678,  0.305858  ,  0.5051112 ,  0.33500445,  0.45391327,\n",
       "           0.43314976,  0.5135239 ,  0.3496046 ,  0.20605987,  0.2981861 ,\n",
       "           0.15319286,  0.1064375 ,  0.29713812,  0.39769012,  0.23771991,\n",
       "           0.18843439,  0.1741204 ,  0.11789739,  0.15137881,  0.1748676 ,\n",
       "           0.04560749,  0.08050419,  0.04251328,  0.04124095,  0.12201616,\n",
       "           0.1156033 ,  0.42996502,  0.46068898,  0.20218176,  0.16567367,\n",
       "           0.19036326,  0.17033818,  0.14660805,  0.30946907,  0.23716561,\n",
       "           0.13852794,  0.10355823,  0.10094912,  0.25900966,  0.25603992,\n",
       "           0.372873  ,  0.29224417,  0.47213855,  0.50015175,  0.42221296,\n",
       "           0.48712632,  0.47935   ,  0.64378095,  0.5432079 ,  0.5317679 ,\n",
       "           0.5368959 ,  0.5509972 ,  0.3746198 ,  0.34729075,  0.28859958,\n",
       "           0.15613514,  0.4895898 ,  0.32682   ,  0.13040584,  0.14564675,\n",
       "           0.1052575 ,  0.17430276,  0.1385677 ,  0.1716951 ,  0.08148101,\n",
       "           0.10956931,  0.06336352,  0.05732042,  0.15804645,  0.11669755,\n",
       "           0.2381799 ,  0.10330121,  0.26749614,  0.24270436,  0.17790127,\n",
       "           0.361966  ,  0.3102916 ,  0.31764436,  0.28361768,  0.25915128,\n",
       "           0.37313074,  0.21589203,  0.33472836,  0.22070523,  0.41075307,\n",
       "           0.38004786,  0.15371636,  0.4495438 ,  0.4154809 ,  0.46824604,\n",
       "           0.44092828,  0.27256605,  0.37546337,  0.30905715,  0.25754893,\n",
       "           0.34879172,  0.06102459,  0.19750129,  0.41472626,  0.25957146,\n",
       "           0.22975068,  0.349642  ,  0.21736822,  0.38738117,  0.06913918,\n",
       "           0.0264115 ,  0.03705528,  0.0146713 ,  0.1301958 ,  0.07893626,\n",
       "           0.1505715 ,  0.15263115,  0.10594568,  0.12808524,  0.45503056,\n",
       "           0.29054356,  0.33852237,  0.26245457,  0.2949379 ,  0.26078022,\n",
       "           0.2670102 ,  0.2688245 ,  0.2269606 ,  0.22132483,  0.29711354,\n",
       "           0.30983454,  0.15612003,  0.16428673,  0.10178895,  0.15055272,\n",
       "           0.09793931,  0.1976142 ,  0.13753086,  0.14158924,  0.13151151,\n",
       "           0.05146058,  0.12086765,  0.21060257,  0.03758653,  0.09504782,\n",
       "           0.30831945,  0.21324417,  0.35901612,  0.22946826,  0.1801858 ,\n",
       "           0.21192357,  0.10562347,  0.09503226,  0.04986794,  0.06371021,\n",
       "           0.04555997,  0.08750537,  0.33508846,  0.23076251,  0.29883525,\n",
       "           0.2287592 ,  0.06676839,  0.15484571,  0.30722934,  0.25143024,\n",
       "           0.24871382,  0.2463053 ,  0.2890017 ,  0.26969746,  0.20684172,\n",
       "           0.1084141 ,  0.19258824,  0.11190578,  0.17428905,  0.16238254,\n",
       "           0.2319228 ,  0.17001198,  0.22833714,  0.34473208,  0.31125736,\n",
       "           0.38486195,  0.21503565,  0.23986259,  0.234427  ,  0.16080917,\n",
       "           0.25444078,  0.29758537,  0.0928719 ,  0.13599345, -0.03626457,\n",
       "           0.03816341,  0.04293951,  0.012317  ,  0.15115716,  0.14215241,\n",
       "           0.36583993,  0.5319619 ,  0.6954302 ,  0.5889943 ,  0.5664532 ,\n",
       "           0.67092216,  0.39405707,  0.28775278,  0.35791147,  0.25078577,\n",
       "           0.22763039,  0.16571644,  0.21861568,  0.13902819,  0.20866062,\n",
       "           0.32535052,  0.20989844,  0.16681248,  0.44150895,  0.45597464,\n",
       "           0.6275562 ,  0.50679815,  0.18864441,  0.35075527,  0.3384978 ,\n",
       "           0.3935081 ,  0.33438617,  0.22309555,  0.1398136 ,  0.21115945,\n",
       "           0.03171675,  0.00833017,  0.07470571,  0.02039655,  0.1441067 ,\n",
       "           0.08584477,  0.5074186 ,  0.33222154,  0.44803903,  0.31944206,\n",
       "           0.29280016,  0.37953046,  0.32150602,  0.3456022 ,  0.14736947,\n",
       "           0.18413582,  0.25721732,  0.24964985,  0.2845787 ,  0.21493205,\n",
       "           0.2304443 ,  0.21598464,  0.3667933 ,  0.32500362,  0.50379336,\n",
       "           0.47632754,  0.32026577,  0.35164833,  0.22909094,  0.14614451,\n",
       "           0.15290713,  0.3347506 ,  0.11689876,  0.18458307,  0.03491405,\n",
       "           0.04401669,  0.08114694,  0.06700775,  0.15439354,  0.20207673,\n",
       "           0.38245803,  0.5026852 ,  0.22770235,  0.21945918,  0.04871394,\n",
       "           0.25190258,  0.10521258,  0.07536638,  0.1064859 ,  0.13542204,\n",
       "           0.09976487,  0.11549504,  0.15225102,  0.16542876,  0.30337292,\n",
       "           0.1415365 ,  0.57289875,  0.20329681,  0.33217788,  0.24826927,\n",
       "           0.61858904,  0.22755027,  0.23407607,  0.18761143,  0.13945593,\n",
       "           0.14979047, -0.00527248,  0.02832724,  0.10622776,  0.0351119 ,\n",
       "           0.1178486 ,  0.12799926,  0.26086092,  0.27663386,  0.24887359,\n",
       "           0.28177866,  0.12656574,  0.17960744,  0.17022282,  0.11734037,\n",
       "           0.14700389,  0.13548943,  0.23939924,  0.12528084,  0.31948358,\n",
       "           0.25211912,  0.62736803,  0.4989823 ,  0.14335659,  0.2969396 ,\n",
       "           0.24429916,  0.21517721,  0.38656744,  0.28071886,  0.14056069,\n",
       "           0.15794   ,  0.05617495,  0.0287791 ,  0.03220883,  0.06924935,\n",
       "           0.13914742,  0.13785848,  0.5534346 ,  0.34163463,  0.4310243 ,\n",
       "           0.34561613,  0.60284793,  0.5146478 ,  0.31309843,  0.36937594,\n",
       "           0.33577803,  0.34364572,  0.43766925,  0.2074198 ,  0.48804927,\n",
       "           0.5359928 ,  0.4267701 ,  0.39410412,  0.29212403,  0.22246136,\n",
       "           0.13097034,  0.16255414,  0.1785832 ,  0.15102756,  0.09332125,\n",
       "           0.03125879,  0.05369496,  0.04013978,  0.1641928 ,  0.16886996,\n",
       "           0.43366426,  0.46223283,  0.7009823 ,  0.68090177,  0.45888877,\n",
       "           0.506181  ,  0.62081695,  0.56706756,  0.2396271 ,  0.1784299 ,\n",
       "           0.17250547,  0.17392436,  0.33367586,  0.43274218,  0.3377339 ,\n",
       "           0.27744514,  0.24418378,  0.32010072,  0.20003253,  0.20435148,\n",
       "           0.10980761,  0.06428316,  0.04170437,  0.03739509,  0.1578326 ,\n",
       "           0.15140486,  0.5863004 ,  0.5719296 ,  0.46653092,  0.4419611 ,\n",
       "           0.43405968,  0.4747473 ,  0.1753011 ,  0.34587908,  0.00794299,\n",
       "           0.3122508 ,  0.32591143,  0.27536875,  0.23313327,  0.18209022,\n",
       "           0.2102084 ,  0.08618355,  0.11676393,  0.14208275,  0.01312669,\n",
       "           0.04433675,  0.01891021,  0.07243207,  0.12110341,  0.10313337,\n",
       "           0.44963974,  0.5669752 ,  0.6091281 ,  0.37138197,  0.4506143 ,\n",
       "           0.34967965,  0.17092168,  0.20094132,  0.41471493,  0.49715924,\n",
       "           0.14647621,  0.16747466,  0.18245895,  0.2093195 ,  0.1087846 ,\n",
       "           0.19510695,  0.09310554,  0.01514462,  0.05383494,  0.01858963,\n",
       "           0.10122183,  0.15464273,  0.46099585,  0.56435966,  0.28751087,\n",
       "           0.16511618,  0.18044184,  0.06723255,  0.15771297,  0.25383726,\n",
       "           0.15487668,  0.14555474,  0.24519862,  0.261681  ,  0.12791158,\n",
       "           0.1289126 ,  0.07146047,  0.02880688,  0.04696916,  0.09248896,\n",
       "           0.11447847,  0.13314354,  0.30745658,  0.2904504 ,  0.19890717,\n",
       "           0.2161766 ,  0.5468475 ,  0.40560704,  0.15987927,  0.29464868,\n",
       "           0.13801041,  0.29625896,  0.12596141,  0.14588785,  0.08657499,\n",
       "           0.06942666,  0.08033665,  0.0486849 ,  0.1360434 ,  0.1411937 ,\n",
       "           0.39017257,  0.4712679 ,  0.4158246 ,  0.64936984,  0.16405818,\n",
       "           0.2851238 ,  0.18817833,  0.2912656 ,  0.02519469,  0.07858719,\n",
       "           0.03724022, -0.08480483,  0.06256589,  0.0164059 ,  0.12198827,\n",
       "           0.1367617 ,  0.3453848 ,  0.40102628,  0.22900133,  0.60634017,\n",
       "           0.19105066,  0.23836976,  0.13993835,  0.29978898,  0.00663081,\n",
       "           0.01731634,  0.06817891,  0.00727734,  0.11899987,  0.11164668,\n",
       "           0.18953837,  0.27678347,  0.1233988 ,  0.3093246 ,  0.11104222,\n",
       "           0.15018994,  0.05310449,  0.01272565,  0.06430286,  0.02277544,\n",
       "           0.16989167,  0.14878666,  0.32736826,  0.24180584,  0.17588337,\n",
       "           0.27377674,  0.08323829,  0.09505485,  0.03865916,  0.02713403,\n",
       "           0.06716473,  0.07713386,  0.22597358,  0.21885629,  0.03703699,\n",
       "           0.04525927,  0.05665208,  0.02682857,  0.07490872,  0.12120557,\n",
       "           0.12097837,  0.13217223,  0.03466022,  0.05935901,  0.08306959,\n",
       "           0.08892071, -0.01495582,  0.05036017,  0.03799704,  0.05573222,\n",
       "           0.06528136,  0.09310383], dtype=float32),\n",
       "   'feature_importances': array([1.3919651e-05, 8.5382790e-06, 4.1927005e-06, ..., 0.0000000e+00,\n",
       "          0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 5.079246184224279e-05,\n",
       "    'mae': 0.005543035661689372,\n",
       "    'r2': 0.9984559646732083,\n",
       "    'pearson_corr': 0.9992393878167726},\n",
       "   'best_val_score': 0.8217112371956888,\n",
       "   'test_metrics': {'mse': 0.004686182744299174,\n",
       "    'mae': 0.042075318389049356,\n",
       "    'r2': 0.87625467371531,\n",
       "    'pearson_corr': 0.9367637072325312,\n",
       "    'connectome_corr': 0.8757148918305248,\n",
       "    'connectome_r2': 0.7217305907702226,\n",
       "    'geodesic_distance': 4.270082379912743},\n",
       "   'y_true': array([ 0.71922  ,  0.71922  ,  0.48128  ,  0.48128  ,  0.43659  ,\n",
       "           0.43659  ,  0.32399  ,  0.32399  ,  0.44948  ,  0.44948  ,\n",
       "           0.33167  ,  0.33167  ,  0.43798  ,  0.43798  ,  0.50016  ,\n",
       "           0.50016  ,  0.1345   ,  0.1345   ,  0.26447  ,  0.26447  ,\n",
       "           0.29921  ,  0.29921  ,  0.30794  ,  0.30794  ,  0.3946   ,\n",
       "           0.3946   ,  0.19136  ,  0.19136  ,  0.23954  ,  0.23954  ,\n",
       "           0.92977  ,  0.92977  ,  0.59727  ,  0.59727  ,  0.49227  ,\n",
       "           0.49227  ,  0.42251  ,  0.42251  ,  0.39702  ,  0.39702  ,\n",
       "           0.32595  ,  0.32595  ,  0.12236  ,  0.12236  ,  0.21177  ,\n",
       "           0.21177  ,  0.16332  ,  0.16332  ,  0.2177   ,  0.2177   ,\n",
       "           0.051711 ,  0.051711 ,  0.045002 ,  0.045002 ,  0.027955 ,\n",
       "           0.027955 ,  0.4555   ,  0.4555   ,  0.70545  ,  0.70545  ,\n",
       "           0.35772  ,  0.35772  ,  0.40483  ,  0.40483  ,  0.31522  ,\n",
       "           0.31522  ,  0.3479   ,  0.3479   ,  0.47006  ,  0.47006  ,\n",
       "           0.11232  ,  0.11232  ,  0.21738  ,  0.21738  ,  0.2367   ,\n",
       "           0.2367   ,  0.19925  ,  0.19925  ,  0.42237  ,  0.42237  ,\n",
       "           0.15409  ,  0.15409  ,  0.12644  ,  0.12644  ,  0.76273  ,\n",
       "           0.76273  ,  1.4159   ,  1.4159   ,  0.53017  ,  0.53017  ,\n",
       "           0.48218  ,  0.48218  ,  0.39509  ,  0.39509  ,  0.46872  ,\n",
       "           0.46872  ,  0.11871  ,  0.11871  ,  0.1184   ,  0.1184   ,\n",
       "           0.088568 ,  0.088568 ,  0.10206  ,  0.10206  ,  0.068446 ,\n",
       "           0.068446 ,  0.033191 ,  0.033191 ,  0.02063  ,  0.02063  ,\n",
       "           0.4428   ,  0.4428   ,  0.41565  ,  0.41565  ,  0.34578  ,\n",
       "           0.34578  ,  0.39808  ,  0.39808  ,  0.44063  ,  0.44063  ,\n",
       "           0.46516  ,  0.46516  ,  0.15989  ,  0.15989  ,  0.26702  ,\n",
       "           0.26702  ,  0.33929  ,  0.33929  ,  0.21536  ,  0.21536  ,\n",
       "           0.56331  ,  0.56331  ,  0.25194  ,  0.25194  ,  0.25443  ,\n",
       "           0.25443  ,  0.49248  ,  0.49248  ,  0.39745  ,  0.39745  ,\n",
       "           0.54343  ,  0.54343  ,  0.4951   ,  0.4951   ,  0.54511  ,\n",
       "           0.54511  ,  0.32239  ,  0.32239  ,  0.14092  ,  0.14092  ,\n",
       "           0.23301  ,  0.23301  ,  0.21211  ,  0.21211  ,  0.23812  ,\n",
       "           0.23812  ,  0.096152 ,  0.096152 ,  0.046146 ,  0.046146 ,\n",
       "           0.034861 ,  0.034861 ,  0.61068  ,  0.61068  ,  0.48006  ,\n",
       "           0.48006  ,  0.34539  ,  0.34539  ,  0.37022  ,  0.37022  ,\n",
       "           0.43956  ,  0.43956  ,  0.17585  ,  0.17585  ,  0.49562  ,\n",
       "           0.49562  ,  0.50252  ,  0.50252  ,  0.18569  ,  0.18569  ,\n",
       "           0.54201  ,  0.54201  ,  0.2935   ,  0.2935   ,  0.13164  ,\n",
       "           0.13164  ,  0.45332  ,  0.45332  ,  0.7205   ,  0.7205   ,\n",
       "           0.38752  ,  0.38752  ,  0.43245  ,  0.43245  ,  0.35889  ,\n",
       "           0.35889  ,  0.67894  ,  0.67894  ,  0.1627   ,  0.1627   ,\n",
       "           0.20474  ,  0.20474  ,  0.23652  ,  0.23652  ,  0.10442  ,\n",
       "           0.10442  ,  0.09912  ,  0.09912  ,  0.036825 ,  0.036825 ,\n",
       "           0.020006 ,  0.020006 ,  0.45635  ,  0.45635  ,  0.45603  ,\n",
       "           0.45603  ,  0.38549  ,  0.38549  ,  0.44073  ,  0.44073  ,\n",
       "           0.1256   ,  0.1256   ,  0.40294  ,  0.40294  ,  0.42535  ,\n",
       "           0.42535  ,  0.1231   ,  0.1231   ,  0.41124  ,  0.41124  ,\n",
       "           0.17447  ,  0.17447  ,  0.044144 ,  0.044144 ,  0.34047  ,\n",
       "           0.34047  ,  0.32739  ,  0.32739  ,  0.37778  ,  0.37778  ,\n",
       "           0.49065  ,  0.49065  ,  0.36448  ,  0.36448  ,  0.58298  ,\n",
       "           0.58298  ,  0.11132  ,  0.11132  ,  0.10034  ,  0.10034  ,\n",
       "           0.073646 ,  0.073646 ,  0.013839 ,  0.013839 ,  0.076694 ,\n",
       "           0.076694 ,  0.037086 ,  0.037086 ,  0.0091754,  0.0091754,\n",
       "           0.34648  ,  0.34648  ,  0.43885  ,  0.43885  ,  0.63528  ,\n",
       "           0.63528  ,  0.1177   ,  0.1177   ,  0.44497  ,  0.44497  ,\n",
       "           0.42537  ,  0.42537  ,  0.4355   ,  0.4355   ,  0.39446  ,\n",
       "           0.39446  ,  0.095204 ,  0.095204 ,  0.14335  ,  0.14335  ,\n",
       "           0.43289  ,  0.43289  ,  0.36495  ,  0.36495  ,  0.2908   ,\n",
       "           0.2908   ,  0.36472  ,  0.36472  ,  0.28198  ,  0.28198  ,\n",
       "           0.47213  ,  0.47213  ,  0.10033  ,  0.10033  ,  0.24096  ,\n",
       "           0.24096  ,  0.15704  ,  0.15704  ,  0.14838  ,  0.14838  ,\n",
       "           0.039112 ,  0.039112 ,  0.039563 ,  0.039563 ,  0.023165 ,\n",
       "           0.023165 ,  0.55964  ,  0.55964  ,  0.4745   ,  0.4745   ,\n",
       "           0.12526  ,  0.12526  ,  0.30224  ,  0.30224  ,  0.42563  ,\n",
       "           0.42563  ,  0.22515  ,  0.22515  ,  0.38099  ,  0.38099  ,\n",
       "           0.20657  ,  0.20657  ,  0.032886 ,  0.032886 ,  0.3238   ,\n",
       "           0.3238   ,  0.27486  ,  0.27486  ,  0.3946   ,  0.3946   ,\n",
       "           0.29752  ,  0.29752  ,  0.26246  ,  0.26246  ,  0.36549  ,\n",
       "           0.36549  ,  0.08782  ,  0.08782  ,  0.1119   ,  0.1119   ,\n",
       "          -0.019675 , -0.019675 , -0.0045123, -0.0045123,  0.04498  ,\n",
       "           0.04498  ,  0.055842 ,  0.055842 ,  0.022106 ,  0.022106 ,\n",
       "           0.53023  ,  0.53023  ,  0.16353  ,  0.16353  ,  0.37731  ,\n",
       "           0.37731  ,  0.46037  ,  0.46037  ,  0.35026  ,  0.35026  ,\n",
       "           0.38297  ,  0.38297  ,  0.25752  ,  0.25752  ,  0.23182  ,\n",
       "           0.23182  ,  0.40406  ,  0.40406  ,  0.31176  ,  0.31176  ,\n",
       "           0.40604  ,  0.40604  ,  0.34375  ,  0.34375  ,  0.34606  ,\n",
       "           0.34606  ,  0.35101  ,  0.35101  ,  0.109    ,  0.109    ,\n",
       "           0.20861  ,  0.20861  ,  0.10234  ,  0.10234  ,  0.16967  ,\n",
       "           0.16967  ,  0.044379 ,  0.044379 ,  0.06157  ,  0.06157  ,\n",
       "           0.040003 ,  0.040003 ,  0.12983  ,  0.12983  ,  0.29599  ,\n",
       "           0.29599  ,  0.34645  ,  0.34645  ,  0.36782  ,  0.36782  ,\n",
       "           0.44099  ,  0.44099  ,  0.13153  ,  0.13153  ,  0.1545   ,\n",
       "           0.1545   ,  0.50111  ,  0.50111  ,  0.41965  ,  0.41965  ,\n",
       "           0.45112  ,  0.45112  ,  0.44269  ,  0.44269  ,  0.39682  ,\n",
       "           0.39682  ,  0.38445  ,  0.38445  ,  0.11951  ,  0.11951  ,\n",
       "           0.16574  ,  0.16574  ,  0.079648 ,  0.079648 ,  0.14578  ,\n",
       "           0.14578  ,  0.054895 ,  0.054895 ,  0.049641 ,  0.049641 ,\n",
       "           0.028757 ,  0.028757 ,  0.21889  ,  0.21889  ,  0.25943  ,\n",
       "           0.25943  ,  0.14335  ,  0.14335  ,  0.14767  ,  0.14767  ,\n",
       "           0.25599  ,  0.25599  ,  0.21192  ,  0.21192  ,  0.11635  ,\n",
       "           0.11635  ,  0.1081   ,  0.1081   ,  0.10367  ,  0.10367  ,\n",
       "           0.10679  ,  0.10679  ,  0.11532  ,  0.11532  ,  0.12951  ,\n",
       "           0.12951  ,  0.11851  ,  0.11851  ,  0.14697  ,  0.14697  ,\n",
       "           0.18269  ,  0.18269  ,  0.15903  ,  0.15903  ,  0.043178 ,\n",
       "           0.043178 ,  0.024286 ,  0.024286 ,  0.020426 ,  0.020426 ,\n",
       "           0.96592  ,  0.96592  ,  0.49741  ,  0.49741  ,  0.33026  ,\n",
       "           0.33026  ,  0.38577  ,  0.38577  ,  0.027429 ,  0.027429 ,\n",
       "           0.23037  ,  0.23037  ,  0.19739  ,  0.19739  ,  0.16974  ,\n",
       "           0.16974  ,  0.19702  ,  0.19702  ,  0.17137  ,  0.17137  ,\n",
       "           0.71622  ,  0.71622  ,  0.13794  ,  0.13794  ,  0.50895  ,\n",
       "           0.50895  ,  0.434    ,  0.434    ,  0.0098301,  0.0098301,\n",
       "           0.017404 ,  0.017404 ,  0.039726 ,  0.039726 ,  0.01181  ,\n",
       "           0.01181  ,  0.35224  ,  0.35224  ,  0.40969  ,  0.40969  ,\n",
       "           0.56554  ,  0.56554  ,  0.086979 ,  0.086979 ,  0.26136  ,\n",
       "           0.26136  ,  0.21452  ,  0.21452  ,  0.23154  ,  0.23154  ,\n",
       "           0.22368  ,  0.22368  ,  0.21647  ,  0.21647  ,  0.53709  ,\n",
       "           0.53709  ,  0.14928  ,  0.14928  ,  0.42003  ,  0.42003  ,\n",
       "           0.31317  ,  0.31317  ,  0.02881  ,  0.02881  ,  0.046027 ,\n",
       "           0.046027 ,  0.048635 ,  0.048635 ,  0.016227 ,  0.016227 ,\n",
       "           0.21579  ,  0.21579  ,  0.086568 ,  0.086568 ,  0.22599  ,\n",
       "           0.22599  ,  0.2586   ,  0.2586   ,  0.17552  ,  0.17552  ,\n",
       "           0.13738  ,  0.13738  ,  0.13292  ,  0.13292  ,  0.091804 ,\n",
       "           0.091804 ,  0.2714   ,  0.2714   ,  0.084917 ,  0.084917 ,\n",
       "           0.37345  ,  0.37345  ,  0.22315  ,  0.22315  ,  0.24222  ,\n",
       "           0.24222  , -0.021994 , -0.021994 ,  0.04059  ,  0.04059  ,\n",
       "           0.036565 ,  0.036565 ,  0.4397   ,  0.4397   ,  0.16518  ,\n",
       "           0.16518  ,  0.4114   ,  0.4114   ,  0.3826   ,  0.3826   ,\n",
       "           0.41368  ,  0.41368  ,  0.39789  ,  0.39789  ,  0.38326  ,\n",
       "           0.38326  ,  0.33313  ,  0.33313  ,  0.14608  ,  0.14608  ,\n",
       "           0.2563   ,  0.2563   ,  0.19318  ,  0.19318  ,  0.14108  ,\n",
       "           0.14108  ,  0.10198  ,  0.10198  ,  0.040245 ,  0.040245 ,\n",
       "           0.015939 ,  0.015939 ,  0.24942  ,  0.24942  ,  0.16531  ,\n",
       "           0.16531  ,  0.1696   ,  0.1696   ,  0.1745   ,  0.1745   ,\n",
       "           0.16606  ,  0.16606  ,  0.21919  ,  0.21919  ,  0.15928  ,\n",
       "           0.15928  ,  0.15385  ,  0.15385  ,  0.36641  ,  0.36641  ,\n",
       "           0.35634  ,  0.35634  ,  0.15194  ,  0.15194  ,  0.07519  ,\n",
       "           0.07519  ,  0.039233 ,  0.039233 ,  0.0028819,  0.0028819,\n",
       "           0.19999  ,  0.19999  ,  0.13245  ,  0.13245  ,  0.13954  ,\n",
       "           0.13954  ,  0.17444  ,  0.17444  ,  0.2211   ,  0.2211   ,\n",
       "          -0.040998 , -0.040998 ,  0.14267  ,  0.14267  ,  0.15903  ,\n",
       "           0.15903  ,  0.29571  ,  0.29571  ,  0.71477  ,  0.71477  ,\n",
       "           0.067065 ,  0.067065 ,  0.028937 ,  0.028937 ,  0.058216 ,\n",
       "           0.058216 ,  0.65175  ,  0.65175  ,  0.53779  ,  0.53779  ,\n",
       "           0.47837  ,  0.47837  ,  0.44458  ,  0.44458  ,  0.33217  ,\n",
       "           0.33217  ,  0.11667  ,  0.11667  ,  0.16692  ,  0.16692  ,\n",
       "           0.13881  ,  0.13881  ,  0.18278  ,  0.18278  ,  0.064966 ,\n",
       "           0.064966 ,  0.03759  ,  0.03759  ,  0.021881 ,  0.021881 ,\n",
       "           0.4799   ,  0.4799   ,  0.44247  ,  0.44247  ,  0.35893  ,\n",
       "           0.35893  ,  0.49644  ,  0.49644  ,  0.11726  ,  0.11726  ,\n",
       "           0.10522  ,  0.10522  ,  0.095924 ,  0.095924 ,  0.1036   ,\n",
       "           0.1036   ,  0.066691 ,  0.066691 ,  0.029888 ,  0.029888 ,\n",
       "           0.018513 ,  0.018513 ,  0.55442  ,  0.55442  ,  0.59529  ,\n",
       "           0.59529  ,  0.30399  ,  0.30399  ,  0.10286  ,  0.10286  ,\n",
       "           0.11277  ,  0.11277  ,  0.080131 ,  0.080131 ,  0.10934  ,\n",
       "           0.10934  ,  0.085419 ,  0.085419 ,  0.041231 ,  0.041231 ,\n",
       "           0.021797 ,  0.021797 ,  0.67374  ,  0.67374  ,  0.35077  ,\n",
       "           0.35077  ,  0.10422  ,  0.10422  ,  0.10999  ,  0.10999  ,\n",
       "           0.12321  ,  0.12321  ,  0.14327  ,  0.14327  ,  0.090271 ,\n",
       "           0.090271 ,  0.028932 ,  0.028932 ,  0.019377 ,  0.019377 ,\n",
       "           0.23682  ,  0.23682  ,  0.10727  ,  0.10727  ,  0.1266   ,\n",
       "           0.1266   ,  0.15259  ,  0.15259  ,  0.18575  ,  0.18575  ,\n",
       "           0.10293  ,  0.10293  ,  0.030834 ,  0.030834 ,  0.023963 ,\n",
       "           0.023963 ,  0.13622  ,  0.13622  ,  0.34866  ,  0.34866  ,\n",
       "           0.28463  ,  0.28463  , -0.024828 , -0.024828 ,  0.037177 ,\n",
       "           0.037177 ,  0.035488 ,  0.035488 ,  0.011733 ,  0.011733 ,\n",
       "           0.16488  ,  0.16488  ,  0.19003  ,  0.19003  ,  0.18605  ,\n",
       "           0.18605  ,  0.036091 ,  0.036091 ,  0.017743 ,  0.017743 ,\n",
       "           0.034771 ,  0.034771 ,  0.67479  ,  0.67479  ,  0.27241  ,\n",
       "           0.27241  ,  0.0089888,  0.0089888,  0.032533 ,  0.032533 ,\n",
       "           0.015969 ,  0.015969 ,  0.40099  ,  0.40099  ,  0.047716 ,\n",
       "           0.047716 ,  0.020583 ,  0.020583 ,  0.022499 ,  0.022499 ,\n",
       "           0.057452 ,  0.057452 ,  0.022605 ,  0.022605 ,  0.060679 ,\n",
       "           0.060679 ,  0.010376 ,  0.010376 ,  0.012433 ,  0.012433 ,\n",
       "           0.0095714,  0.0095714]),\n",
       "   'y_pred': array([ 8.50362778e-01,  8.30377102e-01,  4.93458629e-01,  4.82601732e-01,\n",
       "           4.83291984e-01,  4.68350589e-01,  3.32079768e-01,  3.46408367e-01,\n",
       "           4.83228564e-01,  3.47024113e-01,  3.82952720e-01,  3.58944535e-01,\n",
       "           3.57553959e-01,  3.77747238e-01,  4.56595302e-01,  4.58158940e-01,\n",
       "           1.23224705e-01,  1.38856888e-01,  2.78289765e-01,  2.46618822e-01,\n",
       "           3.77597928e-01,  3.22270751e-01,  2.77542293e-01,  2.69386172e-01,\n",
       "           4.09788340e-01,  3.59756768e-01,  1.98837027e-01,  1.72337726e-01,\n",
       "           3.09952289e-01,  1.96452916e-01,  7.43510425e-01,  8.15971971e-01,\n",
       "           7.38398671e-01,  6.86531901e-01,  4.74197716e-01,  4.73424137e-01,\n",
       "           4.44574714e-01,  4.70120132e-01,  4.31710720e-01,  3.82052779e-01,\n",
       "           3.81605506e-01,  3.31290931e-01,  1.57727256e-01,  1.15381882e-01,\n",
       "           1.66914999e-01,  1.58426434e-01,  1.14276111e-01,  1.64010838e-01,\n",
       "           3.05359662e-01,  1.68172538e-01,  5.78083545e-02,  6.52491003e-02,\n",
       "           4.38239872e-02,  4.55717444e-02,  2.42726803e-02,  4.62199152e-02,\n",
       "           4.80201721e-01,  4.69951600e-01,  6.77351415e-01,  6.67716265e-01,\n",
       "           3.37843359e-01,  4.41820353e-01,  5.00200033e-01,  4.91190314e-01,\n",
       "           2.59665906e-01,  3.09472144e-01,  3.21269989e-01,  3.25387776e-01,\n",
       "           4.56047386e-01,  4.29719687e-01,  1.12567157e-01,  1.11341923e-01,\n",
       "           3.55037630e-01,  3.05248499e-01,  3.07706177e-01,  1.74640179e-01,\n",
       "           2.36138582e-01,  1.32436365e-01,  4.45961684e-01,  4.81288821e-01,\n",
       "           1.36815727e-01,  1.66142523e-01,  5.77779710e-02,  1.70982823e-01,\n",
       "           7.13258624e-01,  7.76579857e-01,  9.16791677e-01,  9.47349310e-01,\n",
       "           4.58696634e-01,  5.17063022e-01,  4.42304283e-01,  5.00043511e-01,\n",
       "           3.48377645e-01,  4.38206017e-01,  5.60666561e-01,  5.61366439e-01,\n",
       "           1.10807627e-01,  7.50372559e-02,  1.26804709e-01,  1.31953105e-01,\n",
       "           1.37018263e-01,  1.72348142e-01,  7.29448646e-02,  2.04919562e-01,\n",
       "           8.35462809e-02,  5.17227948e-02,  3.34619582e-02,  3.70792300e-02,\n",
       "           8.23000073e-03, -8.32870603e-03,  4.46869522e-01,  5.04358172e-01,\n",
       "           4.30845618e-01,  4.61882889e-01,  4.36511546e-01,  3.88343573e-01,\n",
       "           4.08020079e-01,  5.30429602e-01,  4.35112596e-01,  4.83272076e-01,\n",
       "           5.17926693e-01,  4.65353012e-01,  1.53958991e-01,  1.87499255e-01,\n",
       "           3.14592987e-01,  2.39120781e-01,  2.88935095e-01,  3.44339013e-01,\n",
       "           2.08287969e-01,  1.80624187e-01,  5.46628833e-01,  5.35507381e-01,\n",
       "           1.91995278e-01,  2.08076149e-01,  2.75762320e-01,  2.08891779e-01,\n",
       "           5.01103282e-01,  5.00475466e-01,  4.73831058e-01,  4.53282446e-01,\n",
       "           5.64633071e-01,  6.07732534e-01,  5.69546819e-01,  5.62115669e-01,\n",
       "           5.89063287e-01,  5.10399580e-01,  3.88866425e-01,  2.58757025e-01,\n",
       "           2.23517001e-01,  1.77349091e-01,  1.71724513e-01,  3.07519943e-01,\n",
       "           1.89782321e-01,  2.34533235e-01,  2.61793286e-01,  2.05709845e-01,\n",
       "           9.35376287e-02,  9.83853638e-02,  6.65453225e-02,  3.42196226e-02,\n",
       "           5.44209778e-02,  1.29527748e-02,  4.98949587e-01,  4.59848344e-01,\n",
       "           4.04509842e-01,  5.14184892e-01,  3.88654947e-01,  4.60059553e-01,\n",
       "           3.01905513e-01,  2.93453217e-01,  4.24818397e-01,  4.34291661e-01,\n",
       "           1.29025519e-01,  1.94869563e-01,  3.09504926e-01,  2.75746644e-01,\n",
       "           2.75006324e-01,  3.47083509e-01,  2.73997158e-01,  3.02116066e-01,\n",
       "           5.20153284e-01,  4.72233057e-01,  2.22894102e-01,  2.12635472e-01,\n",
       "           2.00747415e-01,  1.87083066e-01,  5.32128811e-01,  5.56363702e-01,\n",
       "           6.01726770e-01,  6.68752551e-01,  4.27772403e-01,  3.86023462e-01,\n",
       "           3.90191197e-01,  4.79560673e-01,  4.18020815e-01,  4.19423997e-01,\n",
       "           4.52050745e-01,  4.88208085e-01,  1.31970018e-01,  9.98555124e-02,\n",
       "           2.92768866e-01,  2.03752577e-01,  2.34661326e-01,  1.81188270e-01,\n",
       "           2.03530312e-01,  1.43397272e-01,  1.06338844e-01,  8.87721777e-02,\n",
       "           5.02216965e-02,  2.36191750e-02,  2.82299668e-02,  3.81443053e-02,\n",
       "           5.78968883e-01,  4.38320279e-01,  2.86897570e-01,  4.60320950e-01,\n",
       "           3.01877499e-01,  2.94331819e-01,  4.54335690e-01,  5.79351425e-01,\n",
       "           6.75372481e-02,  1.05499968e-01,  4.07035679e-01,  2.20669687e-01,\n",
       "           2.60114938e-01,  2.88099587e-01,  1.64743289e-01,  1.55077070e-01,\n",
       "           3.98311794e-01,  4.94999796e-01,  7.99553692e-02,  1.72001436e-01,\n",
       "          -7.43898749e-03,  1.04151160e-01,  3.51062745e-01,  3.84354651e-01,\n",
       "           3.70432019e-01,  5.03213763e-01,  4.33331013e-01,  4.45576131e-01,\n",
       "           4.81640279e-01,  5.23722053e-01,  4.60352242e-01,  4.39589530e-01,\n",
       "           5.10918796e-01,  3.90545070e-01,  6.83666915e-02,  4.39352542e-02,\n",
       "           2.07463920e-01,  2.54305452e-02,  1.02737024e-01,  1.36716872e-01,\n",
       "          -1.83363259e-02,  1.50712922e-01,  5.60071468e-02,  6.39360994e-02,\n",
       "           2.28498429e-02,  3.17310393e-02,  3.80677879e-02, -1.26823187e-02,\n",
       "           2.56234258e-01,  3.53462636e-01,  3.67257535e-01,  3.47270131e-01,\n",
       "           4.94231492e-01,  4.49732780e-01,  8.89419466e-02,  7.16952235e-02,\n",
       "           3.88557971e-01,  4.31320131e-01,  3.21905643e-01,  4.65569556e-01,\n",
       "           3.08459699e-01,  2.80057013e-01,  5.02851784e-01,  5.18739641e-01,\n",
       "           1.43251777e-01,  1.03540689e-01,  2.67781973e-01,  1.66640490e-01,\n",
       "           4.07791525e-01,  4.88117635e-01,  4.02047873e-01,  4.16706383e-01,\n",
       "           4.11525130e-01,  3.40767086e-01,  3.07448149e-01,  4.59022343e-01,\n",
       "           3.58160675e-01,  2.93906122e-01,  5.33352613e-01,  4.35734093e-01,\n",
       "           1.57840431e-01,  5.47594279e-02,  2.38900945e-01,  2.48703107e-01,\n",
       "           2.95320362e-01,  2.05128670e-01,  1.91255420e-01,  2.37158746e-01,\n",
       "           3.42562646e-02,  4.19321358e-02,  1.72152668e-02,  3.63917202e-02,\n",
       "           7.90087879e-03,  4.33381945e-02,  4.54986900e-01,  5.56558490e-01,\n",
       "           4.20598239e-01,  4.12187397e-01,  1.39419377e-01,  5.69208115e-02,\n",
       "           3.07853162e-01,  3.72026056e-01,  4.06847894e-01,  3.78350496e-01,\n",
       "           2.17141733e-01,  2.77142495e-01,  4.40220743e-01,  3.84128660e-01,\n",
       "           2.46146694e-01,  2.43248627e-01,  9.86891836e-02,  8.96985531e-02,\n",
       "           3.04536253e-01,  2.25702584e-01,  3.11846763e-01,  3.03482085e-01,\n",
       "           2.15062454e-01,  4.19574082e-01,  3.22475642e-01,  2.47264475e-01,\n",
       "           4.04278100e-01,  2.60399163e-01,  3.95247012e-01,  2.78955460e-01,\n",
       "           1.40772268e-01,  5.74969649e-02,  3.28928858e-01,  2.66226053e-01,\n",
       "           1.19347841e-01,  1.06466979e-01,  5.38930595e-02,  1.38076425e-01,\n",
       "           5.38210422e-02,  2.38558352e-02,  5.31371981e-02,  4.85191047e-02,\n",
       "           1.79448128e-02,  7.45591521e-03,  5.39674461e-01,  4.46429133e-01,\n",
       "           2.10004061e-01,  1.39855593e-01,  3.20656806e-01,  3.41844112e-01,\n",
       "           3.61038506e-01,  2.92463183e-01,  3.03509474e-01,  3.07054251e-01,\n",
       "           4.09218311e-01,  3.64466131e-01,  2.85920084e-01,  2.80623466e-01,\n",
       "           2.03411788e-01,  2.32430369e-01,  3.85510385e-01,  3.63610655e-01,\n",
       "           3.51806998e-01,  3.42951357e-01,  4.40722764e-01,  3.53722632e-01,\n",
       "           3.45571876e-01,  3.12434822e-01,  4.35284436e-01,  3.14095080e-01,\n",
       "           3.11071187e-01,  3.48626792e-01,  1.17500767e-01,  1.06854394e-01,\n",
       "           2.65840024e-01,  2.59798169e-01,  2.00961009e-01,  2.18554541e-01,\n",
       "           2.37105638e-01,  2.15312943e-01,  5.82446456e-02,  4.38300222e-02,\n",
       "           6.30547702e-02,  6.19465411e-02,  5.51577508e-02,  3.84448171e-02,\n",
       "           1.58005953e-01,  1.12350002e-01,  3.33389103e-01,  2.83100814e-01,\n",
       "           3.72074604e-01,  3.85987461e-01,  2.47301340e-01,  3.10278058e-01,\n",
       "           4.26780879e-01,  5.38413465e-01,  1.40248656e-01,  4.40155864e-02,\n",
       "           3.07007492e-01,  2.28678539e-01,  4.54479069e-01,  4.90115106e-01,\n",
       "           4.21075046e-01,  4.12023604e-01,  4.85117167e-01,  5.51936209e-01,\n",
       "           5.26724935e-01,  4.22123849e-01,  4.75007921e-01,  4.01024222e-01,\n",
       "           4.04933155e-01,  4.13541853e-01,  1.22649089e-01,  6.06628358e-02,\n",
       "           1.69141173e-01,  8.75150412e-02,  1.59746051e-01,  1.25240788e-01,\n",
       "           2.01770559e-01,  1.41515821e-01,  5.13624251e-02,  1.02011800e-01,\n",
       "           6.07102066e-02,  3.82923931e-02,  3.15623879e-02,  3.78006995e-02,\n",
       "           1.76953524e-01,  1.17877111e-01,  1.59884036e-01,  1.54823288e-01,\n",
       "           2.00293496e-01,  9.58772749e-02,  1.99586540e-01,  1.57666415e-01,\n",
       "           2.61915177e-01,  1.83368385e-01,  1.95803851e-01,  2.20011279e-01,\n",
       "           8.51928443e-02,  1.08840063e-01,  7.92474151e-02,  1.01022363e-01,\n",
       "           1.27400249e-01,  1.29737690e-01,  9.60634947e-02,  6.29028529e-02,\n",
       "           1.31725639e-01,  1.29103512e-01,  1.37926161e-01,  1.07012227e-01,\n",
       "           1.16932616e-01,  1.18946627e-01,  1.83232278e-01,  1.64529502e-01,\n",
       "           1.60362497e-01,  1.73312634e-01,  1.28559232e-01,  2.02726588e-01,\n",
       "           2.47188658e-02,  5.63503057e-02,  2.66112834e-02,  3.25359404e-02,\n",
       "           6.27055764e-03,  2.69122720e-02,  6.02522373e-01,  5.06766677e-01,\n",
       "           2.72084713e-01,  4.75300640e-01,  3.10805589e-01,  3.17956626e-01,\n",
       "           2.29581892e-01,  3.12433153e-01,  1.73987538e-01,  1.50149778e-01,\n",
       "           2.49792010e-01,  2.64154553e-01,  3.24791789e-01,  3.49441916e-01,\n",
       "           2.06849292e-01,  2.58294016e-01,  2.85583436e-01,  1.91104874e-01,\n",
       "           2.30525702e-01,  1.10864654e-01,  3.93782139e-01,  5.41192770e-01,\n",
       "           5.74827492e-02,  1.28051430e-01,  3.16495419e-01,  4.99658883e-01,\n",
       "           2.11148560e-01,  2.81925440e-01,  1.31291747e-01,  1.55128539e-01,\n",
       "           3.56386006e-02,  1.38781071e-02,  4.06789184e-02,  4.55842763e-02,\n",
       "          -2.58951783e-02,  3.10161561e-02,  3.84703487e-01,  3.43358219e-01,\n",
       "           4.32520747e-01,  3.64046872e-01,  5.49362659e-01,  3.12789589e-01,\n",
       "           1.16878733e-01,  2.85665035e-01,  2.70846307e-01,  2.47941539e-01,\n",
       "           3.93006504e-01,  2.05117702e-01,  2.92681694e-01,  2.95251280e-01,\n",
       "           3.70921403e-01,  2.12756515e-01,  1.77946538e-01,  1.77946270e-01,\n",
       "           4.71244812e-01,  4.01824713e-01,  1.53048620e-01,  5.72967231e-02,\n",
       "           2.50148833e-01,  4.76937622e-01,  2.38258690e-01,  1.69419214e-01,\n",
       "           1.46394551e-01,  1.64968267e-01,  8.94115269e-02,  2.86552906e-02,\n",
       "           5.77502698e-02,  4.08942103e-02,  4.17027324e-02, -9.36284661e-03,\n",
       "           1.62989885e-01,  1.58393055e-01,  2.09699348e-01,  2.49432981e-01,\n",
       "           1.66088551e-01,  3.89133781e-01,  2.57963181e-01,  2.75853992e-01,\n",
       "           1.71554029e-01,  2.70339072e-01,  1.48570359e-01,  1.21421084e-01,\n",
       "           1.95674628e-01,  7.49596059e-02,  1.46952078e-01,  4.66673225e-02,\n",
       "           3.30235362e-01,  2.45653749e-01,  1.02633551e-01,  6.26102984e-02,\n",
       "           3.45101774e-01,  3.89859080e-01,  3.96860123e-01,  1.72542423e-01,\n",
       "           1.90479562e-01,  1.82621360e-01,  2.61161774e-02,  2.11104900e-02,\n",
       "           5.11976182e-02,  6.12187982e-02,  4.16356921e-02,  9.28887427e-02,\n",
       "           3.44758689e-01,  3.37346256e-01,  2.47677520e-01,  1.52194619e-01,\n",
       "           4.27879840e-01,  4.13668275e-01,  4.33548748e-01,  3.83421600e-01,\n",
       "           4.24626410e-01,  3.64121825e-01,  4.61849153e-01,  3.61053824e-01,\n",
       "           3.47223282e-01,  3.37434709e-01,  5.23681879e-01,  3.38180304e-01,\n",
       "           1.26518905e-01,  1.34482652e-01,  2.94057488e-01,  2.89036214e-01,\n",
       "           2.15913787e-01,  2.51687080e-01,  2.16957331e-01,  1.86823279e-01,\n",
       "           1.14314243e-01,  7.91649222e-02,  4.48293239e-02,  3.96244526e-02,\n",
       "           2.17308998e-02,  2.74755061e-02,  2.38883838e-01,  4.28471446e-01,\n",
       "           1.82275981e-01,  1.09636560e-01,  1.37192249e-01,  2.00497136e-01,\n",
       "           6.52509630e-02,  1.92192346e-01,  2.27751672e-01,  1.51522011e-01,\n",
       "           1.49492860e-01,  1.98409498e-01,  8.78292024e-02,  8.44225734e-02,\n",
       "           1.10484287e-01,  1.56682342e-01,  4.55820382e-01,  4.20249254e-01,\n",
       "           2.99482644e-01,  3.69442493e-01,  2.17609003e-01,  3.24854434e-01,\n",
       "           3.23474705e-02,  1.69797689e-02,  3.29775810e-02,  3.79527509e-02,\n",
       "          -1.16682053e-03,  1.24579221e-02,  2.46674702e-01,  1.89089715e-01,\n",
       "           1.63679719e-01,  8.07923526e-02,  1.96377724e-01,  2.16965243e-01,\n",
       "           1.65638775e-01,  2.11406007e-01,  2.26038754e-01,  2.12815672e-01,\n",
       "           5.74489683e-02,  1.34921700e-01,  2.31359720e-01,  1.51551694e-01,\n",
       "           2.50548959e-01,  2.02476516e-01,  2.78692961e-01,  2.24764049e-01,\n",
       "           2.70602196e-01,  3.44996035e-01,  1.15508124e-01,  7.87022710e-02,\n",
       "           4.12331372e-02,  3.31670344e-02,  4.53722030e-02,  5.68130612e-02,\n",
       "           6.32570565e-01,  5.99933743e-01,  4.45468932e-01,  4.92424965e-01,\n",
       "           5.70527315e-01,  4.80301678e-01,  3.76387089e-01,  4.16626692e-01,\n",
       "           5.40847301e-01,  4.09928739e-01,  1.54510379e-01,  6.57861382e-02,\n",
       "           6.12903386e-02,  2.00922757e-01,  1.94083661e-01,  1.64409295e-01,\n",
       "           1.73714995e-01,  2.80496895e-01,  5.28437644e-02,  6.71282709e-02,\n",
       "           1.42739713e-02,  4.17682678e-02,  3.57371569e-03,  5.04082441e-02,\n",
       "           3.50726038e-01,  4.45844531e-01,  4.53502715e-01,  4.70509678e-01,\n",
       "           3.99013042e-01,  3.07658255e-01,  5.72373629e-01,  4.31825668e-01,\n",
       "           8.97461772e-02,  5.88498414e-02,  8.80418718e-02,  1.42683178e-01,\n",
       "           1.53875202e-01,  1.74837172e-01,  1.16069585e-01,  1.40546471e-01,\n",
       "           3.70759517e-02,  2.36451924e-02,  2.27437466e-02,  2.70473063e-02,\n",
       "          -4.70221043e-04,  1.41598880e-02,  5.88051796e-01,  6.28629982e-01,\n",
       "           5.61785102e-01,  6.05256557e-01,  2.91366637e-01,  3.40372801e-01,\n",
       "           1.46399617e-01,  1.07511014e-01,  1.05018631e-01,  1.36569306e-01,\n",
       "           2.05554664e-02,  1.54458240e-01,  1.90501168e-01,  2.21546680e-01,\n",
       "           5.80581576e-02,  6.24078214e-02,  3.24012935e-02,  3.91603559e-02,\n",
       "           1.72710270e-02,  7.13425279e-02,  5.86424232e-01,  5.46458900e-01,\n",
       "           3.17946851e-01,  3.26436490e-01,  7.53494948e-02,  6.83985800e-02,\n",
       "           1.05356514e-01,  8.79651755e-02,  1.49174497e-01,  1.48904920e-01,\n",
       "           1.86101675e-01,  1.83935180e-01,  9.18837190e-02,  1.00277275e-01,\n",
       "           2.69554108e-02,  3.31754982e-02,  1.73631459e-02,  9.85221565e-03,\n",
       "           2.15261251e-01,  2.81422764e-01,  1.14460617e-01,  1.16545558e-01,\n",
       "           1.01472721e-01, -9.34812427e-03,  1.07014850e-01,  1.37344211e-01,\n",
       "           1.83163851e-01,  1.70168996e-01,  5.62600642e-02,  9.79747623e-02,\n",
       "           3.65980119e-02,  6.06030524e-02,  1.85338706e-02, -2.58417428e-02,\n",
       "           7.53858835e-02,  1.64154902e-01,  5.16789556e-01,  2.83139229e-01,\n",
       "           2.22256541e-01,  3.43751281e-01,  1.41079947e-01,  1.12758175e-01,\n",
       "           5.93044758e-02,  3.83634716e-02,  5.75827807e-02,  3.41891944e-02,\n",
       "           4.18248475e-02, -1.43578649e-03,  1.26102760e-01,  1.60628974e-01,\n",
       "           1.74535930e-01,  8.01716447e-02,  1.62010938e-01,  2.55404085e-01,\n",
       "           3.24089974e-02,  2.14481503e-02, -8.67351890e-03,  3.93457711e-03,\n",
       "          -1.12744272e-02,  1.41573846e-02,  4.60876107e-01,  3.69574368e-01,\n",
       "           3.16826701e-01,  1.86621875e-01,  8.20729136e-03,  1.53984129e-02,\n",
       "           1.85114741e-02,  3.22992951e-02, -7.13071227e-03,  2.41884142e-02,\n",
       "           4.35642779e-01,  4.27960783e-01,  8.70766640e-02,  3.64826620e-02,\n",
       "           4.06639278e-02,  3.24042737e-02,  4.19463366e-02,  1.24918967e-02,\n",
       "           1.10111013e-01,  5.73934168e-02,  5.13519347e-02, -4.47273254e-04,\n",
       "           3.38723511e-02,  6.24124706e-03,  3.99397314e-03,  1.34019107e-02,\n",
       "           7.35381246e-03,  3.35295051e-02,  3.10223252e-02,  4.44487780e-02],\n",
       "         dtype=float32),\n",
       "   'feature_importances': array([1.37714687e-05, 1.22742495e-05, 5.69309350e-06, ...,\n",
       "          0.00000000e+00, 1.47582887e-05, 5.65335085e-07], dtype=float32),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.8,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 7,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 150,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0,\n",
       "    'reg_lambda': 0.1,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 0.6,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 5.8203454388554864e-05,\n",
       "    'mae': 0.005877033071954979,\n",
       "    'r2': 0.9982531601451093,\n",
       "    'pearson_corr': 0.9991390753977303},\n",
       "   'best_val_score': 0.8656291771593819,\n",
       "   'test_metrics': {'mse': 0.007098845717824137,\n",
       "    'mae': 0.057735771286982415,\n",
       "    'r2': 0.7734706225879713,\n",
       "    'pearson_corr': 0.8815650687872534,\n",
       "    'connectome_corr': 0.8459289815607882,\n",
       "    'connectome_r2': 0.5175568404711981,\n",
       "    'geodesic_distance': 5.244207421623032},\n",
       "   'y_true': array([ 0.33148  ,  0.33148  ,  0.25656  ,  0.25656  ,  0.20587  ,\n",
       "           0.20587  ,  0.31174  ,  0.31174  ,  0.21878  ,  0.21878  ,\n",
       "           0.21532  ,  0.21532  ,  0.17031  ,  0.17031  ,  0.2146   ,\n",
       "           0.2146   ,  0.17066  ,  0.17066  ,  0.22792  ,  0.22792  ,\n",
       "           0.16084  ,  0.16084  ,  0.35445  ,  0.35445  ,  0.29327  ,\n",
       "           0.29327  ,  0.27628  ,  0.27628  ,  0.16877  ,  0.16877  ,\n",
       "           0.25129  ,  0.25129  ,  0.23922  ,  0.23922  ,  0.24166  ,\n",
       "           0.24166  ,  0.23041  ,  0.23041  ,  0.1454   ,  0.1454   ,\n",
       "           0.2518   ,  0.2518   ,  0.2819   ,  0.2819   ,  0.1944   ,\n",
       "           0.1944   ,  0.20706  ,  0.20706  ,  0.18931  ,  0.18931  ,\n",
       "           0.1788   ,  0.1788   ,  0.09972  ,  0.09972  ,  0.73677  ,\n",
       "           0.73677  ,  0.52629  ,  0.52629  ,  0.64905  ,  0.64905  ,\n",
       "           0.37377  ,  0.37377  ,  0.28208  ,  0.28208  ,  0.17852  ,\n",
       "           0.17852  ,  0.13445  ,  0.13445  ,  0.23099  ,  0.23099  ,\n",
       "           0.18371  ,  0.18371  ,  0.32211  ,  0.32211  ,  0.5291   ,\n",
       "           0.5291   ,  0.74704  ,  0.74704  ,  0.49516  ,  0.49516  ,\n",
       "           0.3926   ,  0.3926   ,  0.57732  ,  0.57732  ,  0.39736  ,\n",
       "           0.39736  ,  0.46903  ,  0.46903  ,  0.45497  ,  0.45497  ,\n",
       "           0.25314  ,  0.25314  ,  0.28381  ,  0.28381  ,  0.29232  ,\n",
       "           0.29232  ,  0.12649  ,  0.12649  ,  0.17147  ,  0.17147  ,\n",
       "           0.11213  ,  0.11213  ,  0.14714  ,  0.14714  ,  0.16611  ,\n",
       "           0.16611  ,  0.5164   ,  0.5164   ,  0.55261  ,  0.55261  ,\n",
       "           0.36462  ,  0.36462  ,  0.23562  ,  0.23562  ,  0.18814  ,\n",
       "           0.18814  ,  0.10769  ,  0.10769  ,  0.27835  ,  0.27835  ,\n",
       "           0.16852  ,  0.16852  ,  0.2832   ,  0.2832   ,  0.99059  ,\n",
       "           0.99059  ,  1.3686   ,  1.3686   ,  0.51897  ,  0.51897  ,\n",
       "           0.39011  ,  0.39011  ,  0.46333  ,  0.46333  ,  0.42757  ,\n",
       "           0.42757  ,  0.5324   ,  0.5324   ,  0.49968  ,  0.49968  ,\n",
       "           0.18134  ,  0.18134  ,  0.34009  ,  0.34009  ,  0.35677  ,\n",
       "           0.35677  ,  0.088168 ,  0.088168 ,  0.15894  ,  0.15894  ,\n",
       "           0.056894 ,  0.056894 ,  0.12265  ,  0.12265  ,  0.14579  ,\n",
       "           0.14579  ,  0.39599  ,  0.39599  ,  0.32793  ,  0.32793  ,\n",
       "           0.3222   ,  0.3222   ,  0.17706  ,  0.17706  ,  0.17992  ,\n",
       "           0.17992  ,  0.25264  ,  0.25264  ,  0.22991  ,  0.22991  ,\n",
       "           0.25632  ,  0.25632  ,  0.39837  ,  0.39837  ,  0.49432  ,\n",
       "           0.49432  ,  0.57933  ,  0.57933  ,  0.6152   ,  0.6152   ,\n",
       "           0.38098  ,  0.38098  ,  0.30403  ,  0.30403  ,  0.3698   ,\n",
       "           0.3698   ,  0.43567  ,  0.43567  ,  0.2435   ,  0.2435   ,\n",
       "           0.15002  ,  0.15002  ,  0.18289  ,  0.18289  ,  0.14929  ,\n",
       "           0.14929  ,  0.17282  ,  0.17282  ,  0.14255  ,  0.14255  ,\n",
       "           0.16803  ,  0.16803  ,  0.20912  ,  0.20912  ,  0.31918  ,\n",
       "           0.31918  ,  0.15832  ,  0.15832  ,  0.14124  ,  0.14124  ,\n",
       "           0.12168  ,  0.12168  ,  0.21204  ,  0.21204  ,  0.090976 ,\n",
       "           0.090976 ,  0.34417  ,  0.34417  ,  0.48272  ,  0.48272  ,\n",
       "           0.57398  ,  0.57398  ,  0.37793  ,  0.37793  ,  0.27984  ,\n",
       "           0.27984  ,  0.39158  ,  0.39158  ,  0.39081  ,  0.39081  ,\n",
       "           0.46012  ,  0.46012  ,  0.42524  ,  0.42524  ,  0.1816   ,\n",
       "           0.1816   ,  0.34134  ,  0.34134  ,  0.38275  ,  0.38275  ,\n",
       "           0.05783  ,  0.05783  ,  0.17506  ,  0.17506  ,  0.020916 ,\n",
       "           0.020916 ,  0.21375  ,  0.21375  ,  0.13493  ,  0.13493  ,\n",
       "           0.18505  ,  0.18505  ,  0.23936  ,  0.23936  ,  0.10311  ,\n",
       "           0.10311  ,  0.15239  ,  0.15239  ,  0.097861 ,  0.097861 ,\n",
       "           0.18874  ,  0.18874  ,  0.33161  ,  0.33161  ,  0.35783  ,\n",
       "           0.35783  ,  0.47651  ,  0.47651  ,  0.28464  ,  0.28464  ,\n",
       "           0.38285  ,  0.38285  ,  0.49751  ,  0.49751  ,  0.43599  ,\n",
       "           0.43599  ,  0.44142  ,  0.44142  ,  0.18     ,  0.18     ,\n",
       "           0.31861  ,  0.31861  ,  0.25083  ,  0.25083  ,  0.072256 ,\n",
       "           0.072256 ,  0.20171  ,  0.20171  ,  0.08234  ,  0.08234  ,\n",
       "           0.10307  ,  0.10307  ,  0.11407  ,  0.11407  ,  0.37818  ,\n",
       "           0.37818  ,  0.70079  ,  0.70079  ,  0.22818  ,  0.22818  ,\n",
       "           0.4826   ,  0.4826   ,  0.18201  ,  0.18201  ,  0.22923  ,\n",
       "           0.22923  ,  0.22029  ,  0.22029  ,  0.32249  ,  0.32249  ,\n",
       "           0.29678  ,  0.29678  ,  0.37948  ,  0.37948  ,  0.1018   ,\n",
       "           0.1018   ,  0.11987  ,  0.11987  ,  0.15403  ,  0.15403  ,\n",
       "           0.38225  ,  0.38225  ,  0.047428 ,  0.047428 ,  0.17087  ,\n",
       "           0.17087  ,  0.48353  ,  0.48353  ,  0.28801  ,  0.28801  ,\n",
       "           0.49612  ,  0.49612  ,  0.19856  ,  0.19856  ,  0.15518  ,\n",
       "           0.15518  ,  0.35209  ,  0.35209  ,  0.15824  ,  0.15824  ,\n",
       "           0.24511  ,  0.24511  ,  0.11831  ,  0.11831  ,  0.21464  ,\n",
       "           0.21464  ,  0.1765   ,  0.1765   ,  0.24353  ,  0.24353  ,\n",
       "           0.16734  ,  0.16734  ,  0.26438  ,  0.26438  ,  0.23999  ,\n",
       "           0.23999  ,  0.19193  ,  0.19193  ,  0.22554  ,  0.22554  ,\n",
       "           0.20534  ,  0.20534  ,  0.18197  ,  0.18197  ,  0.17313  ,\n",
       "           0.17313  ,  0.2402   ,  0.2402   ,  0.34958  ,  0.34958  ,\n",
       "           0.33265  ,  0.33265  ,  0.12881  ,  0.12881  ,  0.093283 ,\n",
       "           0.093283 ,  0.24612  ,  0.24612  ,  0.54612  ,  0.54612  ,\n",
       "           0.1315   ,  0.1315   ,  0.17622  ,  0.17622  ,  0.094095 ,\n",
       "           0.094095 ,  0.20863  ,  0.20863  ,  0.18872  ,  0.18872  ,\n",
       "           0.22455  ,  0.22455  ,  0.037041 ,  0.037041 ,  0.067595 ,\n",
       "           0.067595 ,  0.071538 ,  0.071538 ,  0.26982  ,  0.26982  ,\n",
       "           0.10338  ,  0.10338  ,  0.23899  ,  0.23899  ,  0.59564  ,\n",
       "           0.59564  ,  0.30835  ,  0.30835  ,  0.58107  ,  0.58107  ,\n",
       "           0.36845  ,  0.36845  ,  0.11855  ,  0.11855  ,  0.61204  ,\n",
       "           0.61204  ,  0.37042  ,  0.37042  ,  0.31008  ,  0.31008  ,\n",
       "           0.22684  ,  0.22684  ,  0.28242  ,  0.28242  ,  0.23092  ,\n",
       "           0.23092  ,  0.24189  ,  0.24189  ,  0.068368 ,  0.068368 ,\n",
       "           0.2144   ,  0.2144   ,  0.11238  ,  0.11238  ,  0.22456  ,\n",
       "           0.22456  ,  0.074981 ,  0.074981 ,  0.45418  ,  0.45418  ,\n",
       "           0.38823  ,  0.38823  ,  0.22955  ,  0.22955  ,  0.24779  ,\n",
       "           0.24779  ,  0.46636  ,  0.46636  ,  0.231    ,  0.231    ,\n",
       "           0.26248  ,  0.26248  ,  0.21753  ,  0.21753  ,  0.13705  ,\n",
       "           0.13705  ,  0.25632  ,  0.25632  ,  0.21373  ,  0.21373  ,\n",
       "           0.23771  ,  0.23771  , -0.029437 , -0.029437 ,  0.11596  ,\n",
       "           0.11596  ,  0.028802 ,  0.028802 ,  0.31914  ,  0.31914  ,\n",
       "           0.011954 ,  0.011954 ,  0.46983  ,  0.46983  ,  0.70364  ,\n",
       "           0.70364  ,  0.28605  ,  0.28605  ,  0.52316  ,  0.52316  ,\n",
       "           0.51308  ,  0.51308  ,  0.23119  ,  0.23119  ,  0.26831  ,\n",
       "           0.26831  ,  0.26953  ,  0.26953  ,  0.28503  ,  0.28503  ,\n",
       "           0.22905  ,  0.22905  ,  0.29339  ,  0.29339  ,  0.16203  ,\n",
       "           0.16203  ,  0.23115  ,  0.23115  ,  0.18003  ,  0.18003  ,\n",
       "           0.28613  ,  0.28613  ,  0.09553  ,  0.09553  ,  0.27519  ,\n",
       "           0.27519  ,  0.21045  ,  0.21045  ,  0.19372  ,  0.19372  ,\n",
       "           0.15535  ,  0.15535  ,  0.28494  ,  0.28494  ,  0.22675  ,\n",
       "           0.22675  ,  1.0155   ,  1.0155   ,  0.46736  ,  0.46736  ,\n",
       "           0.3253   ,  0.3253   ,  0.38513  ,  0.38513  ,  0.38889  ,\n",
       "           0.38889  ,  0.47048  ,  0.47048  ,  0.43624  ,  0.43624  ,\n",
       "           0.16823  ,  0.16823  ,  0.32943  ,  0.32943  ,  0.38119  ,\n",
       "           0.38119  ,  0.15107  ,  0.15107  ,  0.21734  ,  0.21734  ,\n",
       "           0.12138  ,  0.12138  ,  0.19254  ,  0.19254  ,  0.13583  ,\n",
       "           0.13583  ,  0.50327  ,  0.50327  ,  0.36506  ,  0.36506  ,\n",
       "           0.45718  ,  0.45718  ,  0.42915  ,  0.42915  ,  0.52831  ,\n",
       "           0.52831  ,  0.49068  ,  0.49068  ,  0.17272  ,  0.17272  ,\n",
       "           0.34581  ,  0.34581  ,  0.34235  ,  0.34235  ,  0.065032 ,\n",
       "           0.065032 ,  0.1468   ,  0.1468   ,  0.042703 ,  0.042703 ,\n",
       "           0.091022 ,  0.091022 ,  0.13416  ,  0.13416  ,  0.54402  ,\n",
       "           0.54402  ,  0.52982  ,  0.52982  ,  0.51557  ,  0.51557  ,\n",
       "           0.48936  ,  0.48936  ,  0.50885  ,  0.50885  ,  0.25358  ,\n",
       "           0.25358  ,  0.29956  ,  0.29956  ,  0.31516  ,  0.31516  ,\n",
       "           0.19706  ,  0.19706  ,  0.27752  ,  0.27752  ,  0.17393  ,\n",
       "           0.17393  ,  0.22682  ,  0.22682  ,  0.20172  ,  0.20172  ,\n",
       "           0.35103  ,  0.35103  ,  0.29129  ,  0.29129  ,  0.29662  ,\n",
       "           0.29662  ,  0.37846  ,  0.37846  ,  0.2243   ,  0.2243   ,\n",
       "           0.12522  ,  0.12522  ,  0.15747  ,  0.15747  ,  0.16605  ,\n",
       "           0.16605  ,  0.17666  ,  0.17666  ,  0.15819  ,  0.15819  ,\n",
       "           0.17849  ,  0.17849  ,  0.18453  ,  0.18453  ,  0.45635  ,\n",
       "           0.45635  ,  0.46468  ,  0.46468  ,  0.45726  ,  0.45726  ,\n",
       "           0.34839  ,  0.34839  ,  0.3866   ,  0.3866   ,  0.39813  ,\n",
       "           0.39813  ,  0.35559  ,  0.35559  ,  0.28283  ,  0.28283  ,\n",
       "           0.29687  ,  0.29687  ,  0.24929  ,  0.24929  ,  0.1526   ,\n",
       "           0.1526   ,  0.57962  ,  0.57962  ,  0.67896  ,  0.67896  ,\n",
       "           0.1415   ,  0.1415   ,  0.57955  ,  0.57955  ,  0.30799  ,\n",
       "           0.30799  , -0.0033665, -0.0033665,  0.27324  ,  0.27324  ,\n",
       "           0.027971 ,  0.027971 ,  0.077822 ,  0.077822 ,  0.087999 ,\n",
       "           0.087999 ,  0.60946  ,  0.60946  ,  0.15583  ,  0.15583  ,\n",
       "           0.53966  ,  0.53966  ,  0.44818  ,  0.44818  ,  0.10311  ,\n",
       "           0.10311  ,  0.19167  ,  0.19167  ,  0.071054 ,  0.071054 ,\n",
       "           0.18039  ,  0.18039  ,  0.11114  ,  0.11114  ,  0.1465   ,\n",
       "           0.1465   ,  0.50191  ,  0.50191  ,  0.27017  ,  0.27017  ,\n",
       "           0.0045791,  0.0045791,  0.18719  ,  0.18719  ,  0.047998 ,\n",
       "           0.047998 ,  0.084946 ,  0.084946 ,  0.096601 ,  0.096601 ,\n",
       "           0.067615 ,  0.067615 ,  0.15926  ,  0.15926  ,  0.37743  ,\n",
       "           0.37743  ,  0.27607  ,  0.27607  ,  0.41946  ,  0.41946  ,\n",
       "           0.21872  ,  0.21872  ,  0.2139   ,  0.2139   ,  0.5309   ,\n",
       "           0.5309   ,  0.17173  ,  0.17173  ,  0.1977   ,  0.1977   ,\n",
       "           0.15914  ,  0.15914  ,  0.31658  ,  0.31658  ,  0.027    ,\n",
       "           0.027    ,  0.4958   ,  0.4958   ,  0.25637  ,  0.25637  ,\n",
       "           0.25208  ,  0.25208  ,  0.60549  ,  0.60549  ,  0.13979  ,\n",
       "           0.13979  ,  0.36196  ,  0.36196  ,  0.85576  ,  0.85576  ,\n",
       "           0.71396  ,  0.71396  ,  0.17641  ,  0.17641  ,  0.43945  ,\n",
       "           0.43945  ,  0.28797  ,  0.28797  ,  0.15046  ,  0.15046  ,\n",
       "           0.48791  ,  0.48791  ,  0.14908  ,  0.14908  ,  0.18887  ,\n",
       "           0.18887  ]),\n",
       "   'y_pred': array([ 0.49506816,  0.29934907,  0.5615911 ,  0.5681933 ,  0.17429519,\n",
       "           0.08472161,  0.44689643,  0.4169906 ,  0.23114222,  0.17386523,\n",
       "           0.12103741,  0.18365079,  0.24607605,  0.1533648 ,  0.0387281 ,\n",
       "           0.29802126,  0.15068895,  0.12053357,  0.16638547,  0.2514496 ,\n",
       "           0.35098517,  0.24989879,  0.86520815,  0.63650054,  0.570143  ,\n",
       "           0.64582145,  0.35630813,  0.3437592 ,  0.14948508,  0.07574785,\n",
       "           0.17655471,  0.19909565,  0.25454774,  0.2026181 ,  0.2496697 ,\n",
       "           0.25435784,  0.33159387,  0.25658193,  0.1832643 ,  0.16995117,\n",
       "           0.09938084,  0.14970647,  0.28331923,  0.32883263,  0.15818748,\n",
       "           0.16045219,  0.1799776 ,  0.18983737,  0.086216  ,  0.14542039,\n",
       "           0.13497472,  0.21702401,  0.10746737,  0.11433378,  0.6341835 ,\n",
       "           0.5878018 ,  0.5067086 ,  0.4445699 ,  0.606024  ,  0.63457644,\n",
       "           0.38597596,  0.33503225,  0.23893665,  0.31916928,  0.21197955,\n",
       "           0.17142649,  0.21543604,  0.16572507,  0.33514965,  0.23998001,\n",
       "           0.27781168,  0.1248019 ,  0.32990006,  0.26788694,  0.5535923 ,\n",
       "           0.6042025 ,  0.6543425 ,  0.73026335,  0.474858  ,  0.5428941 ,\n",
       "           0.33216226,  0.3487276 ,  0.5159834 ,  0.4325901 ,  0.3894641 ,\n",
       "           0.35272545,  0.490035  ,  0.45252055,  0.4334274 ,  0.36192676,\n",
       "           0.24994688,  0.23509881,  0.35245413,  0.23272985,  0.4002011 ,\n",
       "           0.41781554,  0.23884967,  0.09469457,  0.2397993 ,  0.15890738,\n",
       "           0.13657555,  0.10220332,  0.22480364,  0.16466881,  0.17705244,\n",
       "           0.17613375,  0.43442416,  0.48372483,  0.5725033 ,  0.5900468 ,\n",
       "           0.28414762,  0.34431666,  0.22996321,  0.3007521 ,  0.10063906,\n",
       "           0.1233595 ,  0.17696226,  0.22988653,  0.29780075,  0.4001165 ,\n",
       "           0.23165244,  0.23715647,  0.2631261 ,  0.20922583,  0.9125669 ,\n",
       "           0.85680807,  0.89601994,  0.87131214,  0.6299206 ,  0.5226374 ,\n",
       "           0.31389415,  0.46996987,  0.41527948,  0.39744428,  0.3462829 ,\n",
       "           0.328052  ,  0.45177257,  0.4677451 ,  0.37416917,  0.43257794,\n",
       "           0.17800844,  0.19675669,  0.3288198 ,  0.34099755,  0.308049  ,\n",
       "           0.26749888,  0.09063669,  0.15338504,  0.16887891,  0.17249337,\n",
       "           0.07879567,  0.10802507,  0.16187249,  0.23299408,  0.14013886,\n",
       "           0.1613536 ,  0.42503437,  0.5854294 ,  0.3672266 ,  0.35382867,\n",
       "           0.29365695,  0.35896817,  0.21827221,  0.18661359,  0.28616607,\n",
       "           0.1575154 ,  0.2195999 ,  0.30260602,  0.16256797,  0.25122476,\n",
       "           0.21273229,  0.25155842,  0.42857754,  0.45512238,  0.5084625 ,\n",
       "           0.5190736 ,  0.54041916,  0.5447656 ,  0.53792334,  0.80892766,\n",
       "           0.2811847 ,  0.46236134,  0.38694277,  0.2541351 ,  0.37126324,\n",
       "           0.38906902,  0.47622764,  0.4366383 ,  0.25320712,  0.22100689,\n",
       "           0.22454229,  0.20541829,  0.21361078,  0.28748223,  0.2001987 ,\n",
       "           0.11219676,  0.16116492,  0.12307188,  0.11194678,  0.15555963,\n",
       "           0.23697653,  0.10041226,  0.25768563,  0.22526613,  0.3185612 ,\n",
       "           0.31515303,  0.12598911,  0.1634944 ,  0.2176179 ,  0.06289305,\n",
       "           0.23330334,  0.23473895,  0.22165716,  0.15145066,  0.06349812,\n",
       "           0.16176185,  0.30817324,  0.26800525,  0.5414666 ,  0.50355506,\n",
       "           0.5939748 ,  0.6955635 ,  0.41778126,  0.4650338 ,  0.21550968,\n",
       "           0.37488455,  0.49430633,  0.34701574,  0.38235402,  0.43698508,\n",
       "           0.46573582,  0.5060749 ,  0.43944126,  0.39821574,  0.24417397,\n",
       "           0.17652598,  0.41943026,  0.34605384,  0.42656702,  0.43624374,\n",
       "           0.07549678,  0.08174275,  0.2045804 ,  0.15947315,  0.10737735,\n",
       "           0.04621977,  0.27868026,  0.30979362,  0.1374886 ,  0.14627287,\n",
       "           0.23470779,  0.3449541 ,  0.21875608,  0.22099726,  0.19522183,\n",
       "           0.17837149,  0.33523422,  0.26466197,  0.29885492,  0.16462328,\n",
       "           0.18347919,  0.20716792,  0.27810442,  0.335711  ,  0.31911576,\n",
       "           0.30694604,  0.36058146,  0.4032178 ,  0.33782524,  0.32633856,\n",
       "           0.3660114 ,  0.43448114,  0.41779566,  0.42904466,  0.42386281,\n",
       "           0.38634226,  0.42926618,  0.35978898,  0.19402048,  0.19522196,\n",
       "           0.36976385,  0.26099676,  0.34837735,  0.34042507,  0.16093779,\n",
       "           0.31139785,  0.27118886,  0.19755535,  0.16098282,  0.15373904,\n",
       "           0.19847387,  0.15247145,  0.17087168,  0.12932625,  0.2557706 ,\n",
       "           0.37038904,  0.41090336,  0.5512216 ,  0.18767755,  0.24760818,\n",
       "           0.37792328,  0.29885238,  0.18902105,  0.25362536,  0.27918595,\n",
       "           0.27662197,  0.21360566,  0.15145162,  0.34106064,  0.3845169 ,\n",
       "           0.27213266,  0.28820843,  0.3781287 ,  0.3044566 ,  0.2812678 ,\n",
       "           0.22304219,  0.26326472,  0.28810158,  0.11438423,  0.16785288,\n",
       "           0.303358  ,  0.36181328,  0.19670683,  0.12759161,  0.22325072,\n",
       "           0.2521089 ,  0.393871  ,  0.38282079,  0.29422307,  0.26260683,\n",
       "           0.2857465 ,  0.35943848,  0.22568342,  0.35121608,  0.10512899,\n",
       "           0.15882501,  0.32337964,  0.20026419,  0.1950451 ,  0.22668462,\n",
       "           0.22944139,  0.2816998 ,  0.17863473,  0.17284231,  0.21403326,\n",
       "           0.18139264,  0.10659987,  0.1388644 ,  0.24768817,  0.29306358,\n",
       "           0.1572073 ,  0.1416783 ,  0.22649063,  0.17016006,  0.26830322,\n",
       "           0.23365347,  0.28271934,  0.17750265,  0.19427279,  0.21450731,\n",
       "           0.18048283,  0.23785366,  0.18758988,  0.24968281,  0.25245795,\n",
       "           0.19623828,  0.2472623 ,  0.35293573,  0.3122574 ,  0.23204769,\n",
       "           0.18246758,  0.32778823,  0.16332527,  0.24999177,  0.12443298,\n",
       "           0.14952134,  0.25185972,  0.17088175,  0.46987587,  0.23167746,\n",
       "           0.26750195,  0.23557541,  0.24795552,  0.23209655,  0.192709  ,\n",
       "           0.12426747,  0.27056408,  0.2606692 ,  0.24111663,  0.12248257,\n",
       "           0.2151242 ,  0.31404835,  0.25003305,  0.15039763,  0.19881278,\n",
       "           0.20935993,  0.22786947,  0.10909426,  0.23887855,  0.21430206,\n",
       "           0.1808    ,  0.07806106,  0.19703819,  0.20039201,  0.51294994,\n",
       "           0.43507868,  0.22267592,  0.3393436 ,  0.37416506,  0.5489577 ,\n",
       "           0.3203829 ,  0.45538402,  0.13773443,  0.16571453,  0.55051184,\n",
       "           0.42275465,  0.23356849,  0.30931854,  0.35791278,  0.33627525,\n",
       "           0.20117351,  0.19751579,  0.20886123,  0.24614596,  0.10254371,\n",
       "           0.08628727,  0.237651  ,  0.3325742 ,  0.15949875,  0.11832669,\n",
       "           0.22916244,  0.22217628,  0.2539402 ,  0.0882135 ,  0.24181812,\n",
       "           0.27779576,  0.13282548,  0.2135022 ,  0.5128405 ,  0.35293633,\n",
       "           0.47841722,  0.27737033,  0.32606453,  0.22790383,  0.3036439 ,\n",
       "           0.31010553,  0.23621961,  0.3139308 ,  0.23037922,  0.19763084,\n",
       "           0.33450007,  0.30183738,  0.27881142,  0.2797848 ,  0.1996507 ,\n",
       "           0.21962126,  0.23092505,  0.3294954 ,  0.20298254,  0.0418281 ,\n",
       "           0.20199928,  0.35596812,  0.08534215,  0.07858588,  0.35267383,\n",
       "           0.12758392,  0.41375667,  0.07173936,  0.2807601 ,  0.27397436,\n",
       "           0.29210338,  0.1705696 ,  0.38584346,  0.3177009 ,  0.40711153,\n",
       "           0.59726995,  0.2341109 ,  0.2320653 ,  0.35747764,  0.66803485,\n",
       "           0.31823283,  0.5223103 ,  0.16189982,  0.17621568,  0.27671802,\n",
       "           0.2908969 ,  0.25400686,  0.3178867 ,  0.20234385,  0.30125868,\n",
       "           0.3299575 ,  0.19951625,  0.30205163,  0.3123436 ,  0.1820221 ,\n",
       "           0.12014638,  0.2135517 ,  0.16533539,  0.29690862,  0.16821218,\n",
       "           0.30902186,  0.1898233 ,  0.09248145,  0.18130329,  0.19447294,\n",
       "           0.27739143,  0.26105273,  0.32192153,  0.28177395,  0.2550174 ,\n",
       "           0.25996676,  0.21612197,  0.27682126,  0.32010627,  0.17623276,\n",
       "           0.15483078,  0.7902144 ,  0.92307913,  0.5670383 ,  0.40566862,\n",
       "           0.4250478 ,  0.39541328,  0.41005415,  0.37983418,  0.33653346,\n",
       "           0.2834792 ,  0.47493857,  0.45034945,  0.39655796,  0.40408844,\n",
       "           0.1567766 ,  0.18870866,  0.26528838,  0.25565958,  0.30417183,\n",
       "           0.45533368,  0.19431955,  0.17528114,  0.16505422,  0.2735238 ,\n",
       "           0.14517874,  0.15482193,  0.2090559 ,  0.17517635,  0.19822076,\n",
       "           0.15021643,  0.50440013,  0.44517183,  0.40210304,  0.5758517 ,\n",
       "           0.37307   ,  0.45010173,  0.3731193 ,  0.30295002,  0.5092139 ,\n",
       "           0.53304183,  0.33687648,  0.36298004,  0.19119386,  0.17637563,\n",
       "           0.29741246,  0.37941253,  0.3590952 ,  0.20659205,  0.12467195,\n",
       "           0.1538668 ,  0.15139727,  0.12647554,  0.05695711,  0.03164001,\n",
       "           0.04124019,  0.20525748,  0.19195305,  0.11039051,  0.6521644 ,\n",
       "           0.63983434,  0.52551264,  0.46448845,  0.48599994,  0.45782238,\n",
       "           0.51991916,  0.511307  ,  0.4689715 ,  0.49082765,  0.30149424,\n",
       "           0.24118832,  0.21701032,  0.26985055,  0.293108  ,  0.33429143,\n",
       "           0.20989388,  0.12299575,  0.26239458,  0.2848767 ,  0.1600572 ,\n",
       "           0.27943045,  0.2903246 ,  0.24130097,  0.21632175,  0.13857725,\n",
       "           0.33749375,  0.29381543,  0.27983698,  0.37190896,  0.44138157,\n",
       "           0.36641145,  0.64239633,  0.36164832,  0.14643711,  0.22249655,\n",
       "           0.1848547 ,  0.37087315,  0.27054656,  0.18443602,  0.17136598,\n",
       "           0.03170297,  0.18652387,  0.17923468,  0.12144497,  0.17580113,\n",
       "           0.13015443,  0.20533329,  0.1410116 ,  0.15032685,  0.4438346 ,\n",
       "           0.47534508,  0.48713464,  0.35030556,  0.3221965 ,  0.42661348,\n",
       "           0.30825114,  0.30662   ,  0.19946203,  0.35588837,  0.33087054,\n",
       "           0.35064423,  0.28269064,  0.22810173,  0.1558676 ,  0.29444778,\n",
       "           0.19977692,  0.20729856,  0.3224532 ,  0.20828053,  0.1424934 ,\n",
       "           0.14205854,  0.49711168,  0.5417662 ,  0.58247066,  0.48271284,\n",
       "           0.13161379,  0.17839009,  0.38736993,  0.5032033 ,  0.21506706,\n",
       "           0.31393316,  0.07750544,  0.18479696,  0.24042702,  0.29168886,\n",
       "           0.0758353 ,  0.02478853,  0.088486  ,  0.2457371 ,  0.07916199,\n",
       "           0.09201603,  0.47611555,  0.29849195,  0.1430877 ,  0.15772712,\n",
       "           0.329956  ,  0.3680457 ,  0.3673723 ,  0.38156903,  0.17835245,\n",
       "           0.17313837,  0.20637152,  0.23781165,  0.14309645,  0.16920753,\n",
       "           0.19633722,  0.17042705,  0.14868595,  0.1486401 ,  0.13067964,\n",
       "           0.10509884,  0.3508462 ,  0.4598478 ,  0.31392282,  0.40864933,\n",
       "           0.17353782,  0.1067811 ,  0.28491414,  0.33627006,  0.11059664,\n",
       "          -0.00577718,  0.2025208 ,  0.22863351,  0.1002889 ,  0.16130663,\n",
       "           0.07775679,  0.04220751,  0.21073323,  0.1449995 ,  0.3906352 ,\n",
       "           0.47247759,  0.24071565,  0.2397055 ,  0.3623456 ,  0.3035484 ,\n",
       "           0.2955415 ,  0.24007031,  0.17044821,  0.14768016,  0.3569604 ,\n",
       "           0.3664168 ,  0.24708696,  0.12898588,  0.28862417,  0.23849653,\n",
       "           0.06798522,  0.08803596,  0.36132985,  0.17141613,  0.10273664,\n",
       "           0.0631948 ,  0.25658298,  0.20349598,  0.38614267,  0.3444524 ,\n",
       "           0.18670082,  0.42700732,  0.38593453,  0.27180383,  0.15972018,\n",
       "           0.12979534,  0.2568329 ,  0.1681158 ,  0.4289835 ,  0.5258516 ,\n",
       "           0.31332636,  0.5792835 ,  0.23409578,  0.18959287,  0.41921997,\n",
       "           0.31346208,  0.3688247 ,  0.29719746,  0.17033035,  0.15529677,\n",
       "           0.49498358,  0.5211426 ,  0.15147054,  0.09574947,  0.156104  ,\n",
       "           0.16533794], dtype=float32),\n",
       "   'feature_importances': array([1.1178331e-05, 9.0353997e-06, 9.6354725e-06, ..., 0.0000000e+00,\n",
       "          0.0000000e+00, 1.4635948e-06], dtype=float32),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'objective': 'reg:squarederror',\n",
       "    'base_score': None,\n",
       "    'booster': None,\n",
       "    'callbacks': None,\n",
       "    'colsample_bylevel': None,\n",
       "    'colsample_bynode': None,\n",
       "    'colsample_bytree': 0.6,\n",
       "    'device': 'cuda',\n",
       "    'early_stopping_rounds': None,\n",
       "    'enable_categorical': False,\n",
       "    'eval_metric': None,\n",
       "    'feature_types': None,\n",
       "    'gamma': None,\n",
       "    'grow_policy': None,\n",
       "    'importance_type': None,\n",
       "    'interaction_constraints': None,\n",
       "    'learning_rate': 0.3,\n",
       "    'max_bin': None,\n",
       "    'max_cat_threshold': None,\n",
       "    'max_cat_to_onehot': None,\n",
       "    'max_delta_step': None,\n",
       "    'max_depth': 4,\n",
       "    'max_leaves': None,\n",
       "    'min_child_weight': None,\n",
       "    'missing': nan,\n",
       "    'monotone_constraints': None,\n",
       "    'multi_strategy': None,\n",
       "    'n_estimators': 50,\n",
       "    'n_jobs': None,\n",
       "    'num_parallel_tree': None,\n",
       "    'random_state': 42,\n",
       "    'reg_alpha': 0.0001,\n",
       "    'reg_lambda': 0.01,\n",
       "    'sampling_method': None,\n",
       "    'scale_pos_weight': None,\n",
       "    'subsample': 1,\n",
       "    'tree_method': 'gpu_hist',\n",
       "    'validate_parameters': None,\n",
       "    'verbosity': 0,\n",
       "    'n_gpus': -1},\n",
       "   'train_metrics': {'mse': 0.005917605283539093,\n",
       "    'mae': 0.053143852723005036,\n",
       "    'r2': 0.8225379686861438,\n",
       "    'pearson_corr': 0.9115167685546717},\n",
       "   'best_val_score': 0.8335815664109655,\n",
       "   'test_metrics': {'mse': 0.005859427558856006,\n",
       "    'mae': 0.05088605683237315,\n",
       "    'r2': 0.8095677284948799,\n",
       "    'pearson_corr': 0.9031347687099174,\n",
       "    'connectome_corr': 0.7849199824613721,\n",
       "    'connectome_r2': 0.5661209019113081,\n",
       "    'geodesic_distance': 4.265327472286463},\n",
       "   'y_true': array([ 0.81838 ,  0.81838 ,  0.44134 ,  0.44134 ,  0.3922  ,  0.3922  ,\n",
       "           0.33937 ,  0.33937 ,  0.31612 ,  0.31612 ,  0.34407 ,  0.34407 ,\n",
       "           0.1723  ,  0.1723  ,  0.26053 ,  0.26053 ,  0.9718  ,  0.9718  ,\n",
       "           0.733   ,  0.733   ,  0.40486 ,  0.40486 ,  0.3934  ,  0.3934  ,\n",
       "           0.39635 ,  0.39635 ,  0.39357 ,  0.39357 ,  0.3708  ,  0.3708  ,\n",
       "           0.17249 ,  0.17249 ,  0.13774 ,  0.13774 ,  0.24104 ,  0.24104 ,\n",
       "           0.34322 ,  0.34322 ,  0.22774 ,  0.22774 ,  0.14728 ,  0.14728 ,\n",
       "           0.018301,  0.018301,  0.097647,  0.097647,  0.11349 ,  0.11349 ,\n",
       "           0.14274 ,  0.14274 ,  0.087701,  0.087701,  0.14287 ,  0.14287 ,\n",
       "           0.4315  ,  0.4315  ,  0.4144  ,  0.4144  ,  0.30494 ,  0.30494 ,\n",
       "           0.33556 ,  0.33556 ,  0.35054 ,  0.35054 ,  0.13225 ,  0.13225 ,\n",
       "           0.20896 ,  0.20896 ,  0.88472 ,  0.88472 ,  0.56087 ,  0.56087 ,\n",
       "           0.40877 ,  0.40877 ,  0.40643 ,  0.40643 ,  0.37281 ,  0.37281 ,\n",
       "           0.35164 ,  0.35164 ,  0.39117 ,  0.39117 ,  0.12708 ,  0.12708 ,\n",
       "           0.12771 ,  0.12771 ,  0.23802 ,  0.23802 ,  0.30795 ,  0.30795 ,\n",
       "           0.18453 ,  0.18453 ,  0.11813 ,  0.11813 ,  0.014342,  0.014342,\n",
       "           0.081532,  0.081532,  0.10419 ,  0.10419 ,  0.14018 ,  0.14018 ,\n",
       "           0.070059,  0.070059,  0.13455 ,  0.13455 ,  0.652   ,  0.652   ,\n",
       "           0.31912 ,  0.31912 ,  0.3282  ,  0.3282  ,  0.52059 ,  0.52059 ,\n",
       "           0.16183 ,  0.16183 ,  0.29156 ,  0.29156 ,  0.44222 ,  0.44222 ,\n",
       "           0.4727  ,  0.4727  ,  0.61856 ,  0.61856 ,  0.57925 ,  0.57925 ,\n",
       "           0.70341 ,  0.70341 ,  0.31806 ,  0.31806 ,  0.39637 ,  0.39637 ,\n",
       "           0.12368 ,  0.12368 ,  0.10866 ,  0.10866 ,  0.21268 ,  0.21268 ,\n",
       "           0.37674 ,  0.37674 ,  0.19332 ,  0.19332 ,  0.17454 ,  0.17454 ,\n",
       "           0.025401,  0.025401,  0.10078 ,  0.10078 ,  0.1409  ,  0.1409  ,\n",
       "           0.18789 ,  0.18789 ,  0.083905,  0.083905,  0.15993 ,  0.15993 ,\n",
       "           0.31714 ,  0.31714 ,  0.47498 ,  0.47498 ,  0.46442 ,  0.46442 ,\n",
       "           0.108   ,  0.108   ,  0.17035 ,  0.17035 ,  0.40173 ,  0.40173 ,\n",
       "           0.45769 ,  0.45769 ,  0.49706 ,  0.49706 ,  0.49691 ,  0.49691 ,\n",
       "           0.45997 ,  0.45997 ,  0.32508 ,  0.32508 ,  0.57744 ,  0.57744 ,\n",
       "           0.08625 ,  0.08625 ,  0.11305 ,  0.11305 ,  0.20143 ,  0.20143 ,\n",
       "           0.26651 ,  0.26651 ,  0.12242 ,  0.12242 ,  0.10453 ,  0.10453 ,\n",
       "           0.016032,  0.016032,  0.07877 ,  0.07877 ,  0.1242  ,  0.1242  ,\n",
       "           0.1604  ,  0.1604  ,  0.065636,  0.065636,  0.14085 ,  0.14085 ,\n",
       "           0.61055 ,  0.61055 ,  0.48032 ,  0.48032 ,  0.34818 ,  0.34818 ,\n",
       "           0.13401 ,  0.13401 ,  0.31784 ,  0.31784 ,  0.41468 ,  0.41468 ,\n",
       "           0.31134 ,  0.31134 ,  0.3635  ,  0.3635  ,  0.26366 ,  0.26366 ,\n",
       "           0.72974 ,  0.72974 ,  0.50262 ,  0.50262 ,  0.1968  ,  0.1968  ,\n",
       "           0.063801,  0.063801,  0.31384 ,  0.31384 ,  0.27739 ,  0.27739 ,\n",
       "           0.32239 ,  0.32239 , -0.05647 , -0.05647 ,  0.010482,  0.010482,\n",
       "           0.16089 ,  0.16089 ,  0.18213 ,  0.18213 ,  0.15926 ,  0.15926 ,\n",
       "           0.13811 ,  0.13811 ,  0.13602 ,  0.13602 ,  0.51535 ,  0.51535 ,\n",
       "           0.19514 ,  0.19514 ,  0.026434,  0.026434,  0.31163 ,  0.31163 ,\n",
       "           0.33739 ,  0.33739 ,  0.36784 ,  0.36784 ,  0.48796 ,  0.48796 ,\n",
       "           0.17221 ,  0.17221 ,  0.59915 ,  0.59915 ,  0.9465  ,  0.9465  ,\n",
       "           0.1477  ,  0.1477  ,  0.16485 ,  0.16485 ,  0.36548 ,  0.36548 ,\n",
       "           0.28938 ,  0.28938 ,  0.34738 ,  0.34738 , -0.069929, -0.069929,\n",
       "           0.012004,  0.012004,  0.13788 ,  0.13788 ,  0.19761 ,  0.19761 ,\n",
       "           0.19086 ,  0.19086 ,  0.126   ,  0.126   ,  0.17964 ,  0.17964 ,\n",
       "           0.11124 ,  0.11124 ,  0.17191 ,  0.17191 ,  0.34755 ,  0.34755 ,\n",
       "           0.35721 ,  0.35721 ,  0.43137 ,  0.43137 ,  0.53786 ,  0.53786 ,\n",
       "           0.3672  ,  0.3672  ,  0.43597 ,  0.43597 ,  0.51059 ,  0.51059 ,\n",
       "           0.086054,  0.086054,  0.086132,  0.086132,  0.28085 ,  0.28085 ,\n",
       "           0.31346 ,  0.31346 ,  0.24337 ,  0.24337 ,  0.022592,  0.022592,\n",
       "           0.018079,  0.018079,  0.13688 ,  0.13688 ,  0.18395 ,  0.18395 ,\n",
       "           0.16099 ,  0.16099 ,  0.11672 ,  0.11672 ,  0.14472 ,  0.14472 ,\n",
       "           0.2988  ,  0.2988  ,  0.14685 ,  0.14685 ,  0.20041 ,  0.20041 ,\n",
       "           0.14503 ,  0.14503 ,  0.1079  ,  0.1079  ,  0.1484  ,  0.1484  ,\n",
       "           0.21373 ,  0.21373 ,  0.092073,  0.092073,  0.3208  ,  0.3208  ,\n",
       "           0.15637 ,  0.15637 ,  0.19179 ,  0.19179 ,  0.2362  ,  0.2362  ,\n",
       "           0.23041 ,  0.23041 ,  0.22145 ,  0.22145 ,  0.018007,  0.018007,\n",
       "           0.13568 ,  0.13568 ,  0.097995,  0.097995,  0.13314 ,  0.13314 ,\n",
       "           0.11861 ,  0.11861 ,  0.11165 ,  0.11165 ,  0.25668 ,  0.25668 ,\n",
       "           0.27928 ,  0.27928 ,  0.25164 ,  0.25164 ,  0.17766 ,  0.17766 ,\n",
       "           0.30654 ,  0.30654 ,  0.04025 ,  0.04025 , -0.019243, -0.019243,\n",
       "           0.10886 ,  0.10886 ,  0.079629,  0.079629,  0.037198,  0.037198,\n",
       "           0.55236 ,  0.55236 ,  0.31815 ,  0.31815 ,  0.47026 ,  0.47026 ,\n",
       "           0.023071,  0.023071,  0.096659,  0.096659,  0.07033 ,  0.07033 ,\n",
       "           0.088388,  0.088388,  0.06929 ,  0.06929 ,  0.063706,  0.063706,\n",
       "           0.75899 ,  0.75899 ,  0.4182  ,  0.4182  ,  0.41105 ,  0.41105 ,\n",
       "           0.40217 ,  0.40217 ,  0.38907 ,  0.38907 ,  0.38795 ,  0.38795 ,\n",
       "           0.13881 ,  0.13881 ,  0.1268  ,  0.1268  ,  0.22659 ,  0.22659 ,\n",
       "           0.34833 ,  0.34833 ,  0.20942 ,  0.20942 ,  0.13749 ,  0.13749 ,\n",
       "           0.015259,  0.015259,  0.08026 ,  0.08026 ,  0.1011  ,  0.1011  ,\n",
       "           0.13007 ,  0.13007 ,  0.068697,  0.068697,  0.13054 ,  0.13054 ,\n",
       "           0.41689 ,  0.41689 ,  0.41408 ,  0.41408 ,  0.4566  ,  0.4566  ,\n",
       "           0.52749 ,  0.52749 ,  0.47462 ,  0.47462 ,  0.15207 ,  0.15207 ,\n",
       "           0.10558 ,  0.10558 ,  0.20897 ,  0.20897 ,  0.38513 ,  0.38513 ,\n",
       "           0.22266 ,  0.22266 ,  0.13338 ,  0.13338 ,  0.023153,  0.023153,\n",
       "           0.10571 ,  0.10571 ,  0.12122 ,  0.12122 ,  0.14254 ,  0.14254 ,\n",
       "           0.095759,  0.095759,  0.14333 ,  0.14333 ,  0.68306 ,  0.68306 ,\n",
       "           0.5435  ,  0.5435  ,  0.33372 ,  0.33372 ,  0.48485 ,  0.48485 ,\n",
       "           0.13303 ,  0.13303 ,  0.13193 ,  0.13193 ,  0.24759 ,  0.24759 ,\n",
       "           0.35667 ,  0.35667 ,  0.20544 ,  0.20544 ,  0.16099 ,  0.16099 ,\n",
       "           0.023318,  0.023318,  0.097558,  0.097558,  0.14163 ,  0.14163 ,\n",
       "           0.1726  ,  0.1726  ,  0.084644,  0.084644,  0.1691  ,  0.1691  ,\n",
       "           0.48128 ,  0.48128 ,  0.43514 ,  0.43514 ,  0.67583 ,  0.67583 ,\n",
       "           0.10531 ,  0.10531 ,  0.1171  ,  0.1171  ,  0.25078 ,  0.25078 ,\n",
       "           0.34208 ,  0.34208 ,  0.20078 ,  0.20078 ,  0.068998,  0.068998,\n",
       "           0.018846,  0.018846,  0.09443 ,  0.09443 ,  0.16398 ,  0.16398 ,\n",
       "           0.15269 ,  0.15269 ,  0.082883,  0.082883,  0.14856 ,  0.14856 ,\n",
       "           0.25675 ,  0.25675 ,  0.25601 ,  0.25601 ,  0.10158 ,  0.10158 ,\n",
       "           0.055389,  0.055389,  0.12263 ,  0.12263 ,  0.35918 ,  0.35918 ,\n",
       "           0.16189 ,  0.16189 ,  0.1744  ,  0.1744  ,  0.019853,  0.019853,\n",
       "           0.069583,  0.069583,  0.076881,  0.076881,  0.11079 ,  0.11079 ,\n",
       "           0.053991,  0.053991,  0.10556 ,  0.10556 ,  0.70754 ,  0.70754 ,\n",
       "           0.28536 ,  0.28536 ,  0.12229 ,  0.12229 ,  0.40747 ,  0.40747 ,\n",
       "           0.34977 ,  0.34977 ,  0.45087 ,  0.45087 , -0.029604, -0.029604,\n",
       "           0.012842,  0.012842,  0.14106 ,  0.14106 ,  0.17495 ,  0.17495 ,\n",
       "           0.15435 ,  0.15435 ,  0.14986 ,  0.14986 ,  0.16964 ,  0.16964 ,\n",
       "           0.11954 ,  0.11954 ,  0.15531 ,  0.15531 ,  0.34743 ,  0.34743 ,\n",
       "           0.25542 ,  0.25542 ,  0.2543  ,  0.2543  , -0.076543, -0.076543,\n",
       "           0.01228 ,  0.01228 ,  0.10778 ,  0.10778 ,  0.1859  ,  0.1859  ,\n",
       "           0.1875  ,  0.1875  ,  0.10722 ,  0.10722 ,  0.18352 ,  0.18352 ,\n",
       "           0.32773 ,  0.32773 ,  0.43278 ,  0.43278 ,  0.28433 ,  0.28433 ,\n",
       "           0.34246 ,  0.34246 ,  0.31868 ,  0.31868 ,  0.021865,  0.021865,\n",
       "           0.15572 ,  0.15572 ,  0.10647 ,  0.10647 ,  0.14874 ,  0.14874 ,\n",
       "           0.19075 ,  0.19075 ,  0.17451 ,  0.17451 ,  0.33589 ,  0.33589 ,\n",
       "           0.15461 ,  0.15461 ,  0.12699 ,  0.12699 ,  0.38227 ,  0.38227 ,\n",
       "           0.031151,  0.031151,  0.12646 ,  0.12646 ,  0.10322 ,  0.10322 ,\n",
       "           0.16502 ,  0.16502 ,  0.13441 ,  0.13441 ,  0.17819 ,  0.17819 ,\n",
       "           0.23785 ,  0.23785 ,  0.30948 ,  0.30948 ,  0.17583 ,  0.17583 ,\n",
       "           0.038642,  0.038642,  0.19682 ,  0.19682 ,  0.17838 ,  0.17838 ,\n",
       "           0.21146 ,  0.21146 ,  0.21378 ,  0.21378 ,  0.22637 ,  0.22637 ,\n",
       "           0.60657 ,  0.60657 ,  0.33179 ,  0.33179 ,  0.019819,  0.019819,\n",
       "           0.1191  ,  0.1191  ,  0.10777 ,  0.10777 ,  0.11949 ,  0.11949 ,\n",
       "           0.11785 ,  0.11785 ,  0.12269 ,  0.12269 ,  0.19324 ,  0.19324 ,\n",
       "           0.010799,  0.010799,  0.14439 ,  0.14439 ,  0.12377 ,  0.12377 ,\n",
       "           0.12771 ,  0.12771 ,  0.15287 ,  0.15287 ,  0.13668 ,  0.13668 ,\n",
       "           0.040826,  0.040826,  0.11812 ,  0.11812 ,  0.062458,  0.062458,\n",
       "           0.14295 ,  0.14295 ,  0.1203  ,  0.1203  ,  0.14825 ,  0.14825 ,\n",
       "           0.07886 ,  0.07886 ,  0.060065,  0.060065,  0.04385 ,  0.04385 ,\n",
       "           0.06811 ,  0.06811 ,  0.045071,  0.045071,  0.28453 ,  0.28453 ,\n",
       "           0.25202 ,  0.25202 ,  0.40311 ,  0.40311 ,  0.23479 ,  0.23479 ,\n",
       "           0.22615 ,  0.22615 ,  0.27356 ,  0.27356 ,  0.21588 ,  0.21588 ,\n",
       "           0.23607 ,  0.23607 ,  0.35804 ,  0.35804 ,  0.24593 ,  0.24593 ]),\n",
       "   'y_pred': array([ 0.7258388 ,  0.76276374,  0.44594735,  0.43838543,  0.41337228,\n",
       "           0.40708512,  0.3360654 ,  0.3299281 ,  0.35166222,  0.41084796,\n",
       "           0.32743114,  0.3399388 ,  0.17006801,  0.1617575 ,  0.1752882 ,\n",
       "           0.16026112,  0.79662967,  0.7208866 ,  0.5948545 ,  0.6746184 ,\n",
       "           0.41274858,  0.42216364,  0.39928898,  0.40060264,  0.4361984 ,\n",
       "           0.40526545,  0.41089672,  0.36180604,  0.4228291 ,  0.42221332,\n",
       "           0.14930059,  0.18108338,  0.17918518,  0.1707346 ,  0.26974612,\n",
       "           0.23192786,  0.28764755,  0.32476613,  0.16178851,  0.18332644,\n",
       "           0.12445162,  0.19465083,  0.03018647,  0.00444891,  0.07905456,\n",
       "           0.11238398,  0.11605315,  0.12624513,  0.14639007,  0.1460762 ,\n",
       "           0.09788865,  0.12304008,  0.14639007,  0.14693293,  0.42967492,\n",
       "           0.43657237,  0.48529768,  0.40896326,  0.29528287,  0.30729565,\n",
       "           0.31641695,  0.39391005,  0.27644348,  0.29456693,  0.15592797,\n",
       "           0.15143737,  0.18491483,  0.19733614,  0.6782461 ,  0.66915727,\n",
       "           0.54159915,  0.5566876 ,  0.44321775,  0.37006634,  0.4152711 ,\n",
       "           0.41330123,  0.4080356 ,  0.37638587,  0.39835015,  0.32361877,\n",
       "           0.43302345,  0.4040532 ,  0.10163307,  0.18540472,  0.13309784,\n",
       "           0.20845854,  0.28023815,  0.22227454,  0.3283293 ,  0.3154829 ,\n",
       "           0.16684753,  0.19579381,  0.21538757,  0.22992614,  0.03561941,\n",
       "           0.00959264,  0.08163996,  0.11103441,  0.12155192,  0.1242833 ,\n",
       "           0.13990033,  0.14959995,  0.09945725,  0.12300998,  0.14169005,\n",
       "           0.1466372 ,  0.49651268,  0.5007413 ,  0.414304  ,  0.4911455 ,\n",
       "           0.39901125,  0.47034404,  0.47830272,  0.45744374,  0.22362529,\n",
       "           0.16419533,  0.22985879,  0.22324418,  0.46659407,  0.47328675,\n",
       "           0.44670653,  0.42675182,  0.53472507,  0.4674225 ,  0.48217252,\n",
       "           0.56109095,  0.4981076 ,  0.572378  ,  0.3908872 ,  0.43300036,\n",
       "           0.50648546,  0.49430436,  0.13899183,  0.08811398,  0.10619746,\n",
       "           0.12873906,  0.28377134,  0.3252289 ,  0.42671347,  0.39830136,\n",
       "           0.21793722,  0.21222407,  0.10133664,  0.03076069,  0.04489291,\n",
       "           0.07141946,  0.14492321,  0.15307438,  0.15840031,  0.18314147,\n",
       "           0.1855543 ,  0.22261852,  0.14269364,  0.16650787,  0.1855543 ,\n",
       "           0.2350014 ,  0.3678171 ,  0.39302558,  0.3879875 ,  0.44357872,\n",
       "           0.45201635,  0.40440977,  0.20598464,  0.14794646,  0.17731321,\n",
       "           0.13817048,  0.4697745 ,  0.45843282,  0.46963948,  0.5241121 ,\n",
       "           0.39353222,  0.31840476,  0.37324974,  0.41971803,  0.4270836 ,\n",
       "           0.3988698 ,  0.38187438,  0.41169012,  0.46919328,  0.48914662,\n",
       "           0.17812736,  0.18995196,  0.12993649,  0.1595785 ,  0.3303874 ,\n",
       "           0.2302872 ,  0.26480314,  0.3179096 ,  0.19441801,  0.1850421 ,\n",
       "           0.09766144,  0.0980985 ,  0.03981192,  0.00199953,  0.10989131,\n",
       "           0.09633672,  0.12515035,  0.11456916,  0.16357535,  0.15477404,\n",
       "           0.10459609,  0.09089172,  0.1729559 ,  0.15776205,  0.45701623,\n",
       "           0.44541875,  0.43829376,  0.3256668 ,  0.26564837,  0.22628313,\n",
       "           0.24399976,  0.18718515,  0.34824166,  0.3505458 ,  0.41567463,\n",
       "           0.3851124 ,  0.3728388 ,  0.34711578,  0.34388816,  0.38690248,\n",
       "           0.35946885,  0.32915938,  0.4268343 ,  0.45494592,  0.4452151 ,\n",
       "           0.42265207,  0.2133502 ,  0.25841475,  0.13010824,  0.21235132,\n",
       "           0.35216254,  0.30680984,  0.38019443,  0.40006334,  0.30657458,\n",
       "           0.38176876,  0.10059401,  0.0747945 ,  0.01851374,  0.00650671,\n",
       "           0.16905141,  0.15969011,  0.14283185,  0.14584957,  0.14543602,\n",
       "           0.17140794,  0.15489832,  0.16719988,  0.16813907,  0.17544293,\n",
       "           0.40282965,  0.38244674,  0.20063548,  0.2379442 ,  0.22030957,\n",
       "           0.22913115,  0.43045563,  0.34902442,  0.35490796,  0.4584947 ,\n",
       "           0.37082964,  0.43101513,  0.3802445 ,  0.41138113,  0.3969468 ,\n",
       "           0.30971506,  0.39028746,  0.41363335,  0.4915539 ,  0.45451856,\n",
       "           0.17114756,  0.12155817,  0.14132407,  0.11679025,  0.3443045 ,\n",
       "           0.36162043,  0.30544746,  0.34909543,  0.25308716,  0.24860187,\n",
       "           0.06392647,  0.12159473,  0.03902598,  0.00228837,  0.14414144,\n",
       "           0.09271942,  0.12875009,  0.09428233,  0.14409207,  0.13886026,\n",
       "           0.17005655,  0.10013774,  0.15526232,  0.13632122,  0.24154288,\n",
       "           0.16989322,  0.1930157 ,  0.16739671,  0.34638458,  0.36553788,\n",
       "           0.34908006,  0.42298353,  0.3672951 ,  0.35738993,  0.36897108,\n",
       "           0.44195643,  0.3928566 ,  0.32049227,  0.37524024,  0.385175  ,\n",
       "           0.4729857 ,  0.38534385,  0.20802373,  0.08535282,  0.14813599,\n",
       "           0.10233293,  0.36266983,  0.30296126,  0.3516545 ,  0.29732907,\n",
       "           0.24401248,  0.23337936,  0.0545384 ,  0.02581456,  0.05814393,\n",
       "           0.00675778,  0.1918073 ,  0.1422587 ,  0.161407  ,  0.15939963,\n",
       "           0.15296796,  0.18627694,  0.18159764,  0.1546104 ,  0.17567101,\n",
       "           0.19237985,  0.36345968,  0.30546838,  0.15464279,  0.18396813,\n",
       "           0.25070536,  0.2199498 ,  0.23503791,  0.18496682,  0.19072308,\n",
       "           0.20533776,  0.12765115,  0.14484756,  0.20553267,  0.22681707,\n",
       "           0.16790578,  0.15238726,  0.28622165,  0.32864305,  0.17361693,\n",
       "           0.13456067,  0.20843977,  0.16626318,  0.26780325,  0.32194448,\n",
       "           0.3352415 ,  0.32797965,  0.32488576,  0.25111648, -0.00766346,\n",
       "          -0.01274961,  0.10597074,  0.14495668,  0.11450118,  0.09831938,\n",
       "           0.12957287,  0.09760441,  0.11263369,  0.10659823,  0.12957287,\n",
       "           0.09937453,  0.17185347,  0.20956261,  0.36065358,  0.3193553 ,\n",
       "           0.25308058,  0.1594689 ,  0.23013347,  0.16349956,  0.18879582,\n",
       "           0.20120664,  0.29649577,  0.19863036,  0.14389858,  0.16003144,\n",
       "           0.16384211,  0.26306197,  0.20969369,  0.15841678,  0.17184164,\n",
       "           0.21616963,  0.3513983 ,  0.35459653,  0.30600527,  0.28200555,\n",
       "           0.28606203,  0.2787871 , -0.05157757, -0.02876064,  0.09669597,\n",
       "           0.14850591,  0.10873537,  0.10906777,  0.10881376,  0.12320408,\n",
       "           0.11528245,  0.08141071,  0.12213628,  0.10504062,  0.5572058 ,\n",
       "           0.5529617 ,  0.4636011 ,  0.5155126 ,  0.4634679 ,  0.43480903,\n",
       "           0.4577669 ,  0.40835273,  0.48756617,  0.35225976,  0.47578388,\n",
       "           0.4402123 ,  0.18393235,  0.17817724,  0.16069153,  0.15319826,\n",
       "           0.25687522,  0.21766168,  0.3898784 ,  0.362275  ,  0.19092125,\n",
       "           0.20410913,  0.09720147,  0.20815912, -0.00368789,  0.03462024,\n",
       "           0.08865416,  0.10892135,  0.11590278,  0.10259558,  0.12335652,\n",
       "           0.13561739,  0.10748824,  0.09207167,  0.12335652,  0.145343  ,\n",
       "           0.38793325,  0.36109972,  0.40311396,  0.40450275,  0.4109457 ,\n",
       "           0.36170566,  0.37328333,  0.34111643,  0.45101184,  0.40078998,\n",
       "           0.12426212,  0.23187594,  0.17021498,  0.1779153 ,  0.29342228,\n",
       "           0.2814763 ,  0.39776707,  0.3922932 ,  0.3225781 ,  0.28098476,\n",
       "           0.20609447,  0.29213285,  0.03788629,  0.04638107,  0.11274219,\n",
       "           0.15668193,  0.13095313,  0.13210757,  0.15533027,  0.16065899,\n",
       "           0.12253857,  0.11803748,  0.15533027,  0.17094253,  0.53315073,\n",
       "           0.46151984,  0.39390948,  0.5163158 ,  0.5171485 ,  0.36473554,\n",
       "           0.42706084,  0.41783205,  0.13740633,  0.18569762,  0.14203435,\n",
       "           0.14960389,  0.35942954,  0.35269803,  0.43372285,  0.36732572,\n",
       "           0.27000943,  0.23449837,  0.16919905,  0.17506757,  0.03147796,\n",
       "           0.03874134,  0.17224935,  0.13464956,  0.1543042 ,  0.13819061,\n",
       "           0.16017751,  0.1806954 ,  0.16085128,  0.14231479,  0.16017751,\n",
       "           0.18686303,  0.4484247 ,  0.445963  ,  0.40618506,  0.43359435,\n",
       "           0.5101209 ,  0.47502893,  0.1129638 ,  0.17459475,  0.11113475,\n",
       "           0.11020212,  0.32833004,  0.31146923,  0.3563064 ,  0.3826267 ,\n",
       "           0.20035139,  0.2686814 ,  0.11739776,  0.11440592,  0.02958651,\n",
       "           0.06208096,  0.11669004,  0.1252969 ,  0.12846762,  0.15319148,\n",
       "           0.16396397,  0.18740106,  0.11446047,  0.14854285,  0.17334452,\n",
       "           0.20622262,  0.3264403 ,  0.33492914,  0.40142342,  0.43348205,\n",
       "           0.10841985,  0.11478461,  0.0685935 ,  0.1193697 ,  0.24190342,\n",
       "           0.23116049,  0.34175462,  0.33711922,  0.19576839,  0.19110112,\n",
       "           0.06251462,  0.06000592,  0.01113762,  0.02012004,  0.11416343,\n",
       "           0.09923458,  0.12411591,  0.12077387,  0.1470437 ,  0.15659103,\n",
       "           0.11831827,  0.11182074,  0.15642425,  0.17103522,  0.47236884,\n",
       "           0.4006145 ,  0.32912442,  0.189928  ,  0.18681647,  0.1246628 ,\n",
       "           0.41027147,  0.44021928,  0.41636926,  0.41553372,  0.30372906,\n",
       "           0.29573435,  0.18617892,  0.19341224,  0.03827514,  0.04549466,\n",
       "           0.13054982,  0.15834308,  0.11372082,  0.15578213,  0.16074792,\n",
       "           0.17715715,  0.1101155 ,  0.17098871,  0.16074792,  0.19511217,\n",
       "           0.12253711,  0.15479413,  0.13222092,  0.13018227,  0.38480505,\n",
       "           0.34750265,  0.3558958 ,  0.31450453,  0.20345376,  0.18735021,\n",
       "           0.08520843, -0.01407522,  0.00868686,  0.02214713,  0.13986164,\n",
       "           0.09503728,  0.13499865,  0.11002104,  0.11371237,  0.14683902,\n",
       "           0.129201  ,  0.12077241,  0.12703483,  0.16335116,  0.26378852,\n",
       "           0.26223767,  0.20428978,  0.22777474,  0.17077982,  0.2055091 ,\n",
       "           0.2381562 ,  0.20302175,  0.57324916,  0.44462848,  0.0408074 ,\n",
       "           0.05922739,  0.14696766,  0.1659565 ,  0.10635066,  0.1297719 ,\n",
       "           0.17539912,  0.12522112,  0.17611352,  0.17216703,  0.16672593,\n",
       "           0.15362817,  0.16371691,  0.20478664,  0.20237148,  0.13455322,\n",
       "           0.14687672,  0.17717989,  0.33090967,  0.39807284,  0.00154325,\n",
       "          -0.03371614,  0.05143376,  0.08146207,  0.0760057 ,  0.08351149,\n",
       "           0.12609446,  0.09629303,  0.08643186,  0.09031959,  0.11742125,\n",
       "           0.10522921,  0.29159337,  0.25314716,  0.26997483,  0.28523764,\n",
       "           0.24619488,  0.29354784,  0.03180659,  0.04536733,  0.15037377,\n",
       "           0.18436208,  0.1190715 ,  0.14528647,  0.12918174,  0.15447825,\n",
       "           0.15100165,  0.18607567,  0.13440846,  0.16507088,  0.31454477,\n",
       "           0.3078539 ,  0.23683195,  0.2911869 ,  0.03592859,  0.04350111,\n",
       "           0.11863898,  0.14601374,  0.14562309,  0.14775307,  0.16813207,\n",
       "           0.2092148 ,  0.14375561,  0.12390867,  0.17751262,  0.2025075 ,\n",
       "           0.19241937,  0.27154067,  0.01730645, -0.01988354,  0.14137617,\n",
       "           0.17098233,  0.13073334,  0.12267831,  0.1302687 ,  0.11367363,\n",
       "           0.14518951,  0.1474416 ,  0.1302687 ,  0.11368696, -0.01317698,\n",
       "          -0.01031405,  0.08202313,  0.16957927,  0.09521626,  0.1066238 ,\n",
       "           0.14443186,  0.10775554,  0.08659659,  0.10734993,  0.13575867,\n",
       "           0.08545719,  0.04087573,  0.06723244,  0.04759264,  0.04489669,\n",
       "           0.04268172,  0.0433726 ,  0.03773254,  0.06374042,  0.04408066,\n",
       "           0.04485723,  0.17462635,  0.1974878 ,  0.18302923,  0.15324342,\n",
       "           0.1525998 ,  0.17944989,  0.18263844,  0.16765553,  0.2166206 ,\n",
       "           0.17712492,  0.20028436,  0.19283627,  0.2162298 ,  0.17860955,\n",
       "           0.16141826,  0.18498011,  0.1736553 ,  0.17580424,  0.18458931,\n",
       "           0.16290289], dtype=float32),\n",
       "   'feature_importances': array([0.00015955, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ], dtype=float32),\n",
       "   'model_json': None}]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='xgboost',\n",
    "              feature_type=[{'transcriptome': None}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('bayes', 'pearson'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8e5dd13-8696-4451-85e0-d1caf612da16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name transcriptome\n",
      "processing_type None\n",
      "feature_name structural\n",
      "processing_type spectral_A_20\n",
      "X shape (114, 11073)\n",
      "\n",
      " Test fold num: 1\n",
      "(12070, 22146) (12070,) (812, 22146) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "2\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.969, test=0.866) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.973, test=0.765) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.970, test=0.873) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.848, test=0.850) total time=   5.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.845, test=0.731) total time=   5.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.839, test=0.824) total time=   5.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.865, test=0.858) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.860, test=0.729) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.852, test=0.821) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.811, test=0.828) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.808, test=0.643) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.785, test=0.781) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.700, test=0.753) total time=   0.6s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.714, test=0.469) total time=   0.6s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.689, test=0.729) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.880) total time=   5.7s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.827) total time=   5.7s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.874) total time=   5.7s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.951, test=0.898) total time=   0.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.948, test=0.802) total time=   0.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.949, test=0.890) total time=   0.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.893, test=0.871) total time=   4.4s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.888, test=0.806) total time=   4.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.886, test=0.844) total time=   4.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.738, test=0.785) total time=   2.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.748, test=0.524) total time=   2.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.716, test=0.725) total time=   2.0s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.755, test=0.784) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.772, test=0.562) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.759, test=0.765) total time=   2.7s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.6), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 4), ('n_estimators', 50), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0.0001), ('reg_lambda', 0.01), ('subsample', 1), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8633208419924943\n",
      "\n",
      "Train Metrics: {'mse': 0.005729807659153166, 'mae': 0.05184347689869086, 'r2': 0.8285951649190557, 'pearson_corr': 0.9146143028471779}\n",
      "Test Metrics: {'mse': 0.008222502204808278, 'mae': 0.06087608310945559, 'r2': 0.7234144005532671, 'pearson_corr': 0.8535659012902297, 'connectome_corr': 0.785929393646174, 'connectome_r2': 0.5800107129607752, 'geodesic_distance': 5.184341943956099}\n",
      "BEST VAL SCORE 0.8633208419924943\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 4, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 50, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0.0001, 'reg_lambda': 0.01, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 1, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_114503-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_transcriptome+structural_spectral_A_20_predFC_random_fold0_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.86332</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_transcriptome+structural_spectral_A_20_predFC_random_fold0_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_114503-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 42.6%\n",
      "RAM Usage: 7.2%\n",
      "Available RAM: 934.9G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 43% |\n",
      "\n",
      " Test fold num: 2\n",
      "(12070, 22146) (12070,) (812, 22146) (812,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "3\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.961, test=0.783) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.959, test=0.717) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.954, test=0.866) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.803, test=0.787) total time=   5.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.804, test=0.686) total time=   5.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.798, test=0.823) total time=   5.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.809, test=0.773) total time=   2.6s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.811, test=0.674) total time=   2.6s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.799, test=0.825) total time=   2.6s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.762, test=0.754) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.758, test=0.594) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.749, test=0.806) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.640, test=0.651) total time=   0.6s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.641, test=0.433) total time=   0.6s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.624, test=0.714) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.778) total time=   5.7s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.785) total time=   5.7s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.860) total time=   5.7s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.933, test=0.834) total time=   0.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.932, test=0.800) total time=   0.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.3, max_depth=4, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=0.0001, reg_lambda=0.01, subsample=1, tree_method=gpu_hist, verbosity=0;, score=(train=0.928, test=0.898) total time=   0.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.848, test=0.817) total time=   4.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.845, test=0.757) total time=   4.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.844, test=0.842) total time=   4.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.691, test=0.689) total time=   2.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.695, test=0.519) total time=   2.1s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=4, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.673, test=0.757) total time=   2.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.712, test=0.688) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.717, test=0.551) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.01, reg_lambda=0.1, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.696, test=0.758) total time=   2.7s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  OrderedDict([('colsample_bytree', 0.6), ('device', 'cuda'), ('learning_rate', 0.3), ('max_depth', 4), ('n_estimators', 50), ('n_gpus', -1), ('random_state', 42), ('reg_alpha', 0.0001), ('reg_lambda', 0.01), ('subsample', 1), ('tree_method', 'gpu_hist'), ('verbosity', 0)])\n",
      "Best Cross-Validation Score:  0.8436636814627807\n",
      "\n",
      "Train Metrics: {'mse': 0.005881388020867061, 'mae': 0.05291421384840757, 'r2': 0.821212232181358, 'pearson_corr': 0.9116186169343279}\n",
      "Test Metrics: {'mse': 0.007048312111400806, 'mae': 0.05204744828809472, 'r2': 0.8138792852151002, 'pearson_corr': 0.9038405727613418, 'connectome_corr': 0.8044654345346139, 'connectome_r2': 0.555619084900371, 'geodesic_distance': 4.301747937993443}\n",
      "BEST VAL SCORE 0.8436636814627807\n",
      "BEST MODEL PARAMS {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6, 'device': 'cuda', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.3, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 4, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 50, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 0.0001, 'reg_lambda': 0.01, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 1, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': 0, 'n_gpus': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/asr655/neuroinformatics/GeneEx2Conn/wandb/run-20241202_114642-v4ird0tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">xgboost_transcriptome+structural_spectral_A_20_predFC_random_fold1_final_eval</a></strong> to <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/sweeps/gx2ldexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.84366</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost_transcriptome+structural_spectral_A_20_predFC_random_fold1_final_eval</strong> at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn/runs/v4ird0tn</a><br/> View project at: <a href='https://wandb.ai/alexander-ratzan-new-york-university/gx2conn' target=\"_blank\">https://wandb.ai/alexander-ratzan-new-york-university/gx2conn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_114642-v4ird0tn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics logged successfully.\n",
      "CPU Usage: 43.4%\n",
      "RAM Usage: 7.2%\n",
      "Available RAM: 934.6G\n",
      "Total RAM: 1007.0G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 43% |\n",
      "\n",
      " Test fold num: 3\n",
      "(12126, 22146) (12126,) (756, 22146) (756,)\n",
      "SEARCH METHOD ('bayes', 'pearson')\n",
      "1\n",
      "2\n",
      "4\n",
      "ACCELERATING\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.971, test=0.812) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.969, test=0.892) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=3, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=0.1, reg_lambda=0.0001, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.968, test=0.875) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.846, test=0.795) total time=   5.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.844, test=0.832) total time=   5.3s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.001, max_depth=6, n_estimators=250, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=0.01, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.841, test=0.822) total time=   5.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.855, test=0.788) total time=   2.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.850, test=0.835) total time=   2.7s\n",
      "[CV 3/3] END colsample_bytree=0.6, device=cuda, learning_rate=0.01, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0, subsample=0.8, tree_method=gpu_hist, verbosity=0;, score=(train=0.850, test=0.827) total time=   2.7s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.821, test=0.761) total time=   2.8s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.822, test=0.818) total time=   2.8s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.001, max_depth=5, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.816, test=0.805) total time=   2.8s\n",
      "[CV 1/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.731, test=0.654) total time=   0.7s\n",
      "[CV 2/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.717, test=0.744) total time=   0.7s\n",
      "[CV 3/3] END colsample_bytree=1, device=cuda, learning_rate=0.01, max_depth=3, n_estimators=50, n_gpus=-1, random_state=42, reg_alpha=1, reg_lambda=1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=0.713, test=0.726) total time=   0.6s\n",
      "[CV 1/3] END colsample_bytree=0.8, device=cuda, learning_rate=0.3, max_depth=7, n_estimators=150, n_gpus=-1, random_state=42, reg_alpha=0, reg_lambda=0.1, subsample=0.6, tree_method=gpu_hist, verbosity=0;, score=(train=1.000, test=0.804) total time=   5.8s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m single_sim_run(\n\u001b[1;32m      2\u001b[0m               cv_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m               random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      4\u001b[0m               model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m               feature_type\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscriptome\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructural\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspectral_A_20\u001b[39m\u001b[38;5;124m'\u001b[39m}],\n\u001b[1;32m      6\u001b[0m               connectome_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m               use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m               use_shared_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m               test_shared_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m               save_sim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m               search_method\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbayes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m               track_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m               )\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim_run.py:211\u001b[0m, in \u001b[0;36msingle_sim_run\u001b[0;34m(feature_type, cv_type, model_type, use_gpu, connectome_target, feature_interactions, use_shared_regions, test_shared_regions, resolution, random_seed, save_sim, search_method, save_model_json, track_wandb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Structural\u001b[39;00m\n\u001b[1;32m    195\u001b[0m sim \u001b[38;5;241m=\u001b[39m Simulation(\n\u001b[1;32m    196\u001b[0m                 feature_type\u001b[38;5;241m=\u001b[39mfeature_type,\n\u001b[1;32m    197\u001b[0m                 cv_type\u001b[38;5;241m=\u001b[39mcv_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 save_model_json\u001b[38;5;241m=\u001b[39msave_model_json\n\u001b[1;32m    209\u001b[0m             )\n\u001b[0;32m--> 211\u001b[0m sim\u001b[38;5;241m.\u001b[39mrun_sim(search_method, track_wandb)\n\u001b[1;32m    212\u001b[0m single_model_results\u001b[38;5;241m.\u001b[39mappend(sim\u001b[38;5;241m.\u001b[39mresults)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Save sim data\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim.py:334\u001b[0m, in \u001b[0;36mSimulation.run_sim\u001b[0;34m(self, search_method, track_wandb)\u001b[0m\n\u001b[1;32m    332\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, val_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     best_model, best_val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_innercv(train_indices, test_indices, train_network_dict, search_method\u001b[38;5;241m=\u001b[39msearch_method, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    335\u001b[0m     best_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m    336\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim.py:285\u001b[0m, in \u001b[0;36mSimulation.run_innercv\u001b[0;34m(self, train_indices, test_indices, train_network_dict, search_method, n_iter)\u001b[0m\n\u001b[1;32m    282\u001b[0m     param_search, X_combined, Y_combined \u001b[38;5;241m=\u001b[39m bayes_search_init(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_acceleration, model, X_combined, Y_combined, param_dist, train_test_indices, n_iter\u001b[38;5;241m=\u001b[39mn_iter, metric\u001b[38;5;241m=\u001b[39mmetric)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV on the combined data\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m param_search\u001b[38;5;241m.\u001b[39mfit(X_combined, Y_combined)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Display comprehensive results\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mParameter Search CV Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/skopt/searchcv.py:542\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/skopt/searchcv.py:599\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 599\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(\n\u001b[1;32m    600\u001b[0m         search_space,\n\u001b[1;32m    601\u001b[0m         optimizer,\n\u001b[1;32m    602\u001b[0m         score_name,\n\u001b[1;32m    603\u001b[0m         evaluate_candidates,\n\u001b[1;32m    604\u001b[0m         n_points\u001b[38;5;241m=\u001b[39mn_points_adjusted,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/skopt/searchcv.py:453\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    451\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m--> 453\u001b[0m all_results \u001b[38;5;241m=\u001b[39m evaluate_candidates(params_dict)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1090\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m (\n\u001b[1;32m   1082\u001b[0m     model,\n\u001b[1;32m   1083\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1089\u001b[0m )\n\u001b[0;32m-> 1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1091\u001b[0m     params,\n\u001b[1;32m   1092\u001b[0m     train_dmatrix,\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1094\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1095\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1096\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1097\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1098\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1099\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1100\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1101\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/xgboost/core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[0;32m-> 2051\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2052\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2053\u001b[0m         )\n\u001b[1;32m   2054\u001b[0m     )\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='xgboost',\n",
    "              feature_type=[{'transcriptome': None}, {'structural': 'spectral_A_20'}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('bayes', 'pearson'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75079b-dff2-42f5-8e0a-05c3df6c9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='dynamic_nn',\n",
    "              feature_type=[{'transcriptome': None}],\n",
    "              connectome_target='FC',\n",
    "              use_gpu=True,\n",
    "              use_shared_regions=True,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=False,\n",
    "              search_method=('grid', 'mse'),\n",
    "              track_wandb=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5497b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing eig of laplacian\n",
      "computing eig of adjacency\n",
      "feature_type [{'transcriptome': None}, {'structural': 'spectral_A_20'}]\n",
      "feature_name transcriptome\n",
      "processing_type None\n",
      "feature_name structural\n",
      "processing_type spectral_A_20\n",
      "features ['transcriptome', 'structural_spectral_A_20']\n",
      "feature transcriptome\n",
      "feature structural_spectral_A_20\n",
      "X shape (114, 11073)\n",
      "\n",
      " Test fold num: 1\n",
      "(7140, 22146) (7140,) (812, 22146) (812,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "2\n",
      "3\n",
      "4\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.026 total time=   3.2s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.028 total time=   3.1s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.041 total time=   3.2s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.028 total time=   1.3s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.034 total time=   1.3s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.028 total time=   1.3s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.034 total time=   1.3s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.026 total time=   1.2s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.028 total time=   1.3s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.034 total time=   1.3s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.028 total time=   1.3s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.034 total time=   1.3s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.028 total time=   1.3s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.034 total time=   1.3s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   1.7s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.033 total time=   1.6s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   1.4s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.029 total time=   1.4s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.02714152611248866\n",
      "\n",
      "Train Metrics: {'mse': 0.021032101193248658, 'mae': 0.10500131429428843, 'r2': -1.139266997025029, 'pearson_corr': 0.6331792766809794, 'connectome_corr': 0.4598820738973434, 'connectome_r2': -1.2561298441624122, 'geodesic_distance': 13.408910811166907}\n",
      "Test Metrics: {'mse': 0.021574909319022582, 'mae': 0.10984827356812175, 'r2': -0.8116136167331731, 'pearson_corr': 0.5387020135228298, 'connectome_corr': 0.4097680692386645, 'connectome_r2': -1.0358187236230398, 'geodesic_distance': 6.421698352595828}\n",
      "BEST VAL SCORE -0.02714152611248866\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 16.4%\n",
      "RAM Usage: 29.5%\n",
      "Available RAM: 266.0G\n",
      "Total RAM: 377.1G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      " Test fold num: 2\n",
      "(7140, 22146) (7140,) (812, 22146) (812,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "3\n",
      "4\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.023 total time=   3.4s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.029 total time=   3.5s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.025 total time=   3.6s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.023 total time=   1.3s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.029 total time=   1.4s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.023 total time=   1.4s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.029 total time=   1.4s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.023 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.029 total time=   1.4s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.025 total time=   1.5s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.023 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.029 total time=   1.4s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.025 total time=   1.5s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.023 total time=   1.3s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.029 total time=   1.4s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.023 total time=   1.4s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.028 total time=   1.4s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.023 total time=   1.4s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.028 total time=   1.4s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.024 total time=   1.4s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.0248969403519967\n",
      "\n",
      "Train Metrics: {'mse': 0.02057781196789063, 'mae': 0.10329275487118939, 'r2': -0.44778701278305116, 'pearson_corr': 0.5866899932380918, 'connectome_corr': 0.42127572301529653, 'connectome_r2': -0.5183517609706955, 'geodesic_distance': 13.870963012371796}\n",
      "Test Metrics: {'mse': 0.023383092455993966, 'mae': 0.11287381062531544, 'r2': -3.3858769196588763, 'pearson_corr': 0.6251578357343585, 'connectome_corr': 0.508181804393245, 'connectome_r2': -4.079728095014497, 'geodesic_distance': 6.075958930897474}\n",
      "BEST VAL SCORE -0.0248969403519967\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 16.5%\n",
      "RAM Usage: 30.1%\n",
      "Available RAM: 263.5G\n",
      "Total RAM: 377.1G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      " Test fold num: 3\n",
      "(7310, 22146) (7310,) (756, 22146) (756,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "2\n",
      "4\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.024 total time=   3.6s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.025 total time=   3.7s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.025 total time=   3.8s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.024 total time=   1.5s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.024 total time=   1.5s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.024 total time=   1.5s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.024 total time=   1.4s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.023 total time=   1.4s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.025 total time=   1.4s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.024 total time=   1.3s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.022 total time=   1.3s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.024 total time=   1.3s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.025 total time=   1.3s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.023830591257576472\n",
      "\n",
      "Train Metrics: {'mse': 0.018657489763462817, 'mae': 0.09982683465927214, 'r2': -1.2686676508584056, 'pearson_corr': 0.660568633044771, 'connectome_corr': 0.49347804968862163, 'connectome_r2': -1.3799254031042436, 'geodesic_distance': 13.44571923665314}\n",
      "Test Metrics: {'mse': 0.026071999695419314, 'mae': 0.12180803788932486, 'r2': -0.09419271342604918, 'pearson_corr': 0.4195403315575419, 'connectome_corr': 0.2328397948720895, 'connectome_r2': -0.3555397065229101, 'geodesic_distance': 6.5205164905396025}\n",
      "BEST VAL SCORE -0.023830591257576472\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 16.5%\n",
      "RAM Usage: 30.2%\n",
      "Available RAM: 263.3G\n",
      "Total RAM: 377.1G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      " Test fold num: 4\n",
      "(7310, 22146) (7310,) (756, 22146) (756,)\n",
      "SEARCH METHOD ('grid', 'mse')\n",
      "1\n",
      "2\n",
      "3\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END .............alpha=0, solver=auto;, score=-0.024 total time=   3.1s\n",
      "[CV 2/3] END .............alpha=0, solver=auto;, score=-0.025 total time=   3.0s\n",
      "[CV 3/3] END .............alpha=0, solver=auto;, score=-0.027 total time=   3.2s\n",
      "[CV 1/3] END .........alpha=0.001, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 2/3] END .........alpha=0.001, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 3/3] END .........alpha=0.001, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 1/3] END ..........alpha=0.01, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 2/3] END ..........alpha=0.01, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 3/3] END ..........alpha=0.01, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 1/3] END ...........alpha=0.1, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=0.1, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 3/3] END ...........alpha=0.1, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 1/3] END ...........alpha=1.0, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=1.0, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 3/3] END ...........alpha=1.0, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 1/3] END ............alpha=10, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 2/3] END ............alpha=10, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 3/3] END ............alpha=10, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 1/3] END ...........alpha=100, solver=auto;, score=-0.024 total time=   1.3s\n",
      "[CV 2/3] END ...........alpha=100, solver=auto;, score=-0.025 total time=   1.3s\n",
      "[CV 3/3] END ...........alpha=100, solver=auto;, score=-0.026 total time=   1.3s\n",
      "[CV 1/3] END ..........alpha=1000, solver=auto;, score=-0.023 total time=   1.3s\n",
      "[CV 2/3] END ..........alpha=1000, solver=auto;, score=-0.024 total time=   1.3s\n",
      "[CV 3/3] END ..........alpha=1000, solver=auto;, score=-0.027 total time=   1.3s\n",
      "\n",
      "Parameter Search CV Results:\n",
      "=============================\n",
      "Best Parameters:  {'alpha': 1000, 'solver': 'auto'}\n",
      "Best Cross-Validation Score:  -0.02457597320735146\n",
      "\n",
      "Train Metrics: {'mse': 0.02127987826931947, 'mae': 0.10500310714815986, 'r2': -0.8717232405556377, 'pearson_corr': 0.611714979310548, 'connectome_corr': 0.47874871303181044, 'connectome_r2': -0.9863603332788347, 'geodesic_distance': 12.700045558568583}\n",
      "Test Metrics: {'mse': 0.025004078413256397, 'mae': 0.11205724440102598, 'r2': -2.7678658059296204, 'pearson_corr': 0.548938108469077, 'connectome_corr': 0.2803813049298135, 'connectome_r2': -3.2599146585393646, 'geodesic_distance': 5.2425304384341525}\n",
      "BEST VAL SCORE -0.02457597320735146\n",
      "BEST MODEL PARAMS {'alpha': 1000, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "CPU Usage: 16.0%\n",
      "RAM Usage: 29.3%\n",
      "Available RAM: 266.6G\n",
      "Total RAM: 377.1G\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "Simulation results have been saved to  /scratch/asr655/neuroinformatics/GeneEx2Conn/sim/sim_results/transcriptome_structural_spectral_A_20_FC_ridge_random_42_grid_mse_search.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.021032101193248658,\n",
       "    'mae': 0.10500131429428843,\n",
       "    'r2': -1.139266997025029,\n",
       "    'pearson_corr': 0.6331792766809794,\n",
       "    'connectome_corr': 0.4598820738973434,\n",
       "    'connectome_r2': -1.2561298441624122,\n",
       "    'geodesic_distance': 13.408910811166907},\n",
       "   'best_val_score': -0.02714152611248866,\n",
       "   'test_metrics': {'mse': 0.021574909319022582,\n",
       "    'mae': 0.10984827356812175,\n",
       "    'r2': -0.8116136167331731,\n",
       "    'pearson_corr': 0.5387020135228298,\n",
       "    'connectome_corr': 0.4097680692386645,\n",
       "    'connectome_r2': -1.0358187236230398,\n",
       "    'geodesic_distance': 6.421698352595828},\n",
       "   'y_true': array([ 0.25718  ,  0.25718  ,  0.21614  ,  0.21614  ,  0.17879  ,\n",
       "           0.17879  ,  0.20003  ,  0.20003  ,  0.18572  ,  0.18572  ,\n",
       "           0.25026  ,  0.25026  ,  0.11669  ,  0.11669  ,  0.24455  ,\n",
       "           0.24455  ,  0.11424  ,  0.11424  ,  0.29925  ,  0.29925  ,\n",
       "           0.18083  ,  0.18083  ,  0.18606  ,  0.18606  ,  0.32224  ,\n",
       "           0.32224  ,  0.18932  ,  0.18932  ,  0.40234  ,  0.40234  ,\n",
       "           0.18223  ,  0.18223  ,  0.26495  ,  0.26495  ,  0.23553  ,\n",
       "           0.23553  ,  0.21608  ,  0.21608  ,  0.059681 ,  0.059681 ,\n",
       "           0.21966  ,  0.21966  ,  0.14268  ,  0.14268  ,  0.22835  ,\n",
       "           0.22835  ,  0.42801  ,  0.42801  ,  0.21521  ,  0.21521  ,\n",
       "           0.057658 ,  0.057658 ,  0.024612 ,  0.024612 ,  0.063759 ,\n",
       "           0.063759 ,  0.37308  ,  0.37308  ,  0.34132  ,  0.34132  ,\n",
       "           0.32791  ,  0.32791  ,  0.27542  ,  0.27542  ,  0.36759  ,\n",
       "           0.36759  ,  0.32945  ,  0.32945  ,  0.19856  ,  0.19856  ,\n",
       "           0.24324  ,  0.24324  ,  0.28955  ,  0.28955  ,  0.30675  ,\n",
       "           0.30675  ,  0.23096  ,  0.23096  ,  0.26071  ,  0.26071  ,\n",
       "           0.73698  ,  0.73698  ,  0.4264   ,  0.4264   ,  0.27143  ,\n",
       "           0.27143  ,  0.45758  ,  0.45758  ,  0.36727  ,  0.36727  ,\n",
       "           0.35085  ,  0.35085  ,  0.25663  ,  0.25663  ,  0.27071  ,\n",
       "           0.27071  ,  0.33455  ,  0.33455  ,  0.20266  ,  0.20266  ,\n",
       "           0.31722  ,  0.31722  ,  0.14571  ,  0.14571  ,  0.027941 ,\n",
       "           0.027941 ,  0.060493 ,  0.060493 ,  0.15441  ,  0.15441  ,\n",
       "           0.6665   ,  0.6665   ,  0.57117  ,  0.57117  ,  0.38161  ,\n",
       "           0.38161  ,  0.37302  ,  0.37302  ,  0.28516  ,  0.28516  ,\n",
       "           0.21945  ,  0.21945  ,  0.17296  ,  0.17296  ,  0.20472  ,\n",
       "           0.20472  ,  0.21497  ,  0.21497  ,  0.19057  ,  0.19057  ,\n",
       "           0.18716  ,  0.18716  ,  0.31581  ,  0.31581  ,  0.36647  ,\n",
       "           0.36647  ,  0.39506  ,  0.39506  ,  0.30871  ,  0.30871  ,\n",
       "           0.41039  ,  0.41039  ,  0.3606   ,  0.3606   ,  0.1418   ,\n",
       "           0.1418   ,  0.13914  ,  0.13914  ,  0.22276  ,  0.22276  ,\n",
       "           0.27225  ,  0.27225  ,  0.24408  ,  0.24408  ,  0.17506  ,\n",
       "           0.17506  ,  0.072852 ,  0.072852 ,  0.051264 ,  0.051264 ,\n",
       "           0.14285  ,  0.14285  ,  0.71373  ,  0.71373  ,  0.51352  ,\n",
       "           0.51352  ,  0.50611  ,  0.50611  ,  0.40024  ,  0.40024  ,\n",
       "           0.21241  ,  0.21241  ,  0.14927  ,  0.14927  ,  0.11245  ,\n",
       "           0.11245  ,  0.17597  ,  0.17597  ,  0.10735  ,  0.10735  ,\n",
       "           0.1207   ,  0.1207   ,  0.29097  ,  0.29097  ,  0.40723  ,\n",
       "           0.40723  ,  0.53582  ,  0.53582  ,  0.33953  ,  0.33953  ,\n",
       "           0.51564  ,  0.51564  ,  0.49405  ,  0.49405  ,  0.15535  ,\n",
       "           0.15535  ,  0.054372 ,  0.054372 ,  0.27296  ,  0.27296  ,\n",
       "           0.26516  ,  0.26516  ,  0.17728  ,  0.17728  ,  0.14978  ,\n",
       "           0.14978  ,  0.081124 ,  0.081124 ,  0.059407 ,  0.059407 ,\n",
       "           0.17772  ,  0.17772  ,  0.41042  ,  0.41042  ,  0.37986  ,\n",
       "           0.37986  ,  0.2326   ,  0.2326   ,  0.23302  ,  0.23302  ,\n",
       "           0.1023   ,  0.1023   ,  0.22117  ,  0.22117  ,  0.18856  ,\n",
       "           0.18856  ,  0.22485  ,  0.22485  ,  0.20126  ,  0.20126  ,\n",
       "           0.27932  ,  0.27932  ,  0.36743  ,  0.36743  ,  0.46038  ,\n",
       "           0.46038  ,  0.30677  ,  0.30677  ,  0.36399  ,  0.36399  ,\n",
       "           0.35942  ,  0.35942  ,  0.081346 ,  0.081346 ,  0.12832  ,\n",
       "           0.12832  ,  0.16343  ,  0.16343  ,  0.30423  ,  0.30423  ,\n",
       "           0.22562  ,  0.22562  ,  0.18438  ,  0.18438  ,  0.10238  ,\n",
       "           0.10238  ,  0.046463 ,  0.046463 ,  0.12617  ,  0.12617  ,\n",
       "           0.57758  ,  0.57758  ,  0.25516  ,  0.25516  ,  0.20396  ,\n",
       "           0.20396  ,  0.082235 ,  0.082235 ,  0.15964  ,  0.15964  ,\n",
       "           0.16099  ,  0.16099  ,  0.13542  ,  0.13542  ,  0.16695  ,\n",
       "           0.16695  ,  0.23593  ,  0.23593  ,  0.40179  ,  0.40179  ,\n",
       "           0.83487  ,  0.83487  ,  0.50918  ,  0.50918  ,  0.49254  ,\n",
       "           0.49254  ,  0.53114  ,  0.53114  ,  0.10013  ,  0.10013  ,\n",
       "           0.10591  ,  0.10591  ,  0.22573  ,  0.22573  ,  0.22525  ,\n",
       "           0.22525  ,  0.1804   ,  0.1804   ,  0.15122  ,  0.15122  ,\n",
       "           0.077285 ,  0.077285 ,  0.032009 ,  0.032009 ,  0.088393 ,\n",
       "           0.088393 ,  0.52482  ,  0.52482  ,  0.17916  ,  0.17916  ,\n",
       "           0.12961  ,  0.12961  ,  0.15284  ,  0.15284  ,  0.23742  ,\n",
       "           0.23742  ,  0.075633 ,  0.075633 ,  0.29751  ,  0.29751  ,\n",
       "           0.26412  ,  0.26412  ,  0.5749   ,  0.5749   ,  0.531    ,\n",
       "           0.531    ,  0.64141  ,  0.64141  ,  0.65289  ,  0.65289  ,\n",
       "           0.82631  ,  0.82631  ,  0.26751  ,  0.26751  ,  0.14327  ,\n",
       "           0.14327  ,  0.41185  ,  0.41185  ,  0.18529  ,  0.18529  ,\n",
       "           0.23687  ,  0.23687  ,  0.12537  ,  0.12537  ,  0.061412 ,\n",
       "           0.061412 ,  0.05069  ,  0.05069  ,  0.16083  ,  0.16083  ,\n",
       "           0.099214 ,  0.099214 ,  0.28009  ,  0.28009  ,  0.14229  ,\n",
       "           0.14229  ,  0.43305  ,  0.43305  ,  0.14596  ,  0.14596  ,\n",
       "           0.24134  ,  0.24134  ,  0.30743  ,  0.30743  ,  0.3538   ,\n",
       "           0.3538   ,  0.22765  ,  0.22765  ,  0.33891  ,  0.33891  ,\n",
       "           0.46497  ,  0.46497  ,  0.45604  ,  0.45604  ,  0.41889  ,\n",
       "           0.41889  ,  0.078087 ,  0.078087 ,  0.38251  ,  0.38251  ,\n",
       "           0.11138  ,  0.11138  ,  0.11192  ,  0.11192  ,  0.049193 ,\n",
       "           0.049193 ,  0.0029833,  0.0029833,  0.064237 ,  0.064237 ,\n",
       "           0.19875  ,  0.19875  ,  0.088349 ,  0.088349 ,  0.31613  ,\n",
       "           0.31613  ,  0.17825  ,  0.17825  ,  0.31984  ,  0.31984  ,\n",
       "           0.20939  ,  0.20939  ,  0.18911  ,  0.18911  ,  0.27698  ,\n",
       "           0.27698  ,  0.20825  ,  0.20825  ,  0.19395  ,  0.19395  ,\n",
       "           0.16338  ,  0.16338  ,  0.15413  ,  0.15413  ,  0.031288 ,\n",
       "           0.031288 ,  0.14008  ,  0.14008  ,  0.090333 ,  0.090333 ,\n",
       "           0.3232   ,  0.3232   ,  0.19489  ,  0.19489  ,  0.21312  ,\n",
       "           0.21312  ,  0.089073 ,  0.089073 ,  0.019102 ,  0.019102 ,\n",
       "           0.057632 ,  0.057632 ,  0.3293   ,  0.3293   ,  0.3667   ,\n",
       "           0.3667   ,  0.21927  ,  0.21927  ,  0.25507  ,  0.25507  ,\n",
       "           0.24062  ,  0.24062  ,  0.15685  ,  0.15685  ,  0.063974 ,\n",
       "           0.063974 ,  0.12795  ,  0.12795  ,  0.14111  ,  0.14111  ,\n",
       "           0.10713  ,  0.10713  ,  0.29691  ,  0.29691  ,  0.31763  ,\n",
       "           0.31763  ,  0.21632  ,  0.21632  ,  0.088533 ,  0.088533 ,\n",
       "           0.19251  ,  0.19251  ,  0.09435  ,  0.09435  , -0.0097822,\n",
       "          -0.0097822,  0.042749 ,  0.042749 ,  0.1148   ,  0.1148   ,\n",
       "           0.63295  ,  0.63295  ,  0.89729  ,  0.89729  ,  0.78426  ,\n",
       "           0.78426  ,  0.25748  ,  0.25748  ,  0.35261  ,  0.35261  ,\n",
       "           0.13637  ,  0.13637  ,  0.12896  ,  0.12896  ,  0.10413  ,\n",
       "           0.10413  ,  0.069807 ,  0.069807 ,  0.29382  ,  0.29382  ,\n",
       "           0.66414  ,  0.66414  ,  0.11436  ,  0.11436  ,  0.33009  ,\n",
       "           0.33009  ,  0.40708  ,  0.40708  ,  0.21692  ,  0.21692  ,\n",
       "           0.046517 ,  0.046517 ,  0.024411 ,  0.024411 ,  0.074275 ,\n",
       "           0.074275 ,  0.49719  ,  0.49719  ,  0.55071  ,  0.55071  ,\n",
       "           0.29181  ,  0.29181  ,  0.22733  ,  0.22733  ,  0.14105  ,\n",
       "           0.14105  ,  0.18604  ,  0.18604  ,  0.15466  ,  0.15466  ,\n",
       "           0.16411  ,  0.16411  ,  0.35593  ,  0.35593  ,  0.45398  ,\n",
       "           0.45398  ,  0.2662   ,  0.2662   ,  0.15843  ,  0.15843  ,\n",
       "           0.23967  ,  0.23967  ,  0.13648  ,  0.13648  ,  0.01001  ,\n",
       "           0.01001  ,  0.041901 ,  0.041901 ,  0.12886  ,  0.12886  ,\n",
       "           0.54538  ,  0.54538  ,  0.23193  ,  0.23193  ,  0.16253  ,\n",
       "           0.16253  ,  0.14732  ,  0.14732  ,  0.037314 ,  0.037314 ,\n",
       "           0.029983 ,  0.029983 ,  0.019508 ,  0.019508 ,  0.039895 ,\n",
       "           0.039895 ,  0.26181  ,  0.26181  , -0.036992 , -0.036992 ,\n",
       "           0.32256  ,  0.32256  ,  0.2233   ,  0.2233   ,  0.18918  ,\n",
       "           0.18918  ,  0.062411 ,  0.062411 ,  0.021565 ,  0.021565 ,\n",
       "           0.072701 ,  0.072701 ,  0.20146  ,  0.20146  ,  0.34138  ,\n",
       "           0.34138  ,  0.14711  ,  0.14711  ,  0.16088  ,  0.16088  ,\n",
       "           0.1457   ,  0.1457   ,  0.16897  ,  0.16897  ,  0.22849  ,\n",
       "           0.22849  ,  0.52415  ,  0.52415  ,  0.19247  ,  0.19247  ,\n",
       "           0.1996   ,  0.1996   ,  0.42029  ,  0.42029  ,  0.19304  ,\n",
       "           0.19304  ,  0.035691 ,  0.035691 ,  0.02462  ,  0.02462  ,\n",
       "           0.0827   ,  0.0827   ,  0.35579  ,  0.35579  ,  0.24985  ,\n",
       "           0.24985  ,  0.44664  ,  0.44664  ,  0.28575  ,  0.28575  ,\n",
       "           0.27349  ,  0.27349  ,  0.22864  ,  0.22864  ,  0.21489  ,\n",
       "           0.21489  ,  0.29219  ,  0.29219  ,  0.20292  ,  0.20292  ,\n",
       "           0.21713  ,  0.21713  ,  0.13414  ,  0.13414  ,  0.033316 ,\n",
       "           0.033316 ,  0.055339 ,  0.055339 ,  0.15933  ,  0.15933  ,\n",
       "           0.43353  ,  0.43353  ,  0.66881  ,  0.66881  ,  0.6345   ,\n",
       "           0.6345   ,  0.58515  ,  0.58515  ,  0.3033   ,  0.3033   ,\n",
       "           0.31446  ,  0.31446  ,  0.40582  ,  0.40582  ,  0.37597  ,\n",
       "           0.37597  ,  0.46799  ,  0.46799  ,  0.1874   ,  0.1874   ,\n",
       "           0.091397 ,  0.091397 ,  0.047718 ,  0.047718 ,  0.14286  ,\n",
       "           0.14286  ,  0.56453  ,  0.56453  ,  0.51335  ,  0.51335  ,\n",
       "           0.5968   ,  0.5968   ,  0.091152 ,  0.091152 ,  0.083658 ,\n",
       "           0.083658 ,  0.24034  ,  0.24034  ,  0.25253  ,  0.25253  ,\n",
       "           0.17331  ,  0.17331  ,  0.15214  ,  0.15214  ,  0.088057 ,\n",
       "           0.088057 ,  0.033493 ,  0.033493 ,  0.10485  ,  0.10485  ,\n",
       "           0.61631  ,  0.61631  ,  0.7151   ,  0.7151   ,  0.29513  ,\n",
       "           0.29513  ,  0.22922  ,  0.22922  ,  0.48951  ,  0.48951  ,\n",
       "           0.1781   ,  0.1781   ,  0.23601  ,  0.23601  ,  0.12201  ,\n",
       "           0.12201  ,  0.057065 ,  0.057065 ,  0.045402 ,  0.045402 ,\n",
       "           0.13086  ,  0.13086  ,  0.69467  ,  0.69467  ,  0.20101  ,\n",
       "           0.20101  ,  0.066433 ,  0.066433 ,  0.30269  ,  0.30269  ,\n",
       "           0.19766  ,  0.19766  ,  0.26341  ,  0.26341  ,  0.1146   ,\n",
       "           0.1146   ,  0.058149 ,  0.058149 ,  0.053463 ,  0.053463 ,\n",
       "           0.13575  ,  0.13575  ,  0.27974  ,  0.27974  ,  0.11393  ,\n",
       "           0.11393  ,  0.44104  ,  0.44104  ,  0.18226  ,  0.18226  ,\n",
       "           0.19928  ,  0.19928  ,  0.10922  ,  0.10922  ,  0.065321 ,\n",
       "           0.065321 ,  0.055716 ,  0.055716 ,  0.16755  ,  0.16755  ,\n",
       "           0.64809  ,  0.64809  ,  0.78581  ,  0.78581  ,  0.027869 ,\n",
       "           0.027869 ,  0.073261 ,  0.073261 ,  0.019392 ,  0.019392 ,\n",
       "          -0.028453 , -0.028453 ,  0.048853 ,  0.048853 ,  0.15552  ,\n",
       "           0.15552  ,  0.47912  ,  0.47912  ,  0.13938  ,  0.13938  ,\n",
       "           0.37744  ,  0.37744  ,  0.14814  ,  0.14814  ,  0.0052557,\n",
       "           0.0052557,  0.029382 ,  0.029382 ,  0.085198 ,  0.085198 ,\n",
       "           0.048122 ,  0.048122 ,  0.12057  ,  0.12057  ,  0.06033  ,\n",
       "           0.06033  ,  0.014264 ,  0.014264 ,  0.054312 ,  0.054312 ,\n",
       "           0.18358  ,  0.18358  ,  0.25977  ,  0.25977  ,  0.18118  ,\n",
       "           0.18118  ,  0.11717  ,  0.11717  ,  0.02445  ,  0.02445  ,\n",
       "           0.074665 ,  0.074665 ,  0.21137  ,  0.21137  ,  0.045916 ,\n",
       "           0.045916 ,  0.027396 ,  0.027396 ,  0.06124  ,  0.06124  ,\n",
       "           0.11414  ,  0.11414  ,  0.028379 ,  0.028379 ,  0.07981  ,\n",
       "           0.07981  ,  0.011463 ,  0.011463 ,  0.04058  ,  0.04058  ,\n",
       "           0.094907 ,  0.094907 ]),\n",
       "   'y_pred': array([ 0.31012038,  0.31012038,  0.30369166,  0.30369166,  0.25996792,\n",
       "           0.25996792,  0.27604832,  0.27604832,  0.29341735,  0.29341735,\n",
       "           0.25142371,  0.25142371,  0.19955649,  0.19955649,  0.17880021,\n",
       "           0.17880021,  0.2770032 ,  0.2770032 ,  0.26249199,  0.26249199,\n",
       "           0.19797684,  0.19797684,  0.19717487,  0.19717487,  0.21777737,\n",
       "           0.21777737,  0.32524037,  0.32524037,  0.29894428,  0.29894428,\n",
       "           0.31265322,  0.31265322,  0.27629567,  0.27629567,  0.26299585,\n",
       "           0.26299585,  0.23971695,  0.23971695,  0.21952646,  0.21952646,\n",
       "           0.24255713,  0.24255713,  0.19861099,  0.19861099,  0.18058869,\n",
       "           0.18058869,  0.28967385,  0.28967385,  0.10640704,  0.10640704,\n",
       "           0.1536834 ,  0.1536834 ,  0.04361044,  0.04361044,  0.15209127,\n",
       "           0.15209127,  0.40464751,  0.40464751,  0.36092377,  0.36092377,\n",
       "           0.37700417,  0.37700417,  0.3943732 ,  0.3943732 ,  0.35237956,\n",
       "           0.35237956,  0.30051234,  0.30051234,  0.27975606,  0.27975606,\n",
       "           0.37795906,  0.37795906,  0.36344784,  0.36344784,  0.29893269,\n",
       "           0.29893269,  0.29813072,  0.29813072,  0.31873323,  0.31873323,\n",
       "           0.42619622,  0.42619622,  0.39990013,  0.39990013,  0.41360908,\n",
       "           0.41360908,  0.37725152,  0.37725152,  0.3639517 ,  0.3639517 ,\n",
       "           0.3406728 ,  0.3406728 ,  0.32048231,  0.32048231,  0.34351299,\n",
       "           0.34351299,  0.29956684,  0.29956684,  0.28154454,  0.28154454,\n",
       "           0.3906297 ,  0.3906297 ,  0.2073629 ,  0.2073629 ,  0.25463926,\n",
       "           0.25463926,  0.1445663 ,  0.1445663 ,  0.25304712,  0.25304712,\n",
       "           0.35449505,  0.35449505,  0.37057545,  0.37057545,  0.38794448,\n",
       "           0.38794448,  0.34595084,  0.34595084,  0.29408362,  0.29408362,\n",
       "           0.27332734,  0.27332734,  0.37153034,  0.37153034,  0.35701912,\n",
       "           0.35701912,  0.29250397,  0.29250397,  0.291702  ,  0.291702  ,\n",
       "           0.31230451,  0.31230451,  0.4197675 ,  0.4197675 ,  0.39347141,\n",
       "           0.39347141,  0.40718036,  0.40718036,  0.3708228 ,  0.3708228 ,\n",
       "           0.35752298,  0.35752298,  0.33424408,  0.33424408,  0.31405359,\n",
       "           0.31405359,  0.33708427,  0.33708427,  0.29313812,  0.29313812,\n",
       "           0.27511582,  0.27511582,  0.38420098,  0.38420098,  0.20093418,\n",
       "           0.20093418,  0.24821054,  0.24821054,  0.13813758,  0.13813758,\n",
       "           0.2466184 ,  0.2466184 ,  0.32685172,  0.32685172,  0.34422075,\n",
       "           0.34422075,  0.30222711,  0.30222711,  0.25035988,  0.25035988,\n",
       "           0.2296036 ,  0.2296036 ,  0.3278066 ,  0.3278066 ,  0.31329539,\n",
       "           0.31329539,  0.24878024,  0.24878024,  0.24797827,  0.24797827,\n",
       "           0.26858077,  0.26858077,  0.37604377,  0.37604377,  0.34974767,\n",
       "           0.34974767,  0.36345662,  0.36345662,  0.32709906,  0.32709906,\n",
       "           0.31379924,  0.31379924,  0.29052034,  0.29052034,  0.27032986,\n",
       "           0.27032986,  0.29336053,  0.29336053,  0.24941439,  0.24941439,\n",
       "           0.23139208,  0.23139208,  0.34047725,  0.34047725,  0.15721044,\n",
       "           0.15721044,  0.2044868 ,  0.2044868 ,  0.09441384,  0.09441384,\n",
       "           0.20289466,  0.20289466,  0.36030115,  0.36030115,  0.31830751,\n",
       "           0.31830751,  0.26644028,  0.26644028,  0.245684  ,  0.245684  ,\n",
       "           0.343887  ,  0.343887  ,  0.32937579,  0.32937579,  0.26486064,\n",
       "           0.26486064,  0.26405867,  0.26405867,  0.28466117,  0.28466117,\n",
       "           0.39212417,  0.39212417,  0.36582807,  0.36582807,  0.37953702,\n",
       "           0.37953702,  0.34317946,  0.34317946,  0.32987964,  0.32987964,\n",
       "           0.30660074,  0.30660074,  0.28641026,  0.28641026,  0.30944093,\n",
       "           0.30944093,  0.26549479,  0.26549479,  0.24747248,  0.24747248,\n",
       "           0.35655765,  0.35655765,  0.17329084,  0.17329084,  0.2205672 ,\n",
       "           0.2205672 ,  0.11049424,  0.11049424,  0.21897506,  0.21897506,\n",
       "           0.33567654,  0.33567654,  0.28380931,  0.28380931,  0.26305303,\n",
       "           0.26305303,  0.36125603,  0.36125603,  0.34674481,  0.34674481,\n",
       "           0.28222967,  0.28222967,  0.2814277 ,  0.2814277 ,  0.3020302 ,\n",
       "           0.3020302 ,  0.4094932 ,  0.4094932 ,  0.3831971 ,  0.3831971 ,\n",
       "           0.39690605,  0.39690605,  0.36054849,  0.36054849,  0.34724867,\n",
       "           0.34724867,  0.32396977,  0.32396977,  0.30377928,  0.30377928,\n",
       "           0.32680996,  0.32680996,  0.28286381,  0.28286381,  0.26484151,\n",
       "           0.26484151,  0.37392667,  0.37392667,  0.19065987,  0.19065987,\n",
       "           0.23793623,  0.23793623,  0.12786327,  0.12786327,  0.23634409,\n",
       "           0.23634409,  0.24181567,  0.24181567,  0.22105939,  0.22105939,\n",
       "           0.31926239,  0.31926239,  0.30475117,  0.30475117,  0.24023603,\n",
       "           0.24023603,  0.23943406,  0.23943406,  0.26003656,  0.26003656,\n",
       "           0.36749956,  0.36749956,  0.34120346,  0.34120346,  0.35491241,\n",
       "           0.35491241,  0.31855485,  0.31855485,  0.30525503,  0.30525503,\n",
       "           0.28197613,  0.28197613,  0.26178564,  0.26178564,  0.28481632,\n",
       "           0.28481632,  0.24087017,  0.24087017,  0.22284787,  0.22284787,\n",
       "           0.33193304,  0.33193304,  0.14866623,  0.14866623,  0.19594259,\n",
       "           0.19594259,  0.08586963,  0.08586963,  0.19435045,  0.19435045,\n",
       "           0.16919217,  0.16919217,  0.26739516,  0.26739516,  0.25288395,\n",
       "           0.25288395,  0.1883688 ,  0.1883688 ,  0.18756683,  0.18756683,\n",
       "           0.20816933,  0.20816933,  0.31563233,  0.31563233,  0.28933624,\n",
       "           0.28933624,  0.30304518,  0.30304518,  0.26668763,  0.26668763,\n",
       "           0.25338781,  0.25338781,  0.23010891,  0.23010891,  0.20991842,\n",
       "           0.20991842,  0.23294909,  0.23294909,  0.18900295,  0.18900295,\n",
       "           0.17098065,  0.17098065,  0.28006581,  0.28006581,  0.096799  ,\n",
       "           0.096799  ,  0.14407536,  0.14407536,  0.0340024 ,  0.0340024 ,\n",
       "           0.14248323,  0.14248323,  0.24663889,  0.24663889,  0.23212767,\n",
       "           0.23212767,  0.16761252,  0.16761252,  0.16681055,  0.16681055,\n",
       "           0.18741305,  0.18741305,  0.29487605,  0.29487605,  0.26857996,\n",
       "           0.26857996,  0.28228891,  0.28228891,  0.24593135,  0.24593135,\n",
       "           0.23263153,  0.23263153,  0.20935263,  0.20935263,  0.18916214,\n",
       "           0.18916214,  0.21219281,  0.21219281,  0.16824667,  0.16824667,\n",
       "           0.15022437,  0.15022437,  0.25930953,  0.25930953,  0.07604273,\n",
       "           0.07604273,  0.12331909,  0.12331909,  0.01324612,  0.01324612,\n",
       "           0.12172695,  0.12172695,  0.33033067,  0.33033067,  0.26581552,\n",
       "           0.26581552,  0.26501355,  0.26501355,  0.28561605,  0.28561605,\n",
       "           0.39307905,  0.39307905,  0.36678296,  0.36678296,  0.3804919 ,\n",
       "           0.3804919 ,  0.34413434,  0.34413434,  0.33083452,  0.33083452,\n",
       "           0.30755562,  0.30755562,  0.28736514,  0.28736514,  0.31039581,\n",
       "           0.31039581,  0.26644967,  0.26644967,  0.24842736,  0.24842736,\n",
       "           0.35751253,  0.35751253,  0.17424572,  0.17424572,  0.22152208,\n",
       "           0.22152208,  0.11144912,  0.11144912,  0.21992994,  0.21992994,\n",
       "           0.2513043 ,  0.2513043 ,  0.25050234,  0.25050234,  0.27110484,\n",
       "           0.27110484,  0.37856783,  0.37856783,  0.35227174,  0.35227174,\n",
       "           0.36598069,  0.36598069,  0.32962313,  0.32962313,  0.31632331,\n",
       "           0.31632331,  0.29304441,  0.29304441,  0.27285392,  0.27285392,\n",
       "           0.2958846 ,  0.2958846 ,  0.25193845,  0.25193845,  0.23391615,\n",
       "           0.23391615,  0.34300131,  0.34300131,  0.15973451,  0.15973451,\n",
       "           0.20701087,  0.20701087,  0.09693791,  0.09693791,  0.20541873,\n",
       "           0.20541873,  0.18598719,  0.18598719,  0.20658969,  0.20658969,\n",
       "           0.31405269,  0.31405269,  0.28775659,  0.28775659,  0.30146554,\n",
       "           0.30146554,  0.26510798,  0.26510798,  0.25180816,  0.25180816,\n",
       "           0.22852926,  0.22852926,  0.20833878,  0.20833878,  0.23136945,\n",
       "           0.23136945,  0.18742331,  0.18742331,  0.169401  ,  0.169401  ,\n",
       "           0.27848617,  0.27848617,  0.09521936,  0.09521936,  0.14249572,\n",
       "           0.14249572,  0.03242276,  0.03242276,  0.14090358,  0.14090358,\n",
       "           0.20578772,  0.20578772,  0.31325072,  0.31325072,  0.28695462,\n",
       "           0.28695462,  0.30066357,  0.30066357,  0.26430601,  0.26430601,\n",
       "           0.25100619,  0.25100619,  0.22772729,  0.22772729,  0.20753681,\n",
       "           0.20753681,  0.23056748,  0.23056748,  0.18662134,  0.18662134,\n",
       "           0.16859903,  0.16859903,  0.2776842 ,  0.2776842 ,  0.09441739,\n",
       "           0.09441739,  0.14169375,  0.14169375,  0.03162079,  0.03162079,\n",
       "           0.14010161,  0.14010161,  0.33385322,  0.33385322,  0.30755712,\n",
       "           0.30755712,  0.32126607,  0.32126607,  0.28490851,  0.28490851,\n",
       "           0.27160869,  0.27160869,  0.24832979,  0.24832979,  0.22813931,\n",
       "           0.22813931,  0.25116998,  0.25116998,  0.20722384,  0.20722384,\n",
       "           0.18920153,  0.18920153,  0.2982867 ,  0.2982867 ,  0.11501989,\n",
       "           0.11501989,  0.16229625,  0.16229625,  0.05222329,  0.05222329,\n",
       "           0.16070411,  0.16070411,  0.41502012,  0.41502012,  0.42872907,\n",
       "           0.42872907,  0.39237151,  0.39237151,  0.37907169,  0.37907169,\n",
       "           0.35579279,  0.35579279,  0.3356023 ,  0.3356023 ,  0.35863298,\n",
       "           0.35863298,  0.31468683,  0.31468683,  0.29666453,  0.29666453,\n",
       "           0.4057497 ,  0.4057497 ,  0.22248289,  0.22248289,  0.26975925,\n",
       "           0.26975925,  0.15968629,  0.15968629,  0.26816711,  0.26816711,\n",
       "           0.40243298,  0.40243298,  0.36607542,  0.36607542,  0.3527756 ,\n",
       "           0.3527756 ,  0.3294967 ,  0.3294967 ,  0.30930621,  0.30930621,\n",
       "           0.33233688,  0.33233688,  0.28839074,  0.28839074,  0.27036844,\n",
       "           0.27036844,  0.3794536 ,  0.3794536 ,  0.1961868 ,  0.1961868 ,\n",
       "           0.24346316,  0.24346316,  0.13339019,  0.13339019,  0.24187102,\n",
       "           0.24187102,  0.37978436,  0.37978436,  0.36648454,  0.36648454,\n",
       "           0.34320564,  0.34320564,  0.32301516,  0.32301516,  0.34604583,\n",
       "           0.34604583,  0.30209969,  0.30209969,  0.28407738,  0.28407738,\n",
       "           0.39316255,  0.39316255,  0.20989574,  0.20989574,  0.2571721 ,\n",
       "           0.2571721 ,  0.14709914,  0.14709914,  0.25557996,  0.25557996,\n",
       "           0.33012699,  0.33012699,  0.30684809,  0.30684809,  0.2866576 ,\n",
       "           0.2866576 ,  0.30968827,  0.30968827,  0.26574213,  0.26574213,\n",
       "           0.24771983,  0.24771983,  0.35680499,  0.35680499,  0.17353818,\n",
       "           0.17353818,  0.22081454,  0.22081454,  0.11074158,  0.11074158,\n",
       "           0.21922241,  0.21922241,  0.29354827,  0.29354827,  0.27335778,\n",
       "           0.27335778,  0.29638845,  0.29638845,  0.25244231,  0.25244231,\n",
       "           0.23442001,  0.23442001,  0.34350517,  0.34350517,  0.16023836,\n",
       "           0.16023836,  0.20751472,  0.20751472,  0.09744176,  0.09744176,\n",
       "           0.20592259,  0.20592259,  0.25007888,  0.25007888,  0.27310955,\n",
       "           0.27310955,  0.22916341,  0.22916341,  0.21114111,  0.21114111,\n",
       "           0.32022627,  0.32022627,  0.13695947,  0.13695947,  0.18423583,\n",
       "           0.18423583,  0.07416286,  0.07416286,  0.18264369,  0.18264369,\n",
       "           0.25291907,  0.25291907,  0.20897292,  0.20897292,  0.19095062,\n",
       "           0.19095062,  0.30003578,  0.30003578,  0.11676898,  0.11676898,\n",
       "           0.16404534,  0.16404534,  0.05397238,  0.05397238,  0.1624532 ,\n",
       "           0.1624532 ,  0.2320036 ,  0.2320036 ,  0.21398129,  0.21398129,\n",
       "           0.32306646,  0.32306646,  0.13979965,  0.13979965,  0.18707601,\n",
       "           0.18707601,  0.07700305,  0.07700305,  0.18548387,  0.18548387,\n",
       "           0.17003515,  0.17003515,  0.27912031,  0.27912031,  0.09585351,\n",
       "           0.09585351,  0.14312987,  0.14312987,  0.03305691,  0.03305691,\n",
       "           0.14153773,  0.14153773,  0.26109801,  0.26109801,  0.0778312 ,\n",
       "           0.0778312 ,  0.12510756,  0.12510756,  0.0150346 ,  0.0150346 ,\n",
       "           0.12351543,  0.12351543,  0.18691637,  0.18691637,  0.23419273,\n",
       "           0.23419273,  0.12411977,  0.12411977,  0.23260059,  0.23260059,\n",
       "           0.05092592,  0.05092592, -0.05914704, -0.05914704,  0.04933379,\n",
       "           0.04933379, -0.01187068, -0.01187068,  0.09661015,  0.09661015,\n",
       "          -0.01346282, -0.01346282]),\n",
       "   'feature_importances': array([-0.00044989,  0.00061473, -0.00017106, ...,  0.00053582,\n",
       "           0.00143571, -0.00061455]),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.02057781196789063,\n",
       "    'mae': 0.10329275487118939,\n",
       "    'r2': -0.44778701278305116,\n",
       "    'pearson_corr': 0.5866899932380918,\n",
       "    'connectome_corr': 0.42127572301529653,\n",
       "    'connectome_r2': -0.5183517609706955,\n",
       "    'geodesic_distance': 13.870963012371796},\n",
       "   'best_val_score': -0.0248969403519967,\n",
       "   'test_metrics': {'mse': 0.023383092455993966,\n",
       "    'mae': 0.11287381062531544,\n",
       "    'r2': -3.3858769196588763,\n",
       "    'pearson_corr': 0.6251578357343585,\n",
       "    'connectome_corr': 0.508181804393245,\n",
       "    'connectome_r2': -4.079728095014497,\n",
       "    'geodesic_distance': 6.075958930897474},\n",
       "   'y_true': array([ 0.71922  ,  0.71922  ,  0.48128  ,  0.48128  ,  0.43659  ,\n",
       "           0.43659  ,  0.32399  ,  0.32399  ,  0.44948  ,  0.44948  ,\n",
       "           0.33167  ,  0.33167  ,  0.43798  ,  0.43798  ,  0.50016  ,\n",
       "           0.50016  ,  0.1345   ,  0.1345   ,  0.26447  ,  0.26447  ,\n",
       "           0.29921  ,  0.29921  ,  0.30794  ,  0.30794  ,  0.3946   ,\n",
       "           0.3946   ,  0.19136  ,  0.19136  ,  0.23954  ,  0.23954  ,\n",
       "           0.92977  ,  0.92977  ,  0.59727  ,  0.59727  ,  0.49227  ,\n",
       "           0.49227  ,  0.42251  ,  0.42251  ,  0.39702  ,  0.39702  ,\n",
       "           0.32595  ,  0.32595  ,  0.12236  ,  0.12236  ,  0.21177  ,\n",
       "           0.21177  ,  0.16332  ,  0.16332  ,  0.2177   ,  0.2177   ,\n",
       "           0.051711 ,  0.051711 ,  0.045002 ,  0.045002 ,  0.027955 ,\n",
       "           0.027955 ,  0.4555   ,  0.4555   ,  0.70545  ,  0.70545  ,\n",
       "           0.35772  ,  0.35772  ,  0.40483  ,  0.40483  ,  0.31522  ,\n",
       "           0.31522  ,  0.3479   ,  0.3479   ,  0.47006  ,  0.47006  ,\n",
       "           0.11232  ,  0.11232  ,  0.21738  ,  0.21738  ,  0.2367   ,\n",
       "           0.2367   ,  0.19925  ,  0.19925  ,  0.42237  ,  0.42237  ,\n",
       "           0.15409  ,  0.15409  ,  0.12644  ,  0.12644  ,  0.76273  ,\n",
       "           0.76273  ,  1.4159   ,  1.4159   ,  0.53017  ,  0.53017  ,\n",
       "           0.48218  ,  0.48218  ,  0.39509  ,  0.39509  ,  0.46872  ,\n",
       "           0.46872  ,  0.11871  ,  0.11871  ,  0.1184   ,  0.1184   ,\n",
       "           0.088568 ,  0.088568 ,  0.10206  ,  0.10206  ,  0.068446 ,\n",
       "           0.068446 ,  0.033191 ,  0.033191 ,  0.02063  ,  0.02063  ,\n",
       "           0.4428   ,  0.4428   ,  0.41565  ,  0.41565  ,  0.34578  ,\n",
       "           0.34578  ,  0.39808  ,  0.39808  ,  0.44063  ,  0.44063  ,\n",
       "           0.46516  ,  0.46516  ,  0.15989  ,  0.15989  ,  0.26702  ,\n",
       "           0.26702  ,  0.33929  ,  0.33929  ,  0.21536  ,  0.21536  ,\n",
       "           0.56331  ,  0.56331  ,  0.25194  ,  0.25194  ,  0.25443  ,\n",
       "           0.25443  ,  0.49248  ,  0.49248  ,  0.39745  ,  0.39745  ,\n",
       "           0.54343  ,  0.54343  ,  0.4951   ,  0.4951   ,  0.54511  ,\n",
       "           0.54511  ,  0.32239  ,  0.32239  ,  0.14092  ,  0.14092  ,\n",
       "           0.23301  ,  0.23301  ,  0.21211  ,  0.21211  ,  0.23812  ,\n",
       "           0.23812  ,  0.096152 ,  0.096152 ,  0.046146 ,  0.046146 ,\n",
       "           0.034861 ,  0.034861 ,  0.61068  ,  0.61068  ,  0.48006  ,\n",
       "           0.48006  ,  0.34539  ,  0.34539  ,  0.37022  ,  0.37022  ,\n",
       "           0.43956  ,  0.43956  ,  0.17585  ,  0.17585  ,  0.49562  ,\n",
       "           0.49562  ,  0.50252  ,  0.50252  ,  0.18569  ,  0.18569  ,\n",
       "           0.54201  ,  0.54201  ,  0.2935   ,  0.2935   ,  0.13164  ,\n",
       "           0.13164  ,  0.45332  ,  0.45332  ,  0.7205   ,  0.7205   ,\n",
       "           0.38752  ,  0.38752  ,  0.43245  ,  0.43245  ,  0.35889  ,\n",
       "           0.35889  ,  0.67894  ,  0.67894  ,  0.1627   ,  0.1627   ,\n",
       "           0.20474  ,  0.20474  ,  0.23652  ,  0.23652  ,  0.10442  ,\n",
       "           0.10442  ,  0.09912  ,  0.09912  ,  0.036825 ,  0.036825 ,\n",
       "           0.020006 ,  0.020006 ,  0.45635  ,  0.45635  ,  0.45603  ,\n",
       "           0.45603  ,  0.38549  ,  0.38549  ,  0.44073  ,  0.44073  ,\n",
       "           0.1256   ,  0.1256   ,  0.40294  ,  0.40294  ,  0.42535  ,\n",
       "           0.42535  ,  0.1231   ,  0.1231   ,  0.41124  ,  0.41124  ,\n",
       "           0.17447  ,  0.17447  ,  0.044144 ,  0.044144 ,  0.34047  ,\n",
       "           0.34047  ,  0.32739  ,  0.32739  ,  0.37778  ,  0.37778  ,\n",
       "           0.49065  ,  0.49065  ,  0.36448  ,  0.36448  ,  0.58298  ,\n",
       "           0.58298  ,  0.11132  ,  0.11132  ,  0.10034  ,  0.10034  ,\n",
       "           0.073646 ,  0.073646 ,  0.013839 ,  0.013839 ,  0.076694 ,\n",
       "           0.076694 ,  0.037086 ,  0.037086 ,  0.0091754,  0.0091754,\n",
       "           0.34648  ,  0.34648  ,  0.43885  ,  0.43885  ,  0.63528  ,\n",
       "           0.63528  ,  0.1177   ,  0.1177   ,  0.44497  ,  0.44497  ,\n",
       "           0.42537  ,  0.42537  ,  0.4355   ,  0.4355   ,  0.39446  ,\n",
       "           0.39446  ,  0.095204 ,  0.095204 ,  0.14335  ,  0.14335  ,\n",
       "           0.43289  ,  0.43289  ,  0.36495  ,  0.36495  ,  0.2908   ,\n",
       "           0.2908   ,  0.36472  ,  0.36472  ,  0.28198  ,  0.28198  ,\n",
       "           0.47213  ,  0.47213  ,  0.10033  ,  0.10033  ,  0.24096  ,\n",
       "           0.24096  ,  0.15704  ,  0.15704  ,  0.14838  ,  0.14838  ,\n",
       "           0.039112 ,  0.039112 ,  0.039563 ,  0.039563 ,  0.023165 ,\n",
       "           0.023165 ,  0.55964  ,  0.55964  ,  0.4745   ,  0.4745   ,\n",
       "           0.12526  ,  0.12526  ,  0.30224  ,  0.30224  ,  0.42563  ,\n",
       "           0.42563  ,  0.22515  ,  0.22515  ,  0.38099  ,  0.38099  ,\n",
       "           0.20657  ,  0.20657  ,  0.032886 ,  0.032886 ,  0.3238   ,\n",
       "           0.3238   ,  0.27486  ,  0.27486  ,  0.3946   ,  0.3946   ,\n",
       "           0.29752  ,  0.29752  ,  0.26246  ,  0.26246  ,  0.36549  ,\n",
       "           0.36549  ,  0.08782  ,  0.08782  ,  0.1119   ,  0.1119   ,\n",
       "          -0.019675 , -0.019675 , -0.0045123, -0.0045123,  0.04498  ,\n",
       "           0.04498  ,  0.055842 ,  0.055842 ,  0.022106 ,  0.022106 ,\n",
       "           0.53023  ,  0.53023  ,  0.16353  ,  0.16353  ,  0.37731  ,\n",
       "           0.37731  ,  0.46037  ,  0.46037  ,  0.35026  ,  0.35026  ,\n",
       "           0.38297  ,  0.38297  ,  0.25752  ,  0.25752  ,  0.23182  ,\n",
       "           0.23182  ,  0.40406  ,  0.40406  ,  0.31176  ,  0.31176  ,\n",
       "           0.40604  ,  0.40604  ,  0.34375  ,  0.34375  ,  0.34606  ,\n",
       "           0.34606  ,  0.35101  ,  0.35101  ,  0.109    ,  0.109    ,\n",
       "           0.20861  ,  0.20861  ,  0.10234  ,  0.10234  ,  0.16967  ,\n",
       "           0.16967  ,  0.044379 ,  0.044379 ,  0.06157  ,  0.06157  ,\n",
       "           0.040003 ,  0.040003 ,  0.12983  ,  0.12983  ,  0.29599  ,\n",
       "           0.29599  ,  0.34645  ,  0.34645  ,  0.36782  ,  0.36782  ,\n",
       "           0.44099  ,  0.44099  ,  0.13153  ,  0.13153  ,  0.1545   ,\n",
       "           0.1545   ,  0.50111  ,  0.50111  ,  0.41965  ,  0.41965  ,\n",
       "           0.45112  ,  0.45112  ,  0.44269  ,  0.44269  ,  0.39682  ,\n",
       "           0.39682  ,  0.38445  ,  0.38445  ,  0.11951  ,  0.11951  ,\n",
       "           0.16574  ,  0.16574  ,  0.079648 ,  0.079648 ,  0.14578  ,\n",
       "           0.14578  ,  0.054895 ,  0.054895 ,  0.049641 ,  0.049641 ,\n",
       "           0.028757 ,  0.028757 ,  0.21889  ,  0.21889  ,  0.25943  ,\n",
       "           0.25943  ,  0.14335  ,  0.14335  ,  0.14767  ,  0.14767  ,\n",
       "           0.25599  ,  0.25599  ,  0.21192  ,  0.21192  ,  0.11635  ,\n",
       "           0.11635  ,  0.1081   ,  0.1081   ,  0.10367  ,  0.10367  ,\n",
       "           0.10679  ,  0.10679  ,  0.11532  ,  0.11532  ,  0.12951  ,\n",
       "           0.12951  ,  0.11851  ,  0.11851  ,  0.14697  ,  0.14697  ,\n",
       "           0.18269  ,  0.18269  ,  0.15903  ,  0.15903  ,  0.043178 ,\n",
       "           0.043178 ,  0.024286 ,  0.024286 ,  0.020426 ,  0.020426 ,\n",
       "           0.96592  ,  0.96592  ,  0.49741  ,  0.49741  ,  0.33026  ,\n",
       "           0.33026  ,  0.38577  ,  0.38577  ,  0.027429 ,  0.027429 ,\n",
       "           0.23037  ,  0.23037  ,  0.19739  ,  0.19739  ,  0.16974  ,\n",
       "           0.16974  ,  0.19702  ,  0.19702  ,  0.17137  ,  0.17137  ,\n",
       "           0.71622  ,  0.71622  ,  0.13794  ,  0.13794  ,  0.50895  ,\n",
       "           0.50895  ,  0.434    ,  0.434    ,  0.0098301,  0.0098301,\n",
       "           0.017404 ,  0.017404 ,  0.039726 ,  0.039726 ,  0.01181  ,\n",
       "           0.01181  ,  0.35224  ,  0.35224  ,  0.40969  ,  0.40969  ,\n",
       "           0.56554  ,  0.56554  ,  0.086979 ,  0.086979 ,  0.26136  ,\n",
       "           0.26136  ,  0.21452  ,  0.21452  ,  0.23154  ,  0.23154  ,\n",
       "           0.22368  ,  0.22368  ,  0.21647  ,  0.21647  ,  0.53709  ,\n",
       "           0.53709  ,  0.14928  ,  0.14928  ,  0.42003  ,  0.42003  ,\n",
       "           0.31317  ,  0.31317  ,  0.02881  ,  0.02881  ,  0.046027 ,\n",
       "           0.046027 ,  0.048635 ,  0.048635 ,  0.016227 ,  0.016227 ,\n",
       "           0.21579  ,  0.21579  ,  0.086568 ,  0.086568 ,  0.22599  ,\n",
       "           0.22599  ,  0.2586   ,  0.2586   ,  0.17552  ,  0.17552  ,\n",
       "           0.13738  ,  0.13738  ,  0.13292  ,  0.13292  ,  0.091804 ,\n",
       "           0.091804 ,  0.2714   ,  0.2714   ,  0.084917 ,  0.084917 ,\n",
       "           0.37345  ,  0.37345  ,  0.22315  ,  0.22315  ,  0.24222  ,\n",
       "           0.24222  , -0.021994 , -0.021994 ,  0.04059  ,  0.04059  ,\n",
       "           0.036565 ,  0.036565 ,  0.4397   ,  0.4397   ,  0.16518  ,\n",
       "           0.16518  ,  0.4114   ,  0.4114   ,  0.3826   ,  0.3826   ,\n",
       "           0.41368  ,  0.41368  ,  0.39789  ,  0.39789  ,  0.38326  ,\n",
       "           0.38326  ,  0.33313  ,  0.33313  ,  0.14608  ,  0.14608  ,\n",
       "           0.2563   ,  0.2563   ,  0.19318  ,  0.19318  ,  0.14108  ,\n",
       "           0.14108  ,  0.10198  ,  0.10198  ,  0.040245 ,  0.040245 ,\n",
       "           0.015939 ,  0.015939 ,  0.24942  ,  0.24942  ,  0.16531  ,\n",
       "           0.16531  ,  0.1696   ,  0.1696   ,  0.1745   ,  0.1745   ,\n",
       "           0.16606  ,  0.16606  ,  0.21919  ,  0.21919  ,  0.15928  ,\n",
       "           0.15928  ,  0.15385  ,  0.15385  ,  0.36641  ,  0.36641  ,\n",
       "           0.35634  ,  0.35634  ,  0.15194  ,  0.15194  ,  0.07519  ,\n",
       "           0.07519  ,  0.039233 ,  0.039233 ,  0.0028819,  0.0028819,\n",
       "           0.19999  ,  0.19999  ,  0.13245  ,  0.13245  ,  0.13954  ,\n",
       "           0.13954  ,  0.17444  ,  0.17444  ,  0.2211   ,  0.2211   ,\n",
       "          -0.040998 , -0.040998 ,  0.14267  ,  0.14267  ,  0.15903  ,\n",
       "           0.15903  ,  0.29571  ,  0.29571  ,  0.71477  ,  0.71477  ,\n",
       "           0.067065 ,  0.067065 ,  0.028937 ,  0.028937 ,  0.058216 ,\n",
       "           0.058216 ,  0.65175  ,  0.65175  ,  0.53779  ,  0.53779  ,\n",
       "           0.47837  ,  0.47837  ,  0.44458  ,  0.44458  ,  0.33217  ,\n",
       "           0.33217  ,  0.11667  ,  0.11667  ,  0.16692  ,  0.16692  ,\n",
       "           0.13881  ,  0.13881  ,  0.18278  ,  0.18278  ,  0.064966 ,\n",
       "           0.064966 ,  0.03759  ,  0.03759  ,  0.021881 ,  0.021881 ,\n",
       "           0.4799   ,  0.4799   ,  0.44247  ,  0.44247  ,  0.35893  ,\n",
       "           0.35893  ,  0.49644  ,  0.49644  ,  0.11726  ,  0.11726  ,\n",
       "           0.10522  ,  0.10522  ,  0.095924 ,  0.095924 ,  0.1036   ,\n",
       "           0.1036   ,  0.066691 ,  0.066691 ,  0.029888 ,  0.029888 ,\n",
       "           0.018513 ,  0.018513 ,  0.55442  ,  0.55442  ,  0.59529  ,\n",
       "           0.59529  ,  0.30399  ,  0.30399  ,  0.10286  ,  0.10286  ,\n",
       "           0.11277  ,  0.11277  ,  0.080131 ,  0.080131 ,  0.10934  ,\n",
       "           0.10934  ,  0.085419 ,  0.085419 ,  0.041231 ,  0.041231 ,\n",
       "           0.021797 ,  0.021797 ,  0.67374  ,  0.67374  ,  0.35077  ,\n",
       "           0.35077  ,  0.10422  ,  0.10422  ,  0.10999  ,  0.10999  ,\n",
       "           0.12321  ,  0.12321  ,  0.14327  ,  0.14327  ,  0.090271 ,\n",
       "           0.090271 ,  0.028932 ,  0.028932 ,  0.019377 ,  0.019377 ,\n",
       "           0.23682  ,  0.23682  ,  0.10727  ,  0.10727  ,  0.1266   ,\n",
       "           0.1266   ,  0.15259  ,  0.15259  ,  0.18575  ,  0.18575  ,\n",
       "           0.10293  ,  0.10293  ,  0.030834 ,  0.030834 ,  0.023963 ,\n",
       "           0.023963 ,  0.13622  ,  0.13622  ,  0.34866  ,  0.34866  ,\n",
       "           0.28463  ,  0.28463  , -0.024828 , -0.024828 ,  0.037177 ,\n",
       "           0.037177 ,  0.035488 ,  0.035488 ,  0.011733 ,  0.011733 ,\n",
       "           0.16488  ,  0.16488  ,  0.19003  ,  0.19003  ,  0.18605  ,\n",
       "           0.18605  ,  0.036091 ,  0.036091 ,  0.017743 ,  0.017743 ,\n",
       "           0.034771 ,  0.034771 ,  0.67479  ,  0.67479  ,  0.27241  ,\n",
       "           0.27241  ,  0.0089888,  0.0089888,  0.032533 ,  0.032533 ,\n",
       "           0.015969 ,  0.015969 ,  0.40099  ,  0.40099  ,  0.047716 ,\n",
       "           0.047716 ,  0.020583 ,  0.020583 ,  0.022499 ,  0.022499 ,\n",
       "           0.057452 ,  0.057452 ,  0.022605 ,  0.022605 ,  0.060679 ,\n",
       "           0.060679 ,  0.010376 ,  0.010376 ,  0.012433 ,  0.012433 ,\n",
       "           0.0095714,  0.0095714]),\n",
       "   'y_pred': array([ 0.34247578,  0.34247578,  0.34316954,  0.34316954,  0.32130454,\n",
       "           0.32130454,  0.34396347,  0.34396347,  0.33490284,  0.33490284,\n",
       "           0.26173461,  0.26173461,  0.24633232,  0.24633232,  0.3356496 ,\n",
       "           0.3356496 ,  0.24844034,  0.24844034,  0.33070944,  0.33070944,\n",
       "           0.29624893,  0.29624893,  0.33899508,  0.33899508,  0.33050184,\n",
       "           0.33050184,  0.28325272,  0.28325272,  0.23727731,  0.23727731,\n",
       "           0.28659538,  0.28659538,  0.34284044,  0.34284044,  0.39895751,\n",
       "           0.39895751,  0.37751677,  0.37751677,  0.35787895,  0.35787895,\n",
       "           0.30043581,  0.30043581,  0.26215164,  0.26215164,  0.29275733,\n",
       "           0.29275733,  0.28816624,  0.28816624,  0.26193523,  0.26193523,\n",
       "           0.10363314,  0.10363314,  0.09658545,  0.09658545,  0.14028907,\n",
       "           0.14028907,  0.3641697 ,  0.3641697 ,  0.34230469,  0.34230469,\n",
       "           0.36496363,  0.36496363,  0.35590299,  0.35590299,  0.28273476,\n",
       "           0.28273476,  0.26733248,  0.26733248,  0.35664976,  0.35664976,\n",
       "           0.2694405 ,  0.2694405 ,  0.35170959,  0.35170959,  0.31724909,\n",
       "           0.31724909,  0.35999524,  0.35999524,  0.35150199,  0.35150199,\n",
       "           0.30425287,  0.30425287,  0.25827747,  0.25827747,  0.30759554,\n",
       "           0.30759554,  0.36384059,  0.36384059,  0.41995766,  0.41995766,\n",
       "           0.39851693,  0.39851693,  0.37887911,  0.37887911,  0.32143597,\n",
       "           0.32143597,  0.28315179,  0.28315179,  0.31375749,  0.31375749,\n",
       "           0.30916639,  0.30916639,  0.28293539,  0.28293539,  0.1246333 ,\n",
       "           0.1246333 ,  0.1175856 ,  0.1175856 ,  0.16128922,  0.16128922,\n",
       "           0.34299846,  0.34299846,  0.36565739,  0.36565739,  0.35659676,\n",
       "           0.35659676,  0.28342853,  0.28342853,  0.26802624,  0.26802624,\n",
       "           0.35734352,  0.35734352,  0.27013427,  0.27013427,  0.35240336,\n",
       "           0.35240336,  0.31794285,  0.31794285,  0.360689  ,  0.360689  ,\n",
       "           0.35219576,  0.35219576,  0.30494664,  0.30494664,  0.25897123,\n",
       "           0.25897123,  0.3082893 ,  0.3082893 ,  0.36453436,  0.36453436,\n",
       "           0.42065143,  0.42065143,  0.39921069,  0.39921069,  0.37957287,\n",
       "           0.37957287,  0.32212974,  0.32212974,  0.28384556,  0.28384556,\n",
       "           0.31445125,  0.31445125,  0.30986016,  0.30986016,  0.28362915,\n",
       "           0.28362915,  0.12532706,  0.12532706,  0.11827937,  0.11827937,\n",
       "           0.16198299,  0.16198299,  0.34379239,  0.34379239,  0.33473175,\n",
       "           0.33473175,  0.26156352,  0.26156352,  0.24616124,  0.24616124,\n",
       "           0.33547852,  0.33547852,  0.24826926,  0.24826926,  0.33053835,\n",
       "           0.33053835,  0.29607785,  0.29607785,  0.338824  ,  0.338824  ,\n",
       "           0.33033075,  0.33033075,  0.28308163,  0.28308163,  0.23710623,\n",
       "           0.23710623,  0.2864243 ,  0.2864243 ,  0.34266935,  0.34266935,\n",
       "           0.39878642,  0.39878642,  0.37734569,  0.37734569,  0.35770787,\n",
       "           0.35770787,  0.30026473,  0.30026473,  0.26198055,  0.26198055,\n",
       "           0.29258625,  0.29258625,  0.28799515,  0.28799515,  0.26176415,\n",
       "           0.26176415,  0.10346206,  0.10346206,  0.09641436,  0.09641436,\n",
       "           0.14011798,  0.14011798,  0.35739068,  0.35739068,  0.28422245,\n",
       "           0.28422245,  0.26882017,  0.26882017,  0.35813745,  0.35813745,\n",
       "           0.27092819,  0.27092819,  0.35319729,  0.35319729,  0.31873678,\n",
       "           0.31873678,  0.36148293,  0.36148293,  0.35298969,  0.35298969,\n",
       "           0.30574057,  0.30574057,  0.25976516,  0.25976516,  0.30908323,\n",
       "           0.30908323,  0.36532829,  0.36532829,  0.42144536,  0.42144536,\n",
       "           0.40000462,  0.40000462,  0.3803668 ,  0.3803668 ,  0.32292366,\n",
       "           0.32292366,  0.28463949,  0.28463949,  0.31524518,  0.31524518,\n",
       "           0.31065409,  0.31065409,  0.28442308,  0.28442308,  0.12612099,\n",
       "           0.12612099,  0.1190733 ,  0.1190733 ,  0.16277692,  0.16277692,\n",
       "           0.27516182,  0.27516182,  0.25975953,  0.25975953,  0.34907682,\n",
       "           0.34907682,  0.26186756,  0.26186756,  0.34413665,  0.34413665,\n",
       "           0.30967614,  0.30967614,  0.35242229,  0.35242229,  0.34392905,\n",
       "           0.34392905,  0.29667993,  0.29667993,  0.25070453,  0.25070453,\n",
       "           0.30002259,  0.30002259,  0.35626765,  0.35626765,  0.41238472,\n",
       "           0.41238472,  0.39094399,  0.39094399,  0.37130616,  0.37130616,\n",
       "           0.31386303,  0.31386303,  0.27557885,  0.27557885,  0.30618454,\n",
       "           0.30618454,  0.30159345,  0.30159345,  0.27536244,  0.27536244,\n",
       "           0.11706036,  0.11706036,  0.11001266,  0.11001266,  0.15371628,\n",
       "           0.15371628,  0.1865913 ,  0.1865913 ,  0.27590859,  0.27590859,\n",
       "           0.18869933,  0.18869933,  0.27096842,  0.27096842,  0.23650791,\n",
       "           0.23650791,  0.27925406,  0.27925406,  0.27076082,  0.27076082,\n",
       "           0.2235117 ,  0.2235117 ,  0.1775363 ,  0.1775363 ,  0.22685436,\n",
       "           0.22685436,  0.28309942,  0.28309942,  0.33921649,  0.33921649,\n",
       "           0.31777576,  0.31777576,  0.29813793,  0.29813793,  0.2406948 ,\n",
       "           0.2406948 ,  0.20241062,  0.20241062,  0.23301631,  0.23301631,\n",
       "           0.22842522,  0.22842522,  0.20219421,  0.20219421,  0.04389213,\n",
       "           0.04389213,  0.03684443,  0.03684443,  0.08054805,  0.08054805,\n",
       "           0.2605063 ,  0.2605063 ,  0.17329704,  0.17329704,  0.25556614,\n",
       "           0.25556614,  0.22110563,  0.22110563,  0.26385178,  0.26385178,\n",
       "           0.25535854,  0.25535854,  0.20810941,  0.20810941,  0.16213401,\n",
       "           0.16213401,  0.21145208,  0.21145208,  0.26769714,  0.26769714,\n",
       "           0.32381421,  0.32381421,  0.30237347,  0.30237347,  0.28273565,\n",
       "           0.28273565,  0.22529251,  0.22529251,  0.18700834,  0.18700834,\n",
       "           0.21761403,  0.21761403,  0.21302294,  0.21302294,  0.18679193,\n",
       "           0.18679193,  0.02848984,  0.02848984,  0.02144214,  0.02144214,\n",
       "           0.06514577,  0.06514577,  0.26261433,  0.26261433,  0.34488342,\n",
       "           0.34488342,  0.31042291,  0.31042291,  0.35316906,  0.35316906,\n",
       "           0.34467582,  0.34467582,  0.2974267 ,  0.2974267 ,  0.25145129,\n",
       "           0.25145129,  0.30076936,  0.30076936,  0.35701442,  0.35701442,\n",
       "           0.41313149,  0.41313149,  0.39169075,  0.39169075,  0.37205293,\n",
       "           0.37205293,  0.3146098 ,  0.3146098 ,  0.27632562,  0.27632562,\n",
       "           0.30693131,  0.30693131,  0.30234022,  0.30234022,  0.27610921,\n",
       "           0.27610921,  0.11780712,  0.11780712,  0.11075943,  0.11075943,\n",
       "           0.15446305,  0.15446305,  0.25767416,  0.25767416,  0.22321365,\n",
       "           0.22321365,  0.2659598 ,  0.2659598 ,  0.25746656,  0.25746656,\n",
       "           0.21021744,  0.21021744,  0.16424204,  0.16424204,  0.2135601 ,\n",
       "           0.2135601 ,  0.26980516,  0.26980516,  0.32592223,  0.32592223,\n",
       "           0.3044815 ,  0.3044815 ,  0.28484367,  0.28484367,  0.22740054,\n",
       "           0.22740054,  0.18911636,  0.18911636,  0.21972205,  0.21972205,\n",
       "           0.21513096,  0.21513096,  0.18889995,  0.18889995,  0.03059787,\n",
       "           0.03059787,  0.02355017,  0.02355017,  0.06725379,  0.06725379,\n",
       "           0.30548275,  0.30548275,  0.3482289 ,  0.3482289 ,  0.33973565,\n",
       "           0.33973565,  0.29248653,  0.29248653,  0.24651113,  0.24651113,\n",
       "           0.2958292 ,  0.2958292 ,  0.35207425,  0.35207425,  0.40819132,\n",
       "           0.40819132,  0.38675059,  0.38675059,  0.36711277,  0.36711277,\n",
       "           0.30966963,  0.30966963,  0.27138545,  0.27138545,  0.30199114,\n",
       "           0.30199114,  0.29740005,  0.29740005,  0.27116905,  0.27116905,\n",
       "           0.11286696,  0.11286696,  0.10581926,  0.10581926,  0.14952288,\n",
       "           0.14952288,  0.31376839,  0.31376839,  0.30527515,  0.30527515,\n",
       "           0.25802602,  0.25802602,  0.21205062,  0.21205062,  0.26136869,\n",
       "           0.26136869,  0.31761375,  0.31761375,  0.37373082,  0.37373082,\n",
       "           0.35229008,  0.35229008,  0.33265226,  0.33265226,  0.27520912,\n",
       "           0.27520912,  0.23692495,  0.23692495,  0.26753064,  0.26753064,\n",
       "           0.26293955,  0.26293955,  0.23670854,  0.23670854,  0.07840645,\n",
       "           0.07840645,  0.07135876,  0.07135876,  0.11506238,  0.11506238,\n",
       "           0.3480213 ,  0.3480213 ,  0.30077217,  0.30077217,  0.25479677,\n",
       "           0.25479677,  0.30411484,  0.30411484,  0.3603599 ,  0.3603599 ,\n",
       "           0.41647697,  0.41647697,  0.39503623,  0.39503623,  0.37539841,\n",
       "           0.37539841,  0.31795527,  0.31795527,  0.2796711 ,  0.2796711 ,\n",
       "           0.31027679,  0.31027679,  0.3056857 ,  0.3056857 ,  0.27945469,\n",
       "           0.27945469,  0.1211526 ,  0.1211526 ,  0.1141049 ,  0.1141049 ,\n",
       "           0.15780853,  0.15780853,  0.29227893,  0.29227893,  0.24630353,\n",
       "           0.24630353,  0.2956216 ,  0.2956216 ,  0.35186666,  0.35186666,\n",
       "           0.40798372,  0.40798372,  0.38654299,  0.38654299,  0.36690517,\n",
       "           0.36690517,  0.30946203,  0.30946203,  0.27117785,  0.27117785,\n",
       "           0.30178355,  0.30178355,  0.29719245,  0.29719245,  0.27096145,\n",
       "           0.27096145,  0.11265936,  0.11265936,  0.10561166,  0.10561166,\n",
       "           0.14931528,  0.14931528,  0.19905441,  0.19905441,  0.24837248,\n",
       "           0.24837248,  0.30461753,  0.30461753,  0.3607346 ,  0.3607346 ,\n",
       "           0.33929387,  0.33929387,  0.31965604,  0.31965604,  0.26221291,\n",
       "           0.26221291,  0.22392873,  0.22392873,  0.25453442,  0.25453442,\n",
       "           0.24994333,  0.24994333,  0.22371233,  0.22371233,  0.06541024,\n",
       "           0.06541024,  0.05836254,  0.05836254,  0.10206616,  0.10206616,\n",
       "           0.20239707,  0.20239707,  0.25864213,  0.25864213,  0.3147592 ,\n",
       "           0.3147592 ,  0.29331847,  0.29331847,  0.27368064,  0.27368064,\n",
       "           0.21623751,  0.21623751,  0.17795333,  0.17795333,  0.20855902,\n",
       "           0.20855902,  0.20396793,  0.20396793,  0.17773692,  0.17773692,\n",
       "           0.01943483,  0.01943483,  0.01238714,  0.01238714,  0.05609076,\n",
       "           0.05609076,  0.3079602 ,  0.3079602 ,  0.36407727,  0.36407727,\n",
       "           0.34263653,  0.34263653,  0.32299871,  0.32299871,  0.26555557,\n",
       "           0.26555557,  0.2272714 ,  0.2272714 ,  0.25787709,  0.25787709,\n",
       "           0.253286  ,  0.253286  ,  0.22705499,  0.22705499,  0.0687529 ,\n",
       "           0.0687529 ,  0.06170521,  0.06170521,  0.10540883,  0.10540883,\n",
       "           0.42032232,  0.42032232,  0.39888159,  0.39888159,  0.37924377,\n",
       "           0.37924377,  0.32180063,  0.32180063,  0.28351645,  0.28351645,\n",
       "           0.31412215,  0.31412215,  0.30953105,  0.30953105,  0.28330005,\n",
       "           0.28330005,  0.12499796,  0.12499796,  0.11795026,  0.11795026,\n",
       "           0.16165388,  0.16165388,  0.45499866,  0.45499866,  0.43536084,\n",
       "           0.43536084,  0.3779177 ,  0.3779177 ,  0.33963352,  0.33963352,\n",
       "           0.37023922,  0.37023922,  0.36564812,  0.36564812,  0.33941712,\n",
       "           0.33941712,  0.18111503,  0.18111503,  0.17406733,  0.17406733,\n",
       "           0.21777095,  0.21777095,  0.4139201 ,  0.4139201 ,  0.35647697,\n",
       "           0.35647697,  0.31819279,  0.31819279,  0.34879848,  0.34879848,\n",
       "           0.34420739,  0.34420739,  0.31797638,  0.31797638,  0.1596743 ,\n",
       "           0.1596743 ,  0.1526266 ,  0.1526266 ,  0.19633022,  0.19633022,\n",
       "           0.33683914,  0.33683914,  0.29855497,  0.29855497,  0.32916066,\n",
       "           0.32916066,  0.32456957,  0.32456957,  0.29833856,  0.29833856,\n",
       "           0.14003647,  0.14003647,  0.13298877,  0.13298877,  0.1766924 ,\n",
       "           0.1766924 ,  0.24111183,  0.24111183,  0.27171752,  0.27171752,\n",
       "           0.26712643,  0.26712643,  0.24089542,  0.24089542,  0.08259334,\n",
       "           0.08259334,  0.07554564,  0.07554564,  0.11924926,  0.11924926,\n",
       "           0.23343335,  0.23343335,  0.22884225,  0.22884225,  0.20261125,\n",
       "           0.20261125,  0.04430916,  0.04430916,  0.03726146,  0.03726146,\n",
       "           0.08096508,  0.08096508,  0.25944794,  0.25944794,  0.23321694,\n",
       "           0.23321694,  0.07491485,  0.07491485,  0.06786715,  0.06786715,\n",
       "           0.11157078,  0.11157078,  0.22862585,  0.22862585,  0.07032376,\n",
       "           0.07032376,  0.06327606,  0.06327606,  0.10697968,  0.10697968,\n",
       "           0.04409275,  0.04409275,  0.03704506,  0.03704506,  0.08074868,\n",
       "           0.08074868, -0.12125703, -0.12125703, -0.07755341, -0.07755341,\n",
       "          -0.08460111, -0.08460111]),\n",
       "   'feature_importances': array([-5.16859836e-04,  2.06879307e-04,  1.22416839e-05, ...,\n",
       "           1.86579294e-03,  1.35758924e-03, -4.77455247e-04]),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.018657489763462817,\n",
       "    'mae': 0.09982683465927214,\n",
       "    'r2': -1.2686676508584056,\n",
       "    'pearson_corr': 0.660568633044771,\n",
       "    'connectome_corr': 0.49347804968862163,\n",
       "    'connectome_r2': -1.3799254031042436,\n",
       "    'geodesic_distance': 13.44571923665314},\n",
       "   'best_val_score': -0.023830591257576472,\n",
       "   'test_metrics': {'mse': 0.026071999695419314,\n",
       "    'mae': 0.12180803788932486,\n",
       "    'r2': -0.09419271342604918,\n",
       "    'pearson_corr': 0.4195403315575419,\n",
       "    'connectome_corr': 0.2328397948720895,\n",
       "    'connectome_r2': -0.3555397065229101,\n",
       "    'geodesic_distance': 6.5205164905396025},\n",
       "   'y_true': array([ 0.33148  ,  0.33148  ,  0.25656  ,  0.25656  ,  0.20587  ,\n",
       "           0.20587  ,  0.31174  ,  0.31174  ,  0.21878  ,  0.21878  ,\n",
       "           0.21532  ,  0.21532  ,  0.17031  ,  0.17031  ,  0.2146   ,\n",
       "           0.2146   ,  0.17066  ,  0.17066  ,  0.22792  ,  0.22792  ,\n",
       "           0.16084  ,  0.16084  ,  0.35445  ,  0.35445  ,  0.29327  ,\n",
       "           0.29327  ,  0.27628  ,  0.27628  ,  0.16877  ,  0.16877  ,\n",
       "           0.25129  ,  0.25129  ,  0.23922  ,  0.23922  ,  0.24166  ,\n",
       "           0.24166  ,  0.23041  ,  0.23041  ,  0.1454   ,  0.1454   ,\n",
       "           0.2518   ,  0.2518   ,  0.2819   ,  0.2819   ,  0.1944   ,\n",
       "           0.1944   ,  0.20706  ,  0.20706  ,  0.18931  ,  0.18931  ,\n",
       "           0.1788   ,  0.1788   ,  0.09972  ,  0.09972  ,  0.73677  ,\n",
       "           0.73677  ,  0.52629  ,  0.52629  ,  0.64905  ,  0.64905  ,\n",
       "           0.37377  ,  0.37377  ,  0.28208  ,  0.28208  ,  0.17852  ,\n",
       "           0.17852  ,  0.13445  ,  0.13445  ,  0.23099  ,  0.23099  ,\n",
       "           0.18371  ,  0.18371  ,  0.32211  ,  0.32211  ,  0.5291   ,\n",
       "           0.5291   ,  0.74704  ,  0.74704  ,  0.49516  ,  0.49516  ,\n",
       "           0.3926   ,  0.3926   ,  0.57732  ,  0.57732  ,  0.39736  ,\n",
       "           0.39736  ,  0.46903  ,  0.46903  ,  0.45497  ,  0.45497  ,\n",
       "           0.25314  ,  0.25314  ,  0.28381  ,  0.28381  ,  0.29232  ,\n",
       "           0.29232  ,  0.12649  ,  0.12649  ,  0.17147  ,  0.17147  ,\n",
       "           0.11213  ,  0.11213  ,  0.14714  ,  0.14714  ,  0.16611  ,\n",
       "           0.16611  ,  0.5164   ,  0.5164   ,  0.55261  ,  0.55261  ,\n",
       "           0.36462  ,  0.36462  ,  0.23562  ,  0.23562  ,  0.18814  ,\n",
       "           0.18814  ,  0.10769  ,  0.10769  ,  0.27835  ,  0.27835  ,\n",
       "           0.16852  ,  0.16852  ,  0.2832   ,  0.2832   ,  0.99059  ,\n",
       "           0.99059  ,  1.3686   ,  1.3686   ,  0.51897  ,  0.51897  ,\n",
       "           0.39011  ,  0.39011  ,  0.46333  ,  0.46333  ,  0.42757  ,\n",
       "           0.42757  ,  0.5324   ,  0.5324   ,  0.49968  ,  0.49968  ,\n",
       "           0.18134  ,  0.18134  ,  0.34009  ,  0.34009  ,  0.35677  ,\n",
       "           0.35677  ,  0.088168 ,  0.088168 ,  0.15894  ,  0.15894  ,\n",
       "           0.056894 ,  0.056894 ,  0.12265  ,  0.12265  ,  0.14579  ,\n",
       "           0.14579  ,  0.39599  ,  0.39599  ,  0.32793  ,  0.32793  ,\n",
       "           0.3222   ,  0.3222   ,  0.17706  ,  0.17706  ,  0.17992  ,\n",
       "           0.17992  ,  0.25264  ,  0.25264  ,  0.22991  ,  0.22991  ,\n",
       "           0.25632  ,  0.25632  ,  0.39837  ,  0.39837  ,  0.49432  ,\n",
       "           0.49432  ,  0.57933  ,  0.57933  ,  0.6152   ,  0.6152   ,\n",
       "           0.38098  ,  0.38098  ,  0.30403  ,  0.30403  ,  0.3698   ,\n",
       "           0.3698   ,  0.43567  ,  0.43567  ,  0.2435   ,  0.2435   ,\n",
       "           0.15002  ,  0.15002  ,  0.18289  ,  0.18289  ,  0.14929  ,\n",
       "           0.14929  ,  0.17282  ,  0.17282  ,  0.14255  ,  0.14255  ,\n",
       "           0.16803  ,  0.16803  ,  0.20912  ,  0.20912  ,  0.31918  ,\n",
       "           0.31918  ,  0.15832  ,  0.15832  ,  0.14124  ,  0.14124  ,\n",
       "           0.12168  ,  0.12168  ,  0.21204  ,  0.21204  ,  0.090976 ,\n",
       "           0.090976 ,  0.34417  ,  0.34417  ,  0.48272  ,  0.48272  ,\n",
       "           0.57398  ,  0.57398  ,  0.37793  ,  0.37793  ,  0.27984  ,\n",
       "           0.27984  ,  0.39158  ,  0.39158  ,  0.39081  ,  0.39081  ,\n",
       "           0.46012  ,  0.46012  ,  0.42524  ,  0.42524  ,  0.1816   ,\n",
       "           0.1816   ,  0.34134  ,  0.34134  ,  0.38275  ,  0.38275  ,\n",
       "           0.05783  ,  0.05783  ,  0.17506  ,  0.17506  ,  0.020916 ,\n",
       "           0.020916 ,  0.21375  ,  0.21375  ,  0.13493  ,  0.13493  ,\n",
       "           0.18505  ,  0.18505  ,  0.23936  ,  0.23936  ,  0.10311  ,\n",
       "           0.10311  ,  0.15239  ,  0.15239  ,  0.097861 ,  0.097861 ,\n",
       "           0.18874  ,  0.18874  ,  0.33161  ,  0.33161  ,  0.35783  ,\n",
       "           0.35783  ,  0.47651  ,  0.47651  ,  0.28464  ,  0.28464  ,\n",
       "           0.38285  ,  0.38285  ,  0.49751  ,  0.49751  ,  0.43599  ,\n",
       "           0.43599  ,  0.44142  ,  0.44142  ,  0.18     ,  0.18     ,\n",
       "           0.31861  ,  0.31861  ,  0.25083  ,  0.25083  ,  0.072256 ,\n",
       "           0.072256 ,  0.20171  ,  0.20171  ,  0.08234  ,  0.08234  ,\n",
       "           0.10307  ,  0.10307  ,  0.11407  ,  0.11407  ,  0.37818  ,\n",
       "           0.37818  ,  0.70079  ,  0.70079  ,  0.22818  ,  0.22818  ,\n",
       "           0.4826   ,  0.4826   ,  0.18201  ,  0.18201  ,  0.22923  ,\n",
       "           0.22923  ,  0.22029  ,  0.22029  ,  0.32249  ,  0.32249  ,\n",
       "           0.29678  ,  0.29678  ,  0.37948  ,  0.37948  ,  0.1018   ,\n",
       "           0.1018   ,  0.11987  ,  0.11987  ,  0.15403  ,  0.15403  ,\n",
       "           0.38225  ,  0.38225  ,  0.047428 ,  0.047428 ,  0.17087  ,\n",
       "           0.17087  ,  0.48353  ,  0.48353  ,  0.28801  ,  0.28801  ,\n",
       "           0.49612  ,  0.49612  ,  0.19856  ,  0.19856  ,  0.15518  ,\n",
       "           0.15518  ,  0.35209  ,  0.35209  ,  0.15824  ,  0.15824  ,\n",
       "           0.24511  ,  0.24511  ,  0.11831  ,  0.11831  ,  0.21464  ,\n",
       "           0.21464  ,  0.1765   ,  0.1765   ,  0.24353  ,  0.24353  ,\n",
       "           0.16734  ,  0.16734  ,  0.26438  ,  0.26438  ,  0.23999  ,\n",
       "           0.23999  ,  0.19193  ,  0.19193  ,  0.22554  ,  0.22554  ,\n",
       "           0.20534  ,  0.20534  ,  0.18197  ,  0.18197  ,  0.17313  ,\n",
       "           0.17313  ,  0.2402   ,  0.2402   ,  0.34958  ,  0.34958  ,\n",
       "           0.33265  ,  0.33265  ,  0.12881  ,  0.12881  ,  0.093283 ,\n",
       "           0.093283 ,  0.24612  ,  0.24612  ,  0.54612  ,  0.54612  ,\n",
       "           0.1315   ,  0.1315   ,  0.17622  ,  0.17622  ,  0.094095 ,\n",
       "           0.094095 ,  0.20863  ,  0.20863  ,  0.18872  ,  0.18872  ,\n",
       "           0.22455  ,  0.22455  ,  0.037041 ,  0.037041 ,  0.067595 ,\n",
       "           0.067595 ,  0.071538 ,  0.071538 ,  0.26982  ,  0.26982  ,\n",
       "           0.10338  ,  0.10338  ,  0.23899  ,  0.23899  ,  0.59564  ,\n",
       "           0.59564  ,  0.30835  ,  0.30835  ,  0.58107  ,  0.58107  ,\n",
       "           0.36845  ,  0.36845  ,  0.11855  ,  0.11855  ,  0.61204  ,\n",
       "           0.61204  ,  0.37042  ,  0.37042  ,  0.31008  ,  0.31008  ,\n",
       "           0.22684  ,  0.22684  ,  0.28242  ,  0.28242  ,  0.23092  ,\n",
       "           0.23092  ,  0.24189  ,  0.24189  ,  0.068368 ,  0.068368 ,\n",
       "           0.2144   ,  0.2144   ,  0.11238  ,  0.11238  ,  0.22456  ,\n",
       "           0.22456  ,  0.074981 ,  0.074981 ,  0.45418  ,  0.45418  ,\n",
       "           0.38823  ,  0.38823  ,  0.22955  ,  0.22955  ,  0.24779  ,\n",
       "           0.24779  ,  0.46636  ,  0.46636  ,  0.231    ,  0.231    ,\n",
       "           0.26248  ,  0.26248  ,  0.21753  ,  0.21753  ,  0.13705  ,\n",
       "           0.13705  ,  0.25632  ,  0.25632  ,  0.21373  ,  0.21373  ,\n",
       "           0.23771  ,  0.23771  , -0.029437 , -0.029437 ,  0.11596  ,\n",
       "           0.11596  ,  0.028802 ,  0.028802 ,  0.31914  ,  0.31914  ,\n",
       "           0.011954 ,  0.011954 ,  0.46983  ,  0.46983  ,  0.70364  ,\n",
       "           0.70364  ,  0.28605  ,  0.28605  ,  0.52316  ,  0.52316  ,\n",
       "           0.51308  ,  0.51308  ,  0.23119  ,  0.23119  ,  0.26831  ,\n",
       "           0.26831  ,  0.26953  ,  0.26953  ,  0.28503  ,  0.28503  ,\n",
       "           0.22905  ,  0.22905  ,  0.29339  ,  0.29339  ,  0.16203  ,\n",
       "           0.16203  ,  0.23115  ,  0.23115  ,  0.18003  ,  0.18003  ,\n",
       "           0.28613  ,  0.28613  ,  0.09553  ,  0.09553  ,  0.27519  ,\n",
       "           0.27519  ,  0.21045  ,  0.21045  ,  0.19372  ,  0.19372  ,\n",
       "           0.15535  ,  0.15535  ,  0.28494  ,  0.28494  ,  0.22675  ,\n",
       "           0.22675  ,  1.0155   ,  1.0155   ,  0.46736  ,  0.46736  ,\n",
       "           0.3253   ,  0.3253   ,  0.38513  ,  0.38513  ,  0.38889  ,\n",
       "           0.38889  ,  0.47048  ,  0.47048  ,  0.43624  ,  0.43624  ,\n",
       "           0.16823  ,  0.16823  ,  0.32943  ,  0.32943  ,  0.38119  ,\n",
       "           0.38119  ,  0.15107  ,  0.15107  ,  0.21734  ,  0.21734  ,\n",
       "           0.12138  ,  0.12138  ,  0.19254  ,  0.19254  ,  0.13583  ,\n",
       "           0.13583  ,  0.50327  ,  0.50327  ,  0.36506  ,  0.36506  ,\n",
       "           0.45718  ,  0.45718  ,  0.42915  ,  0.42915  ,  0.52831  ,\n",
       "           0.52831  ,  0.49068  ,  0.49068  ,  0.17272  ,  0.17272  ,\n",
       "           0.34581  ,  0.34581  ,  0.34235  ,  0.34235  ,  0.065032 ,\n",
       "           0.065032 ,  0.1468   ,  0.1468   ,  0.042703 ,  0.042703 ,\n",
       "           0.091022 ,  0.091022 ,  0.13416  ,  0.13416  ,  0.54402  ,\n",
       "           0.54402  ,  0.52982  ,  0.52982  ,  0.51557  ,  0.51557  ,\n",
       "           0.48936  ,  0.48936  ,  0.50885  ,  0.50885  ,  0.25358  ,\n",
       "           0.25358  ,  0.29956  ,  0.29956  ,  0.31516  ,  0.31516  ,\n",
       "           0.19706  ,  0.19706  ,  0.27752  ,  0.27752  ,  0.17393  ,\n",
       "           0.17393  ,  0.22682  ,  0.22682  ,  0.20172  ,  0.20172  ,\n",
       "           0.35103  ,  0.35103  ,  0.29129  ,  0.29129  ,  0.29662  ,\n",
       "           0.29662  ,  0.37846  ,  0.37846  ,  0.2243   ,  0.2243   ,\n",
       "           0.12522  ,  0.12522  ,  0.15747  ,  0.15747  ,  0.16605  ,\n",
       "           0.16605  ,  0.17666  ,  0.17666  ,  0.15819  ,  0.15819  ,\n",
       "           0.17849  ,  0.17849  ,  0.18453  ,  0.18453  ,  0.45635  ,\n",
       "           0.45635  ,  0.46468  ,  0.46468  ,  0.45726  ,  0.45726  ,\n",
       "           0.34839  ,  0.34839  ,  0.3866   ,  0.3866   ,  0.39813  ,\n",
       "           0.39813  ,  0.35559  ,  0.35559  ,  0.28283  ,  0.28283  ,\n",
       "           0.29687  ,  0.29687  ,  0.24929  ,  0.24929  ,  0.1526   ,\n",
       "           0.1526   ,  0.57962  ,  0.57962  ,  0.67896  ,  0.67896  ,\n",
       "           0.1415   ,  0.1415   ,  0.57955  ,  0.57955  ,  0.30799  ,\n",
       "           0.30799  , -0.0033665, -0.0033665,  0.27324  ,  0.27324  ,\n",
       "           0.027971 ,  0.027971 ,  0.077822 ,  0.077822 ,  0.087999 ,\n",
       "           0.087999 ,  0.60946  ,  0.60946  ,  0.15583  ,  0.15583  ,\n",
       "           0.53966  ,  0.53966  ,  0.44818  ,  0.44818  ,  0.10311  ,\n",
       "           0.10311  ,  0.19167  ,  0.19167  ,  0.071054 ,  0.071054 ,\n",
       "           0.18039  ,  0.18039  ,  0.11114  ,  0.11114  ,  0.1465   ,\n",
       "           0.1465   ,  0.50191  ,  0.50191  ,  0.27017  ,  0.27017  ,\n",
       "           0.0045791,  0.0045791,  0.18719  ,  0.18719  ,  0.047998 ,\n",
       "           0.047998 ,  0.084946 ,  0.084946 ,  0.096601 ,  0.096601 ,\n",
       "           0.067615 ,  0.067615 ,  0.15926  ,  0.15926  ,  0.37743  ,\n",
       "           0.37743  ,  0.27607  ,  0.27607  ,  0.41946  ,  0.41946  ,\n",
       "           0.21872  ,  0.21872  ,  0.2139   ,  0.2139   ,  0.5309   ,\n",
       "           0.5309   ,  0.17173  ,  0.17173  ,  0.1977   ,  0.1977   ,\n",
       "           0.15914  ,  0.15914  ,  0.31658  ,  0.31658  ,  0.027    ,\n",
       "           0.027    ,  0.4958   ,  0.4958   ,  0.25637  ,  0.25637  ,\n",
       "           0.25208  ,  0.25208  ,  0.60549  ,  0.60549  ,  0.13979  ,\n",
       "           0.13979  ,  0.36196  ,  0.36196  ,  0.85576  ,  0.85576  ,\n",
       "           0.71396  ,  0.71396  ,  0.17641  ,  0.17641  ,  0.43945  ,\n",
       "           0.43945  ,  0.28797  ,  0.28797  ,  0.15046  ,  0.15046  ,\n",
       "           0.48791  ,  0.48791  ,  0.14908  ,  0.14908  ,  0.18887  ,\n",
       "           0.18887  ]),\n",
       "   'y_pred': array([0.38934672, 0.38934672, 0.41857467, 0.41857467, 0.43169577,\n",
       "          0.43169577, 0.39614195, 0.39614195, 0.33564279, 0.33564279,\n",
       "          0.39944882, 0.39944882, 0.35852594, 0.35852594, 0.34750234,\n",
       "          0.34750234, 0.35474994, 0.35474994, 0.35868517, 0.35868517,\n",
       "          0.26511344, 0.26511344, 0.39977248, 0.39977248, 0.48041309,\n",
       "          0.48041309, 0.39468903, 0.39468903, 0.42874792, 0.42874792,\n",
       "          0.39306964, 0.39306964, 0.33620921, 0.33620921, 0.38129979,\n",
       "          0.38129979, 0.3703547 , 0.3703547 , 0.30535945, 0.30535945,\n",
       "          0.33342473, 0.33342473, 0.41334816, 0.41334816, 0.38282271,\n",
       "          0.38282271, 0.32758488, 0.32758488, 0.308716  , 0.308716  ,\n",
       "          0.34748418, 0.34748418, 0.23595693, 0.23595693, 0.35602415,\n",
       "          0.35602415, 0.36914524, 0.36914524, 0.33359142, 0.33359142,\n",
       "          0.27309226, 0.27309226, 0.33689829, 0.33689829, 0.29597542,\n",
       "          0.29597542, 0.28495181, 0.28495181, 0.29219941, 0.29219941,\n",
       "          0.29613464, 0.29613464, 0.20256292, 0.20256292, 0.33722195,\n",
       "          0.33722195, 0.41786256, 0.41786256, 0.33213851, 0.33213851,\n",
       "          0.3661974 , 0.3661974 , 0.33051912, 0.33051912, 0.27365869,\n",
       "          0.27365869, 0.31874926, 0.31874926, 0.30780418, 0.30780418,\n",
       "          0.24280893, 0.24280893, 0.27087421, 0.27087421, 0.35079764,\n",
       "          0.35079764, 0.32027218, 0.32027218, 0.26503436, 0.26503436,\n",
       "          0.24616547, 0.24616547, 0.28493366, 0.28493366, 0.17340641,\n",
       "          0.17340641, 0.3983732 , 0.3983732 , 0.36281937, 0.36281937,\n",
       "          0.30232021, 0.30232021, 0.36612624, 0.36612624, 0.32520337,\n",
       "          0.32520337, 0.31417977, 0.31417977, 0.32142737, 0.32142737,\n",
       "          0.3253626 , 0.3253626 , 0.23179087, 0.23179087, 0.3664499 ,\n",
       "          0.3664499 , 0.44709052, 0.44709052, 0.36136646, 0.36136646,\n",
       "          0.39542535, 0.39542535, 0.35974707, 0.35974707, 0.30288664,\n",
       "          0.30288664, 0.34797722, 0.34797722, 0.33703213, 0.33703213,\n",
       "          0.27203688, 0.27203688, 0.30010216, 0.30010216, 0.38002559,\n",
       "          0.38002559, 0.34950013, 0.34950013, 0.29426231, 0.29426231,\n",
       "          0.27539342, 0.27539342, 0.31416161, 0.31416161, 0.20263436,\n",
       "          0.20263436, 0.37594047, 0.37594047, 0.31544131, 0.31544131,\n",
       "          0.37924734, 0.37924734, 0.33832446, 0.33832446, 0.32730086,\n",
       "          0.32730086, 0.33454846, 0.33454846, 0.33848369, 0.33848369,\n",
       "          0.24491196, 0.24491196, 0.379571  , 0.379571  , 0.46021161,\n",
       "          0.46021161, 0.37448755, 0.37448755, 0.40854644, 0.40854644,\n",
       "          0.37286816, 0.37286816, 0.31600773, 0.31600773, 0.36109831,\n",
       "          0.36109831, 0.35015322, 0.35015322, 0.28515797, 0.28515797,\n",
       "          0.31322325, 0.31322325, 0.39314669, 0.39314669, 0.36262123,\n",
       "          0.36262123, 0.30738341, 0.30738341, 0.28851452, 0.28851452,\n",
       "          0.3272827 , 0.3272827 , 0.21575545, 0.21575545, 0.27988749,\n",
       "          0.27988749, 0.34369352, 0.34369352, 0.30277064, 0.30277064,\n",
       "          0.29174704, 0.29174704, 0.29899464, 0.29899464, 0.30292987,\n",
       "          0.30292987, 0.20935814, 0.20935814, 0.34401718, 0.34401718,\n",
       "          0.42465779, 0.42465779, 0.33893373, 0.33893373, 0.37299262,\n",
       "          0.37299262, 0.33731434, 0.33731434, 0.28045391, 0.28045391,\n",
       "          0.32554449, 0.32554449, 0.3145994 , 0.3145994 , 0.24960415,\n",
       "          0.24960415, 0.27766943, 0.27766943, 0.35759286, 0.35759286,\n",
       "          0.32706741, 0.32706741, 0.27182958, 0.27182958, 0.2529607 ,\n",
       "          0.2529607 , 0.29172888, 0.29172888, 0.18020163, 0.18020163,\n",
       "          0.28319435, 0.28319435, 0.24227148, 0.24227148, 0.23124788,\n",
       "          0.23124788, 0.23849548, 0.23849548, 0.24243071, 0.24243071,\n",
       "          0.14885898, 0.14885898, 0.28351801, 0.28351801, 0.36415863,\n",
       "          0.36415863, 0.27843457, 0.27843457, 0.31249346, 0.31249346,\n",
       "          0.27681518, 0.27681518, 0.21995475, 0.21995475, 0.26504533,\n",
       "          0.26504533, 0.25410024, 0.25410024, 0.18910499, 0.18910499,\n",
       "          0.21717027, 0.21717027, 0.2970937 , 0.2970937 , 0.26656825,\n",
       "          0.26656825, 0.21133042, 0.21133042, 0.19246153, 0.19246153,\n",
       "          0.23122972, 0.23122972, 0.11970247, 0.11970247, 0.30607751,\n",
       "          0.30607751, 0.29505391, 0.29505391, 0.30230151, 0.30230151,\n",
       "          0.30623674, 0.30623674, 0.21266501, 0.21266501, 0.34732405,\n",
       "          0.34732405, 0.42796466, 0.42796466, 0.3422406 , 0.3422406 ,\n",
       "          0.37629949, 0.37629949, 0.34062121, 0.34062121, 0.28376078,\n",
       "          0.28376078, 0.32885136, 0.32885136, 0.31790627, 0.31790627,\n",
       "          0.25291102, 0.25291102, 0.2809763 , 0.2809763 , 0.36089973,\n",
       "          0.36089973, 0.33037428, 0.33037428, 0.27513645, 0.27513645,\n",
       "          0.25626757, 0.25626757, 0.29503575, 0.29503575, 0.1835085 ,\n",
       "          0.1835085 , 0.25413104, 0.25413104, 0.26137863, 0.26137863,\n",
       "          0.26531387, 0.26531387, 0.17174214, 0.17174214, 0.30640117,\n",
       "          0.30640117, 0.38704178, 0.38704178, 0.30131773, 0.30131773,\n",
       "          0.33537662, 0.33537662, 0.29969834, 0.29969834, 0.24283791,\n",
       "          0.24283791, 0.28792849, 0.28792849, 0.2769834 , 0.2769834 ,\n",
       "          0.21198815, 0.21198815, 0.24005343, 0.24005343, 0.31997686,\n",
       "          0.31997686, 0.2894514 , 0.2894514 , 0.23421358, 0.23421358,\n",
       "          0.21534469, 0.21534469, 0.25411288, 0.25411288, 0.14258563,\n",
       "          0.14258563, 0.25035503, 0.25035503, 0.25429026, 0.25429026,\n",
       "          0.16071853, 0.16071853, 0.29537757, 0.29537757, 0.37601818,\n",
       "          0.37601818, 0.29029413, 0.29029413, 0.32435301, 0.32435301,\n",
       "          0.28867474, 0.28867474, 0.2318143 , 0.2318143 , 0.27690488,\n",
       "          0.27690488, 0.26595979, 0.26595979, 0.20096454, 0.20096454,\n",
       "          0.22902982, 0.22902982, 0.30895326, 0.30895326, 0.2784278 ,\n",
       "          0.2784278 , 0.22318998, 0.22318998, 0.20432109, 0.20432109,\n",
       "          0.24308927, 0.24308927, 0.13156202, 0.13156202, 0.26153786,\n",
       "          0.26153786, 0.16796613, 0.16796613, 0.30262517, 0.30262517,\n",
       "          0.38326578, 0.38326578, 0.29754172, 0.29754172, 0.33160061,\n",
       "          0.33160061, 0.29592233, 0.29592233, 0.2390619 , 0.2390619 ,\n",
       "          0.28415248, 0.28415248, 0.27320739, 0.27320739, 0.20821214,\n",
       "          0.20821214, 0.23627742, 0.23627742, 0.31620085, 0.31620085,\n",
       "          0.2856754 , 0.2856754 , 0.23043758, 0.23043758, 0.21156869,\n",
       "          0.21156869, 0.25033687, 0.25033687, 0.13880962, 0.13880962,\n",
       "          0.17190136, 0.17190136, 0.3065604 , 0.3065604 , 0.38720101,\n",
       "          0.38720101, 0.30147696, 0.30147696, 0.33553585, 0.33553585,\n",
       "          0.29985757, 0.29985757, 0.24299713, 0.24299713, 0.28808771,\n",
       "          0.28808771, 0.27714263, 0.27714263, 0.21214737, 0.21214737,\n",
       "          0.24021266, 0.24021266, 0.32013609, 0.32013609, 0.28961063,\n",
       "          0.28961063, 0.23437281, 0.23437281, 0.21550392, 0.21550392,\n",
       "          0.2542721 , 0.2542721 , 0.14274485, 0.14274485, 0.21298867,\n",
       "          0.21298867, 0.29362928, 0.29362928, 0.20790523, 0.20790523,\n",
       "          0.24196412, 0.24196412, 0.20628584, 0.20628584, 0.14942541,\n",
       "          0.14942541, 0.19451598, 0.19451598, 0.1835709 , 0.1835709 ,\n",
       "          0.11857565, 0.11857565, 0.14664093, 0.14664093, 0.22656436,\n",
       "          0.22656436, 0.1960389 , 0.1960389 , 0.14080108, 0.14080108,\n",
       "          0.12193219, 0.12193219, 0.16070038, 0.16070038, 0.04917313,\n",
       "          0.04917313, 0.42828832, 0.42828832, 0.34256426, 0.34256426,\n",
       "          0.37662315, 0.37662315, 0.34094487, 0.34094487, 0.28408444,\n",
       "          0.28408444, 0.32917502, 0.32917502, 0.31822993, 0.31822993,\n",
       "          0.25323468, 0.25323468, 0.28129996, 0.28129996, 0.36122339,\n",
       "          0.36122339, 0.33069794, 0.33069794, 0.27546011, 0.27546011,\n",
       "          0.25659123, 0.25659123, 0.29535941, 0.29535941, 0.18383216,\n",
       "          0.18383216, 0.42320487, 0.42320487, 0.45726376, 0.45726376,\n",
       "          0.42158548, 0.42158548, 0.36472505, 0.36472505, 0.40981563,\n",
       "          0.40981563, 0.39887054, 0.39887054, 0.33387529, 0.33387529,\n",
       "          0.36194057, 0.36194057, 0.44186401, 0.44186401, 0.41133855,\n",
       "          0.41133855, 0.35610073, 0.35610073, 0.33723184, 0.33723184,\n",
       "          0.37600002, 0.37600002, 0.26447277, 0.26447277, 0.37153971,\n",
       "          0.37153971, 0.33586143, 0.33586143, 0.279001  , 0.279001  ,\n",
       "          0.32409158, 0.32409158, 0.31314649, 0.31314649, 0.24815124,\n",
       "          0.24815124, 0.27621652, 0.27621652, 0.35613995, 0.35613995,\n",
       "          0.32561449, 0.32561449, 0.27037667, 0.27037667, 0.25150778,\n",
       "          0.25150778, 0.29027597, 0.29027597, 0.17874872, 0.17874872,\n",
       "          0.36992032, 0.36992032, 0.31305989, 0.31305989, 0.35815047,\n",
       "          0.35815047, 0.34720538, 0.34720538, 0.28221013, 0.28221013,\n",
       "          0.31027541, 0.31027541, 0.39019884, 0.39019884, 0.35967338,\n",
       "          0.35967338, 0.30443556, 0.30443556, 0.28556667, 0.28556667,\n",
       "          0.32433486, 0.32433486, 0.21280761, 0.21280761, 0.27738161,\n",
       "          0.27738161, 0.32247219, 0.32247219, 0.3115271 , 0.3115271 ,\n",
       "          0.24653185, 0.24653185, 0.27459713, 0.27459713, 0.35452056,\n",
       "          0.35452056, 0.3239951 , 0.3239951 , 0.26875728, 0.26875728,\n",
       "          0.24988839, 0.24988839, 0.28865658, 0.28865658, 0.17712933,\n",
       "          0.17712933, 0.26561175, 0.26561175, 0.25466667, 0.25466667,\n",
       "          0.18967142, 0.18967142, 0.2177367 , 0.2177367 , 0.29766013,\n",
       "          0.29766013, 0.26713467, 0.26713467, 0.21189685, 0.21189685,\n",
       "          0.19302796, 0.19302796, 0.23179615, 0.23179615, 0.1202689 ,\n",
       "          0.1202689 , 0.29975725, 0.29975725, 0.23476199, 0.23476199,\n",
       "          0.26282728, 0.26282728, 0.34275071, 0.34275071, 0.31222525,\n",
       "          0.31222525, 0.25698743, 0.25698743, 0.23811854, 0.23811854,\n",
       "          0.27688672, 0.27688672, 0.16535947, 0.16535947, 0.22381691,\n",
       "          0.22381691, 0.25188219, 0.25188219, 0.33180562, 0.33180562,\n",
       "          0.30128016, 0.30128016, 0.24604234, 0.24604234, 0.22717345,\n",
       "          0.22717345, 0.26594164, 0.26594164, 0.15441439, 0.15441439,\n",
       "          0.18688694, 0.18688694, 0.26681037, 0.26681037, 0.23628491,\n",
       "          0.23628491, 0.18104709, 0.18104709, 0.1621782 , 0.1621782 ,\n",
       "          0.20094639, 0.20094639, 0.08941914, 0.08941914, 0.29487565,\n",
       "          0.29487565, 0.26435019, 0.26435019, 0.20911237, 0.20911237,\n",
       "          0.19024348, 0.19024348, 0.22901167, 0.22901167, 0.11748442,\n",
       "          0.11748442, 0.34427362, 0.34427362, 0.2890358 , 0.2890358 ,\n",
       "          0.27016691, 0.27016691, 0.3089351 , 0.3089351 , 0.19740785,\n",
       "          0.19740785, 0.25851034, 0.25851034, 0.23964146, 0.23964146,\n",
       "          0.27840964, 0.27840964, 0.16688239, 0.16688239, 0.18440363,\n",
       "          0.18440363, 0.22317182, 0.22317182, 0.11164457, 0.11164457,\n",
       "          0.20430293, 0.20430293, 0.09277568, 0.09277568, 0.13154387,\n",
       "          0.13154387]),\n",
       "   'feature_importances': array([-6.46521271e-04,  2.60630184e-04,  9.95232744e-05, ...,\n",
       "           1.07590086e-03,  4.70727613e-04, -8.15994667e-04]),\n",
       "   'model_json': None},\n",
       "  {'model_parameters': {'alpha': 1000,\n",
       "    'copy_X': True,\n",
       "    'fit_intercept': True,\n",
       "    'max_iter': None,\n",
       "    'positive': False,\n",
       "    'random_state': None,\n",
       "    'solver': 'auto',\n",
       "    'tol': 0.0001},\n",
       "   'train_metrics': {'mse': 0.02127987826931947,\n",
       "    'mae': 0.10500310714815986,\n",
       "    'r2': -0.8717232405556377,\n",
       "    'pearson_corr': 0.611714979310548,\n",
       "    'connectome_corr': 0.47874871303181044,\n",
       "    'connectome_r2': -0.9863603332788347,\n",
       "    'geodesic_distance': 12.700045558568583},\n",
       "   'best_val_score': -0.02457597320735146,\n",
       "   'test_metrics': {'mse': 0.025004078413256397,\n",
       "    'mae': 0.11205724440102598,\n",
       "    'r2': -2.7678658059296204,\n",
       "    'pearson_corr': 0.548938108469077,\n",
       "    'connectome_corr': 0.2803813049298135,\n",
       "    'connectome_r2': -3.2599146585393646,\n",
       "    'geodesic_distance': 5.2425304384341525},\n",
       "   'y_true': array([ 0.81838 ,  0.81838 ,  0.44134 ,  0.44134 ,  0.3922  ,  0.3922  ,\n",
       "           0.33937 ,  0.33937 ,  0.31612 ,  0.31612 ,  0.34407 ,  0.34407 ,\n",
       "           0.1723  ,  0.1723  ,  0.26053 ,  0.26053 ,  0.9718  ,  0.9718  ,\n",
       "           0.733   ,  0.733   ,  0.40486 ,  0.40486 ,  0.3934  ,  0.3934  ,\n",
       "           0.39635 ,  0.39635 ,  0.39357 ,  0.39357 ,  0.3708  ,  0.3708  ,\n",
       "           0.17249 ,  0.17249 ,  0.13774 ,  0.13774 ,  0.24104 ,  0.24104 ,\n",
       "           0.34322 ,  0.34322 ,  0.22774 ,  0.22774 ,  0.14728 ,  0.14728 ,\n",
       "           0.018301,  0.018301,  0.097647,  0.097647,  0.11349 ,  0.11349 ,\n",
       "           0.14274 ,  0.14274 ,  0.087701,  0.087701,  0.14287 ,  0.14287 ,\n",
       "           0.4315  ,  0.4315  ,  0.4144  ,  0.4144  ,  0.30494 ,  0.30494 ,\n",
       "           0.33556 ,  0.33556 ,  0.35054 ,  0.35054 ,  0.13225 ,  0.13225 ,\n",
       "           0.20896 ,  0.20896 ,  0.88472 ,  0.88472 ,  0.56087 ,  0.56087 ,\n",
       "           0.40877 ,  0.40877 ,  0.40643 ,  0.40643 ,  0.37281 ,  0.37281 ,\n",
       "           0.35164 ,  0.35164 ,  0.39117 ,  0.39117 ,  0.12708 ,  0.12708 ,\n",
       "           0.12771 ,  0.12771 ,  0.23802 ,  0.23802 ,  0.30795 ,  0.30795 ,\n",
       "           0.18453 ,  0.18453 ,  0.11813 ,  0.11813 ,  0.014342,  0.014342,\n",
       "           0.081532,  0.081532,  0.10419 ,  0.10419 ,  0.14018 ,  0.14018 ,\n",
       "           0.070059,  0.070059,  0.13455 ,  0.13455 ,  0.652   ,  0.652   ,\n",
       "           0.31912 ,  0.31912 ,  0.3282  ,  0.3282  ,  0.52059 ,  0.52059 ,\n",
       "           0.16183 ,  0.16183 ,  0.29156 ,  0.29156 ,  0.44222 ,  0.44222 ,\n",
       "           0.4727  ,  0.4727  ,  0.61856 ,  0.61856 ,  0.57925 ,  0.57925 ,\n",
       "           0.70341 ,  0.70341 ,  0.31806 ,  0.31806 ,  0.39637 ,  0.39637 ,\n",
       "           0.12368 ,  0.12368 ,  0.10866 ,  0.10866 ,  0.21268 ,  0.21268 ,\n",
       "           0.37674 ,  0.37674 ,  0.19332 ,  0.19332 ,  0.17454 ,  0.17454 ,\n",
       "           0.025401,  0.025401,  0.10078 ,  0.10078 ,  0.1409  ,  0.1409  ,\n",
       "           0.18789 ,  0.18789 ,  0.083905,  0.083905,  0.15993 ,  0.15993 ,\n",
       "           0.31714 ,  0.31714 ,  0.47498 ,  0.47498 ,  0.46442 ,  0.46442 ,\n",
       "           0.108   ,  0.108   ,  0.17035 ,  0.17035 ,  0.40173 ,  0.40173 ,\n",
       "           0.45769 ,  0.45769 ,  0.49706 ,  0.49706 ,  0.49691 ,  0.49691 ,\n",
       "           0.45997 ,  0.45997 ,  0.32508 ,  0.32508 ,  0.57744 ,  0.57744 ,\n",
       "           0.08625 ,  0.08625 ,  0.11305 ,  0.11305 ,  0.20143 ,  0.20143 ,\n",
       "           0.26651 ,  0.26651 ,  0.12242 ,  0.12242 ,  0.10453 ,  0.10453 ,\n",
       "           0.016032,  0.016032,  0.07877 ,  0.07877 ,  0.1242  ,  0.1242  ,\n",
       "           0.1604  ,  0.1604  ,  0.065636,  0.065636,  0.14085 ,  0.14085 ,\n",
       "           0.61055 ,  0.61055 ,  0.48032 ,  0.48032 ,  0.34818 ,  0.34818 ,\n",
       "           0.13401 ,  0.13401 ,  0.31784 ,  0.31784 ,  0.41468 ,  0.41468 ,\n",
       "           0.31134 ,  0.31134 ,  0.3635  ,  0.3635  ,  0.26366 ,  0.26366 ,\n",
       "           0.72974 ,  0.72974 ,  0.50262 ,  0.50262 ,  0.1968  ,  0.1968  ,\n",
       "           0.063801,  0.063801,  0.31384 ,  0.31384 ,  0.27739 ,  0.27739 ,\n",
       "           0.32239 ,  0.32239 , -0.05647 , -0.05647 ,  0.010482,  0.010482,\n",
       "           0.16089 ,  0.16089 ,  0.18213 ,  0.18213 ,  0.15926 ,  0.15926 ,\n",
       "           0.13811 ,  0.13811 ,  0.13602 ,  0.13602 ,  0.51535 ,  0.51535 ,\n",
       "           0.19514 ,  0.19514 ,  0.026434,  0.026434,  0.31163 ,  0.31163 ,\n",
       "           0.33739 ,  0.33739 ,  0.36784 ,  0.36784 ,  0.48796 ,  0.48796 ,\n",
       "           0.17221 ,  0.17221 ,  0.59915 ,  0.59915 ,  0.9465  ,  0.9465  ,\n",
       "           0.1477  ,  0.1477  ,  0.16485 ,  0.16485 ,  0.36548 ,  0.36548 ,\n",
       "           0.28938 ,  0.28938 ,  0.34738 ,  0.34738 , -0.069929, -0.069929,\n",
       "           0.012004,  0.012004,  0.13788 ,  0.13788 ,  0.19761 ,  0.19761 ,\n",
       "           0.19086 ,  0.19086 ,  0.126   ,  0.126   ,  0.17964 ,  0.17964 ,\n",
       "           0.11124 ,  0.11124 ,  0.17191 ,  0.17191 ,  0.34755 ,  0.34755 ,\n",
       "           0.35721 ,  0.35721 ,  0.43137 ,  0.43137 ,  0.53786 ,  0.53786 ,\n",
       "           0.3672  ,  0.3672  ,  0.43597 ,  0.43597 ,  0.51059 ,  0.51059 ,\n",
       "           0.086054,  0.086054,  0.086132,  0.086132,  0.28085 ,  0.28085 ,\n",
       "           0.31346 ,  0.31346 ,  0.24337 ,  0.24337 ,  0.022592,  0.022592,\n",
       "           0.018079,  0.018079,  0.13688 ,  0.13688 ,  0.18395 ,  0.18395 ,\n",
       "           0.16099 ,  0.16099 ,  0.11672 ,  0.11672 ,  0.14472 ,  0.14472 ,\n",
       "           0.2988  ,  0.2988  ,  0.14685 ,  0.14685 ,  0.20041 ,  0.20041 ,\n",
       "           0.14503 ,  0.14503 ,  0.1079  ,  0.1079  ,  0.1484  ,  0.1484  ,\n",
       "           0.21373 ,  0.21373 ,  0.092073,  0.092073,  0.3208  ,  0.3208  ,\n",
       "           0.15637 ,  0.15637 ,  0.19179 ,  0.19179 ,  0.2362  ,  0.2362  ,\n",
       "           0.23041 ,  0.23041 ,  0.22145 ,  0.22145 ,  0.018007,  0.018007,\n",
       "           0.13568 ,  0.13568 ,  0.097995,  0.097995,  0.13314 ,  0.13314 ,\n",
       "           0.11861 ,  0.11861 ,  0.11165 ,  0.11165 ,  0.25668 ,  0.25668 ,\n",
       "           0.27928 ,  0.27928 ,  0.25164 ,  0.25164 ,  0.17766 ,  0.17766 ,\n",
       "           0.30654 ,  0.30654 ,  0.04025 ,  0.04025 , -0.019243, -0.019243,\n",
       "           0.10886 ,  0.10886 ,  0.079629,  0.079629,  0.037198,  0.037198,\n",
       "           0.55236 ,  0.55236 ,  0.31815 ,  0.31815 ,  0.47026 ,  0.47026 ,\n",
       "           0.023071,  0.023071,  0.096659,  0.096659,  0.07033 ,  0.07033 ,\n",
       "           0.088388,  0.088388,  0.06929 ,  0.06929 ,  0.063706,  0.063706,\n",
       "           0.75899 ,  0.75899 ,  0.4182  ,  0.4182  ,  0.41105 ,  0.41105 ,\n",
       "           0.40217 ,  0.40217 ,  0.38907 ,  0.38907 ,  0.38795 ,  0.38795 ,\n",
       "           0.13881 ,  0.13881 ,  0.1268  ,  0.1268  ,  0.22659 ,  0.22659 ,\n",
       "           0.34833 ,  0.34833 ,  0.20942 ,  0.20942 ,  0.13749 ,  0.13749 ,\n",
       "           0.015259,  0.015259,  0.08026 ,  0.08026 ,  0.1011  ,  0.1011  ,\n",
       "           0.13007 ,  0.13007 ,  0.068697,  0.068697,  0.13054 ,  0.13054 ,\n",
       "           0.41689 ,  0.41689 ,  0.41408 ,  0.41408 ,  0.4566  ,  0.4566  ,\n",
       "           0.52749 ,  0.52749 ,  0.47462 ,  0.47462 ,  0.15207 ,  0.15207 ,\n",
       "           0.10558 ,  0.10558 ,  0.20897 ,  0.20897 ,  0.38513 ,  0.38513 ,\n",
       "           0.22266 ,  0.22266 ,  0.13338 ,  0.13338 ,  0.023153,  0.023153,\n",
       "           0.10571 ,  0.10571 ,  0.12122 ,  0.12122 ,  0.14254 ,  0.14254 ,\n",
       "           0.095759,  0.095759,  0.14333 ,  0.14333 ,  0.68306 ,  0.68306 ,\n",
       "           0.5435  ,  0.5435  ,  0.33372 ,  0.33372 ,  0.48485 ,  0.48485 ,\n",
       "           0.13303 ,  0.13303 ,  0.13193 ,  0.13193 ,  0.24759 ,  0.24759 ,\n",
       "           0.35667 ,  0.35667 ,  0.20544 ,  0.20544 ,  0.16099 ,  0.16099 ,\n",
       "           0.023318,  0.023318,  0.097558,  0.097558,  0.14163 ,  0.14163 ,\n",
       "           0.1726  ,  0.1726  ,  0.084644,  0.084644,  0.1691  ,  0.1691  ,\n",
       "           0.48128 ,  0.48128 ,  0.43514 ,  0.43514 ,  0.67583 ,  0.67583 ,\n",
       "           0.10531 ,  0.10531 ,  0.1171  ,  0.1171  ,  0.25078 ,  0.25078 ,\n",
       "           0.34208 ,  0.34208 ,  0.20078 ,  0.20078 ,  0.068998,  0.068998,\n",
       "           0.018846,  0.018846,  0.09443 ,  0.09443 ,  0.16398 ,  0.16398 ,\n",
       "           0.15269 ,  0.15269 ,  0.082883,  0.082883,  0.14856 ,  0.14856 ,\n",
       "           0.25675 ,  0.25675 ,  0.25601 ,  0.25601 ,  0.10158 ,  0.10158 ,\n",
       "           0.055389,  0.055389,  0.12263 ,  0.12263 ,  0.35918 ,  0.35918 ,\n",
       "           0.16189 ,  0.16189 ,  0.1744  ,  0.1744  ,  0.019853,  0.019853,\n",
       "           0.069583,  0.069583,  0.076881,  0.076881,  0.11079 ,  0.11079 ,\n",
       "           0.053991,  0.053991,  0.10556 ,  0.10556 ,  0.70754 ,  0.70754 ,\n",
       "           0.28536 ,  0.28536 ,  0.12229 ,  0.12229 ,  0.40747 ,  0.40747 ,\n",
       "           0.34977 ,  0.34977 ,  0.45087 ,  0.45087 , -0.029604, -0.029604,\n",
       "           0.012842,  0.012842,  0.14106 ,  0.14106 ,  0.17495 ,  0.17495 ,\n",
       "           0.15435 ,  0.15435 ,  0.14986 ,  0.14986 ,  0.16964 ,  0.16964 ,\n",
       "           0.11954 ,  0.11954 ,  0.15531 ,  0.15531 ,  0.34743 ,  0.34743 ,\n",
       "           0.25542 ,  0.25542 ,  0.2543  ,  0.2543  , -0.076543, -0.076543,\n",
       "           0.01228 ,  0.01228 ,  0.10778 ,  0.10778 ,  0.1859  ,  0.1859  ,\n",
       "           0.1875  ,  0.1875  ,  0.10722 ,  0.10722 ,  0.18352 ,  0.18352 ,\n",
       "           0.32773 ,  0.32773 ,  0.43278 ,  0.43278 ,  0.28433 ,  0.28433 ,\n",
       "           0.34246 ,  0.34246 ,  0.31868 ,  0.31868 ,  0.021865,  0.021865,\n",
       "           0.15572 ,  0.15572 ,  0.10647 ,  0.10647 ,  0.14874 ,  0.14874 ,\n",
       "           0.19075 ,  0.19075 ,  0.17451 ,  0.17451 ,  0.33589 ,  0.33589 ,\n",
       "           0.15461 ,  0.15461 ,  0.12699 ,  0.12699 ,  0.38227 ,  0.38227 ,\n",
       "           0.031151,  0.031151,  0.12646 ,  0.12646 ,  0.10322 ,  0.10322 ,\n",
       "           0.16502 ,  0.16502 ,  0.13441 ,  0.13441 ,  0.17819 ,  0.17819 ,\n",
       "           0.23785 ,  0.23785 ,  0.30948 ,  0.30948 ,  0.17583 ,  0.17583 ,\n",
       "           0.038642,  0.038642,  0.19682 ,  0.19682 ,  0.17838 ,  0.17838 ,\n",
       "           0.21146 ,  0.21146 ,  0.21378 ,  0.21378 ,  0.22637 ,  0.22637 ,\n",
       "           0.60657 ,  0.60657 ,  0.33179 ,  0.33179 ,  0.019819,  0.019819,\n",
       "           0.1191  ,  0.1191  ,  0.10777 ,  0.10777 ,  0.11949 ,  0.11949 ,\n",
       "           0.11785 ,  0.11785 ,  0.12269 ,  0.12269 ,  0.19324 ,  0.19324 ,\n",
       "           0.010799,  0.010799,  0.14439 ,  0.14439 ,  0.12377 ,  0.12377 ,\n",
       "           0.12771 ,  0.12771 ,  0.15287 ,  0.15287 ,  0.13668 ,  0.13668 ,\n",
       "           0.040826,  0.040826,  0.11812 ,  0.11812 ,  0.062458,  0.062458,\n",
       "           0.14295 ,  0.14295 ,  0.1203  ,  0.1203  ,  0.14825 ,  0.14825 ,\n",
       "           0.07886 ,  0.07886 ,  0.060065,  0.060065,  0.04385 ,  0.04385 ,\n",
       "           0.06811 ,  0.06811 ,  0.045071,  0.045071,  0.28453 ,  0.28453 ,\n",
       "           0.25202 ,  0.25202 ,  0.40311 ,  0.40311 ,  0.23479 ,  0.23479 ,\n",
       "           0.22615 ,  0.22615 ,  0.27356 ,  0.27356 ,  0.21588 ,  0.21588 ,\n",
       "           0.23607 ,  0.23607 ,  0.35804 ,  0.35804 ,  0.24593 ,  0.24593 ]),\n",
       "   'y_pred': array([ 0.36005686,  0.36005686,  0.38099286,  0.38099286,  0.36531488,\n",
       "           0.36531488,  0.34321701,  0.34321701,  0.34968519,  0.34968519,\n",
       "           0.33141066,  0.33141066,  0.34304291,  0.34304291,  0.32292691,\n",
       "           0.32292691,  0.35855505,  0.35855505,  0.33960209,  0.33960209,\n",
       "           0.29900314,  0.29900314,  0.37045009,  0.37045009,  0.38701342,\n",
       "           0.38701342,  0.3483129 ,  0.3483129 ,  0.35628533,  0.35628533,\n",
       "           0.27416022,  0.27416022,  0.3402443 ,  0.3402443 ,  0.31244663,\n",
       "           0.31244663,  0.32955888,  0.32955888,  0.2940104 ,  0.2940104 ,\n",
       "           0.32749496,  0.32749496,  0.03755182,  0.03755182,  0.09954425,\n",
       "           0.09954425,  0.14473415,  0.14473415,  0.1212222 ,  0.1212222 ,\n",
       "           0.11559097,  0.11559097,  0.1428257 ,  0.1428257 ,  0.35071971,\n",
       "           0.35071971,  0.33504173,  0.33504173,  0.31294387,  0.31294387,\n",
       "           0.31941204,  0.31941204,  0.30113751,  0.30113751,  0.31276976,\n",
       "           0.31276976,  0.29265377,  0.29265377,  0.3282819 ,  0.3282819 ,\n",
       "           0.30932894,  0.30932894,  0.26872999,  0.26872999,  0.34017694,\n",
       "           0.34017694,  0.35674028,  0.35674028,  0.31803975,  0.31803975,\n",
       "           0.32601218,  0.32601218,  0.24388708,  0.24388708,  0.30997115,\n",
       "           0.30997115,  0.28217348,  0.28217348,  0.29928573,  0.29928573,\n",
       "           0.26373725,  0.26373725,  0.29722181,  0.29722181,  0.00727867,\n",
       "           0.00727867,  0.0692711 ,  0.0692711 ,  0.11446101,  0.11446101,\n",
       "           0.09094905,  0.09094905,  0.08531782,  0.08531782,  0.11255255,\n",
       "           0.11255255,  0.35597773,  0.35597773,  0.33387987,  0.33387987,\n",
       "           0.34034804,  0.34034804,  0.32207351,  0.32207351,  0.33370576,\n",
       "           0.33370576,  0.31358977,  0.31358977,  0.3492179 ,  0.3492179 ,\n",
       "           0.33026494,  0.33026494,  0.289666  ,  0.289666  ,  0.36111294,\n",
       "           0.36111294,  0.37767628,  0.37767628,  0.33897575,  0.33897575,\n",
       "           0.34694818,  0.34694818,  0.26482308,  0.26482308,  0.33090715,\n",
       "           0.33090715,  0.30310948,  0.30310948,  0.32022173,  0.32022173,\n",
       "           0.28467325,  0.28467325,  0.31815781,  0.31815781,  0.02821467,\n",
       "           0.02821467,  0.09020711,  0.09020711,  0.13539701,  0.13539701,\n",
       "           0.11188505,  0.11188505,  0.10625382,  0.10625382,  0.13348855,\n",
       "           0.13348855,  0.31820188,  0.31820188,  0.32467006,  0.32467006,\n",
       "           0.30639552,  0.30639552,  0.31802778,  0.31802778,  0.29791178,\n",
       "           0.29791178,  0.33353992,  0.33353992,  0.31458695,  0.31458695,\n",
       "           0.27398801,  0.27398801,  0.34543495,  0.34543495,  0.36199829,\n",
       "           0.36199829,  0.32329776,  0.32329776,  0.33127019,  0.33127019,\n",
       "           0.24914509,  0.24914509,  0.31522917,  0.31522917,  0.28743149,\n",
       "           0.28743149,  0.30454375,  0.30454375,  0.26899527,  0.26899527,\n",
       "           0.30247982,  0.30247982,  0.01253668,  0.01253668,  0.07452912,\n",
       "           0.07452912,  0.11971902,  0.11971902,  0.09620707,  0.09620707,\n",
       "           0.09057583,  0.09057583,  0.11781057,  0.11781057,  0.30257219,\n",
       "           0.30257219,  0.28429766,  0.28429766,  0.29592992,  0.29592992,\n",
       "           0.27581392,  0.27581392,  0.31144205,  0.31144205,  0.29248909,\n",
       "           0.29248909,  0.25189015,  0.25189015,  0.32333709,  0.32333709,\n",
       "           0.33990043,  0.33990043,  0.3011999 ,  0.3011999 ,  0.30917233,\n",
       "           0.30917233,  0.22704723,  0.22704723,  0.2931313 ,  0.2931313 ,\n",
       "           0.26533363,  0.26533363,  0.28244588,  0.28244588,  0.2468974 ,\n",
       "           0.2468974 ,  0.28038196,  0.28038196, -0.00956118, -0.00956118,\n",
       "           0.05243126,  0.05243126,  0.09762116,  0.09762116,  0.0741092 ,\n",
       "           0.0741092 ,  0.06847797,  0.06847797,  0.09571271,  0.09571271,\n",
       "           0.29076584,  0.29076584,  0.30239809,  0.30239809,  0.2822821 ,\n",
       "           0.2822821 ,  0.31791023,  0.31791023,  0.29895727,  0.29895727,\n",
       "           0.25835832,  0.25835832,  0.32980527,  0.32980527,  0.3463686 ,\n",
       "           0.3463686 ,  0.30766808,  0.30766808,  0.31564051,  0.31564051,\n",
       "           0.2335154 ,  0.2335154 ,  0.29959948,  0.29959948,  0.27180181,\n",
       "           0.27180181,  0.28891406,  0.28891406,  0.25336558,  0.25336558,\n",
       "           0.28685014,  0.28685014, -0.003093  , -0.003093  ,  0.05889943,\n",
       "           0.05889943,  0.10408933,  0.10408933,  0.08057738,  0.08057738,\n",
       "           0.07494615,  0.07494615,  0.10218088,  0.10218088,  0.28412356,\n",
       "           0.28412356,  0.26400756,  0.26400756,  0.2996357 ,  0.2996357 ,\n",
       "           0.28068274,  0.28068274,  0.24008379,  0.24008379,  0.31153073,\n",
       "           0.31153073,  0.32809407,  0.32809407,  0.28939355,  0.28939355,\n",
       "           0.29736598,  0.29736598,  0.21524087,  0.21524087,  0.28132495,\n",
       "           0.28132495,  0.25352728,  0.25352728,  0.27063953,  0.27063953,\n",
       "           0.23509105,  0.23509105,  0.2685756 ,  0.2685756 , -0.02136754,\n",
       "          -0.02136754,  0.0406249 ,  0.0406249 ,  0.0858148 ,  0.0858148 ,\n",
       "           0.06230285,  0.06230285,  0.05667162,  0.05667162,  0.08390635,\n",
       "           0.08390635,  0.27563982,  0.27563982,  0.31126795,  0.31126795,\n",
       "           0.29231499,  0.29231499,  0.25171605,  0.25171605,  0.32316299,\n",
       "           0.32316299,  0.33972633,  0.33972633,  0.3010258 ,  0.3010258 ,\n",
       "           0.30899823,  0.30899823,  0.22687313,  0.22687313,  0.2929572 ,\n",
       "           0.2929572 ,  0.26515953,  0.26515953,  0.28227178,  0.28227178,\n",
       "           0.2467233 ,  0.2467233 ,  0.28020786,  0.28020786, -0.00973528,\n",
       "          -0.00973528,  0.05225716,  0.05225716,  0.09744706,  0.09744706,\n",
       "           0.0739351 ,  0.0739351 ,  0.06830387,  0.06830387,  0.0955386 ,\n",
       "           0.0955386 ,  0.29115195,  0.29115195,  0.27219899,  0.27219899,\n",
       "           0.23160005,  0.23160005,  0.30304699,  0.30304699,  0.31961033,\n",
       "           0.31961033,  0.2809098 ,  0.2809098 ,  0.28888223,  0.28888223,\n",
       "           0.20675713,  0.20675713,  0.2728412 ,  0.2728412 ,  0.24504353,\n",
       "           0.24504353,  0.26215578,  0.26215578,  0.2266073 ,  0.2266073 ,\n",
       "           0.26009186,  0.26009186, -0.02985128, -0.02985128,  0.03214116,\n",
       "           0.03214116,  0.07733106,  0.07733106,  0.0538191 ,  0.0538191 ,\n",
       "           0.04818787,  0.04818787,  0.07542261,  0.07542261,  0.30782713,\n",
       "           0.30782713,  0.26722818,  0.26722818,  0.33867512,  0.33867512,\n",
       "           0.35523846,  0.35523846,  0.31653794,  0.31653794,  0.32451037,\n",
       "           0.32451037,  0.24238526,  0.24238526,  0.30846934,  0.30846934,\n",
       "           0.28067167,  0.28067167,  0.29778392,  0.29778392,  0.26223544,\n",
       "           0.26223544,  0.29571999,  0.29571999,  0.00577685,  0.00577685,\n",
       "           0.06776929,  0.06776929,  0.11295919,  0.11295919,  0.08944724,\n",
       "           0.08944724,  0.08381601,  0.08381601,  0.11105074,  0.11105074,\n",
       "           0.24827522,  0.24827522,  0.31972216,  0.31972216,  0.3362855 ,\n",
       "           0.3362855 ,  0.29758498,  0.29758498,  0.30555741,  0.30555741,\n",
       "           0.2234323 ,  0.2234323 ,  0.28951638,  0.28951638,  0.2617187 ,\n",
       "           0.2617187 ,  0.27883096,  0.27883096,  0.24328248,  0.24328248,\n",
       "           0.27676703,  0.27676703, -0.01317611, -0.01317611,  0.04881633,\n",
       "           0.04881633,  0.09400623,  0.09400623,  0.07049428,  0.07049428,\n",
       "           0.06486305,  0.06486305,  0.09209778,  0.09209778,  0.27912322,\n",
       "           0.27912322,  0.29568656,  0.29568656,  0.25698603,  0.25698603,\n",
       "           0.26495846,  0.26495846,  0.18283336,  0.18283336,  0.24891743,\n",
       "           0.24891743,  0.22111976,  0.22111976,  0.23823201,  0.23823201,\n",
       "           0.20268353,  0.20268353,  0.23616809,  0.23616809, -0.05377505,\n",
       "          -0.05377505,  0.00821739,  0.00821739,  0.05340729,  0.05340729,\n",
       "           0.02989533,  0.02989533,  0.0242641 ,  0.0242641 ,  0.05149884,\n",
       "           0.05149884,  0.3671335 ,  0.3671335 ,  0.32843297,  0.32843297,\n",
       "           0.3364054 ,  0.3364054 ,  0.2542803 ,  0.2542803 ,  0.32036437,\n",
       "           0.32036437,  0.2925667 ,  0.2925667 ,  0.30967896,  0.30967896,\n",
       "           0.27413047,  0.27413047,  0.30761503,  0.30761503,  0.01767189,\n",
       "           0.01767189,  0.07966433,  0.07966433,  0.12485423,  0.12485423,\n",
       "           0.10134228,  0.10134228,  0.09571104,  0.09571104,  0.12294578,\n",
       "           0.12294578,  0.34499631,  0.34499631,  0.35296874,  0.35296874,\n",
       "           0.27084364,  0.27084364,  0.33692771,  0.33692771,  0.30913004,\n",
       "           0.30913004,  0.32624229,  0.32624229,  0.29069381,  0.29069381,\n",
       "           0.32417837,  0.32417837,  0.03423523,  0.03423523,  0.09622767,\n",
       "           0.09622767,  0.14141757,  0.14141757,  0.11790561,  0.11790561,\n",
       "           0.11227438,  0.11227438,  0.13950912,  0.13950912,  0.31426822,\n",
       "           0.31426822,  0.23214311,  0.23214311,  0.29822719,  0.29822719,\n",
       "           0.27042951,  0.27042951,  0.28754177,  0.28754177,  0.25199329,\n",
       "           0.25199329,  0.28547784,  0.28547784, -0.0044653 , -0.0044653 ,\n",
       "           0.05752714,  0.05752714,  0.10271704,  0.10271704,  0.07920509,\n",
       "           0.07920509,  0.07357386,  0.07357386,  0.10080859,  0.10080859,\n",
       "           0.24011554,  0.24011554,  0.30619962,  0.30619962,  0.27840194,\n",
       "           0.27840194,  0.2955142 ,  0.2955142 ,  0.25996572,  0.25996572,\n",
       "           0.29345027,  0.29345027,  0.00350713,  0.00350713,  0.06549957,\n",
       "           0.06549957,  0.11068947,  0.11068947,  0.08717752,  0.08717752,\n",
       "           0.08154629,  0.08154629,  0.10878102,  0.10878102,  0.22407451,\n",
       "           0.22407451,  0.19627684,  0.19627684,  0.21338909,  0.21338909,\n",
       "           0.17784061,  0.17784061,  0.21132517,  0.21132517, -0.07861797,\n",
       "          -0.07861797, -0.01662553, -0.01662553,  0.02856437,  0.02856437,\n",
       "           0.00505241,  0.00505241, -0.00057882, -0.00057882,  0.02665592,\n",
       "           0.02665592,  0.26236092,  0.26236092,  0.27947317,  0.27947317,\n",
       "           0.24392469,  0.24392469,  0.27740924,  0.27740924, -0.0125339 ,\n",
       "          -0.0125339 ,  0.04945854,  0.04945854,  0.09464844,  0.09464844,\n",
       "           0.07113649,  0.07113649,  0.06550526,  0.06550526,  0.09273999,\n",
       "           0.09273999,  0.2516755 ,  0.2516755 ,  0.21612702,  0.21612702,\n",
       "           0.24961157,  0.24961157, -0.04033157, -0.04033157,  0.02166087,\n",
       "           0.02166087,  0.06685077,  0.06685077,  0.04333882,  0.04333882,\n",
       "           0.03770758,  0.03770758,  0.06494232,  0.06494232,  0.23323927,\n",
       "           0.23323927,  0.26672383,  0.26672383, -0.02321931, -0.02321931,\n",
       "           0.03877312,  0.03877312,  0.08396302,  0.08396302,  0.06045107,\n",
       "           0.06045107,  0.05481984,  0.05481984,  0.08205457,  0.08205457,\n",
       "           0.23117534,  0.23117534, -0.0587678 , -0.0587678 ,  0.00322464,\n",
       "           0.00322464,  0.04841454,  0.04841454,  0.02490259,  0.02490259,\n",
       "           0.01927136,  0.01927136,  0.04650609,  0.04650609, -0.02528324,\n",
       "          -0.02528324,  0.0367092 ,  0.0367092 ,  0.0818991 ,  0.0818991 ,\n",
       "           0.05838715,  0.05838715,  0.05275591,  0.05275591,  0.07999065,\n",
       "           0.07999065, -0.25323394, -0.25323394, -0.20804404, -0.20804404,\n",
       "          -0.23155599, -0.23155599, -0.23718723, -0.23718723, -0.20995249,\n",
       "          -0.20995249, -0.1460516 , -0.1460516 , -0.16956356, -0.16956356,\n",
       "          -0.17519479, -0.17519479, -0.14796005, -0.14796005, -0.12437366,\n",
       "          -0.12437366, -0.13000489, -0.13000489, -0.10277015, -0.10277015,\n",
       "          -0.15351684, -0.15351684, -0.12628211, -0.12628211, -0.13191334,\n",
       "          -0.13191334]),\n",
       "   'feature_importances': array([-0.00042779,  0.00031734, -0.00083357, ..., -0.00029927,\n",
       "           0.00047522,  0.00108976]),\n",
       "   'model_json': None}]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sim_run(\n",
    "              cv_type='random',\n",
    "              random_seed=42,\n",
    "              model_type='ridge',\n",
    "              feature_type=[{'transcriptome': None}, \n",
    "                            {'structural': 'spectral_A_20'}\n",
    "                            ],\n",
    "              use_gpu=False,\n",
    "              use_shared_regions=False,\n",
    "              test_shared_regions=False,\n",
    "              save_sim=True,\n",
    "              connectome_target='FC',\n",
    "              search_method=('grid', 'mse')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045622f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfdb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
